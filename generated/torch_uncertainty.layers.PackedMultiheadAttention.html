


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PackedMultiheadAttention &mdash; TorchUncertainty 0.4.2 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/logo_torch_uncertainty.png" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-codeautolink.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="PackedLayerNorm" href="torch_uncertainty.layers.PackedLayerNorm.html" />
  <link rel="prev" title="PackedConv2d" href="torch_uncertainty.layers.PackedConv2d.html" />
  <!-- Google Analytics -->
  <script type="text/javascript">
    var collapsedSections = [];
  </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://torch-uncertainty.github.io/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/ENSTA-U2IS-AI/torch-uncertainty" target="_blank">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli_guide.html">CLI Guide</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
    <li>
      <a href="../index.html">
        Docs
      </a> &gt;
    </li>

    
    <li><a href="../api.html">API Reference</a> &gt;</li>
    
    <li>PackedMultiheadAttention</li>
    
    <!-- 
    <li class="pytorch-breadcrumbs-aside">
      
      
      
      
      
      <a href="/zh_CN//generated/torch_uncertainty.layers.PackedMultiheadAttention.html" class="fa fa-language"> 以中文阅读</a>
      
      
    </li>
    
     -->
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        

        <div class="pytorch-call-to-action-links">
          <div id="tutorial-type">generated/torch_uncertainty.layers.PackedMultiheadAttention</div>

          <!-- <div id="google-colab-link">
            <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg" />
            <div class="call-to-action-desktop-view">Run in Google Colab</div>
            <div class="call-to-action-mobile-view">Colab</div>
          </div> -->
          <div id="download-notebook-link">
            <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg" />
            <div class="call-to-action-desktop-view">Download Notebook</div>
            <div class="call-to-action-mobile-view">Notebook</div>
          </div>
          <div id="github-view-link">
            <img class="call-to-action-img" src="../_static/images/pytorch-github.svg" />
            <div class="call-to-action-desktop-view">View on GitHub</div>
            <div class="call-to-action-mobile-view">GitHub</div>
          </div>
        </div>

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
                
  <section id="packedmultiheadattention">
<h1>PackedMultiheadAttention<a class="headerlink" href="#packedmultiheadattention" title="Permalink to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="torch_uncertainty.layers.PackedMultiheadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torch_uncertainty.layers.</span></span><span class="sig-name descname"><span class="pre">PackedMultiheadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_estimators</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_bias_kv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_zero_attn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_first</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch_uncertainty/layers/packed.html#PackedMultiheadAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_uncertainty.layers.PackedMultiheadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Packed-Ensembles-style MultiheadAttention layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embed_dim</strong> (<em>int</em>) – Size of the embedding dimension.</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) – Number of parallel attention heads.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – The width multiplier of the embedding dimension.</p></li>
<li><p><strong>num_estimators</strong> (<em>int</em>) – The number of estimators packed in the layer.</p></li>
<li><p><strong>gamma</strong> (<em>int</em><em>, </em><em>optional</em>) – Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – Dropout probability on <code class="docutils literal notranslate"><span class="pre">attn_output_weights</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">0.0</span></code>
(no dropout).</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – Ì specified, adds bias to input / output projection layers.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>add_bias_kv</strong> (<em>bool</em><em>, </em><em>optional</em>) – If specified, adds bias to the key and value sequences at
<code class="docutils literal notranslate"><span class="pre">dim=0</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>add_zero_attn</strong> (<em>bool</em><em>, </em><em>optional</em>) – If specified, adds a new batch of zeros to the key and
value sequences at <code class="docutils literal notranslate"><span class="pre">dim=1</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>kdim</strong> (<em>int</em><em> | </em><em>None</em><em>, </em><em>optional</em>) – Total number of features for keys. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>
(uses <code class="docutils literal notranslate"><span class="pre">kdim=embed_dim</span></code>).</p></li>
<li><p><strong>vdim</strong> (<em>int</em><em> | </em><em>None</em><em>, </em><em>optional</em>) – Total number of features for values. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>
(uses <code class="docutils literal notranslate"><span class="pre">vdim=embed_dim</span></code>).</p></li>
<li><p><strong>batch_first</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature). Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code> (seq, batch, feature).</p></li>
<li><p><strong>first</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether this is the first layer of the network. Defaults to
<code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>last</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether this is the last layer of the network. Defaults to
<code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em><em>, </em><em>optional</em>) – The device to use for the layer’s parameters. Defaults
to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em><em>, </em><em>optional</em>) – The dtype to use for the layer’s parameters. Defaults to
<code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Reference:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>: Original Multihead Attention formulation.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2403.17678">Hierarchical Light Tranformer Ensembles for Multimodal Trajectory Forecasting</a>
: Packed-Ensembles-style Multihead Attention formulation.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="torch_uncertainty.layers.PackedMultiheadAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key_padding_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average_attn_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_causal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch_uncertainty/layers/packed.html#PackedMultiheadAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch_uncertainty.layers.PackedMultiheadAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes attention outputs given query, key, and value tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>Tensor</em>) – Query embeddings of shape <span class="math notranslate nohighlight">\((L, E_q)\)</span> for unbatched input,
<span class="math notranslate nohighlight">\((L, B, E_q)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code> or <span class="math notranslate nohighlight">\((B, L, E_q)\)</span> when
<code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code>, where <span class="math notranslate nohighlight">\(L\)</span> is the target sequence length, <span class="math notranslate nohighlight">\(B\)</span> is
the batch size, and <span class="math notranslate nohighlight">\(E_q\)</span> is the query embedding dimension <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code>.</p></li>
<li><p><strong>key</strong> (<em>Tensor</em>) – Key embeddingd of shape <span class="math notranslate nohighlight">\((S, E_k)\)</span> for unbatched input,
<span class="math notranslate nohighlight">\((S, B, E_k)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code> or <span class="math notranslate nohighlight">\((B, S, E_k)\)</span> when
<code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code>, where <span class="math notranslate nohighlight">\(S\)</span> is the source sequence length, <span class="math notranslate nohighlight">\(B\)</span> is
the batch size and <span class="math notranslate nohighlight">\(E_k\)</span> is the key embedding dimension <code class="docutils literal notranslate"><span class="pre">kdim</span></code>.</p></li>
<li><p><strong>value</strong> (<em>Tensor</em>) – Value embeddings of shape <span class="math notranslate nohighlight">\((S, E_v)\)</span> for unbatched input,
<span class="math notranslate nohighlight">\((S, B, E_v)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code> or <span class="math notranslate nohighlight">\((B, S, E_v)\)</span> when
<code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code>, where <span class="math notranslate nohighlight">\(S\)</span> is the source sequence length, <span class="math notranslate nohighlight">\(B\)</span> is
the batch size and <span class="math notranslate nohighlight">\(E_v\)</span> is the value embedding dimension <code class="docutils literal notranslate"><span class="pre">vdim</span></code>.</p></li>
<li><p><strong>key_padding_mask</strong> (<em>Tensor</em><em> | </em><em>None</em><em>, </em><em>optional</em>) – If specified, a mask of shape
<span class="math notranslate nohighlight">\((B, S)\)</span> indicating which elements within <code class="docutils literal notranslate"><span class="pre">key</span></code> to ignore for the purpose
of attention (i.e. treat as “padding”). For unbatched <cite>query</cite>, shape should be
<span class="math notranslate nohighlight">\((S)\)</span>. Binary and float masks are supported. For a binary mask, a <code class="docutils literal notranslate"><span class="pre">True</span></code>
value indicates that the corresponding <code class="docutils literal notranslate"><span class="pre">key</span></code> value will be ignored for the
purpose of attention. For a float mask, it will be directly added to the
corresponding <code class="docutils literal notranslate"><span class="pre">key</span></code> value. Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>need_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If specified, returns <code class="docutils literal notranslate"><span class="pre">attn_output_weights</span></code> in
addition to <code class="docutils literal notranslate"><span class="pre">attn_outputs</span></code>. Set <code class="docutils literal notranslate"><span class="pre">need_weights=False</span></code> to use the optimized
<code class="docutils literal notranslate"><span class="pre">scale_dot_product_attention</span></code> and achieve the best performance for MHA.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>attn_mask</strong> (<em>Tensor</em><em> | </em><em>None</em><em>, </em><em>optional</em>) – If specified, a 2D or 3D mask preventing attention
to certain positions. Must be of shape <span class="math notranslate nohighlight">\((L,S)\)</span> or
<span class="math notranslate nohighlight">\((B \times \text{num_heads}, L, S)\)</span>, where <span class="math notranslate nohighlight">\(B\)</span> is the batch size, <span class="math notranslate nohighlight">\(L\)</span>
is the target sequence length, and <span class="math notranslate nohighlight">\(S\)</span> is the source sequence length. A 2D mask
will be broadcasted across the batch while a 3D mask allows for a different mask for
each entry in the batch. Binary and float masks are supported. For a binary mask, a
<code class="docutils literal notranslate"><span class="pre">True</span></code> value indicates that the corresponding position is not allowed to attend to.
For a float mask, the mask values will be added to the attention weight. If both
<code class="docutils literal notranslate"><span class="pre">attn_mask</span></code> and <code class="docutils literal notranslate"><span class="pre">key_padding_mask</span></code> are provided, their types should match.
Defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>average_attn_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, indicates that the returned
<code class="docutils literal notranslate"><span class="pre">attn_weights</span></code> should be averaged across heads. Otherwise, <code class="docutils literal notranslate"><span class="pre">attn_weights</span></code> are
provided separately per head. Note that this flag only has an effect when
<code class="docutils literal notranslate"><span class="pre">need_weights=True</span></code>. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>is_causal</strong> (<em>bool</em><em>, </em><em>optional</em>) – _description_. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">need_weights=True</span></code> and therefore <code class="docutils literal notranslate"><span class="pre">average_attn_weights</span></code> are not supported yet thus
have no effect.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><em>attn_output</em> (Tensor): The output tensor of shape <span class="math notranslate nohighlight">\((L, E_q)\)</span>, <span class="math notranslate nohighlight">\((L, B, E_q)\)</span>
or <span class="math notranslate nohighlight">\((B, L, E_q)\)</span> where <span class="math notranslate nohighlight">\(L\)</span> is the target sequence length, <span class="math notranslate nohighlight">\(B\)</span> is
the batch size, and <span class="math notranslate nohighlight">\(E_q\)</span> is the embedding dimension <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code>.</p></li>
<li><p><em>attn_output_weights</em> (None): Always <code class="docutils literal notranslate"><span class="pre">None</span></code> has we do not support
<code class="docutils literal notranslate"><span class="pre">need_weights=True</span></code> yet.</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>tuple[Tensor, None]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="torch_uncertainty.layers.PackedLayerNorm.html" class="btn btn-neutral float-right" title="PackedLayerNorm" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-teal.svg"
        class="next-page"></a>
    
    
    <a href="torch_uncertainty.layers.PackedConv2d.html" class="btn btn-neutral" title="PackedConv2d" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-teal.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2025, Adrien Lafage and Olivier Laurent.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">PackedMultiheadAttention</a><ul>
<li><a class="reference internal" href="#torch_uncertainty.layers.PackedMultiheadAttention"><code class="docutils literal notranslate"><span class="pre">PackedMultiheadAttention</span></code></a><ul>
<li><a class="reference internal" href="#torch_uncertainty.layers.PackedMultiheadAttention.forward"><code class="docutils literal notranslate"><span class="pre">PackedMultiheadAttention.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="../_static/doctools.js"></script>
  <script src="../_static/sphinx_highlight.js"></script>
  <script src="../_static/clipboard.min.js"></script>
  <script src="../_static/copybutton.js"></script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://torch-uncertainty.github.io/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/ENSTA-U2IS-AI/torch-uncertainty" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>