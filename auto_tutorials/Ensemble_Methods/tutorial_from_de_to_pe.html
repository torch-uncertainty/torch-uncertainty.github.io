
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Improved Ensemble parameter-efficiency with Packed-Ensembles &#8212; TorchUncertainty 0.9.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-codeautolink.css?v=b2176991" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=39bb1c6d"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'auto_tutorials/Ensemble_Methods/tutorial_from_de_to_pe';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Segmentation" href="../Segmentation/index.html" />
    <link rel="prev" title="Ensemble Methods" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="TorchUncertainty Logo"/>
    <img src="../../_static/logo_dark.png" class="logo__image only-dark pst-js-only" alt="TorchUncertainty Logo"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../installation.html">
    Installation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../quickstart.html">
    Quickstart
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../cli_guide.html">
    CLI Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api.html">
    API Reference
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../contributing.html">
    Contributing
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../../references.html">
    References
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/ENSTA-U2IS-AI/torch-uncertainty" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../installation.html">
    Installation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../quickstart.html">
    Quickstart
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../index.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../cli_guide.html">
    CLI Guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../contributing.html">
    Contributing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../references.html">
    References
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/ENSTA-U2IS-AI/torch-uncertainty" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Classification/index.html">Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Classification/tutorial_classification.html">Training a LeNet for Image Classification with TorchUncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Classification/tutorial_distribution_shift.html">Evaluating Model Performance Under Distribution Shift with TorchUncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Classification/tutorial_evidential_classification.html">Deep Evidential Classification on a Toy Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Classification/tutorial_ood_detection.html">Out-of-distribution detection with TorchUncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Classification/tutorial_pe_cifar10.html">From a Standard Classifier to a Packed-Ensemble</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Regression/index.html">Regression with Uncertainty</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Regression/tutorial_der_cubic.html">Deep Evidential Regression on a Toy Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Regression/tutorial_probabilistic_regression.html">Deep Probabilistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Regression/tutorial_regression.html">Training an MLP for Tabular Regression with TorchUncertainty</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Post_Hoc_Methods/index.html">Post-hoc Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Post_Hoc_Methods/tutorial_conformal.html">Conformal Prediction on CIFAR-10 with TorchUncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Post_Hoc_Methods/tutorial_scaler.html">Improve Top-label Calibration with Temperature Scaling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Bayesian_Methods/index.html">Bayesian Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Bayesian_Methods/tutorial_bayesian.html">Training a Bayesian Neural Network in 20 seconds</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Bayesian_Methods/tutorial_mc_batch_norm.html">Training a LeNet with Monte Carlo Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Bayesian_Methods/tutorial_mc_dropout.html">Training a LeNet with Monte-Carlo Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Bayesian_Methods/tutorial_muad_mc_drop.html">Monte Carlo Dropout for Semantic Segmentation on MUAD</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Ensemble Methods</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Improved Ensemble parameter-efficiency with Packed-Ensembles</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Segmentation/index.html">Segmentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Segmentation/tutorial_muad_deep_en.html">Deep ensembles Segmentation Tutorial using Muad Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Segmentation/tutorial_muad_packed.html">Packed ensembles Segmentation Tutorial using Muad Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Segmentation/tutorial_muad_seg.html">Segmentation Tutorial using Muad Dataset</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Data_Augmentation/index.html">Data Augmentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Data_Augmentation/tutorial_corruption.html">Corrupting Images with TorchUncertainty to Benchmark Robustness</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
        <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Tutorials</a></li>
    
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Ensemble Methods</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Improved Ensemble parameter-efficiency with Packed-Ensembles</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-tutorials-ensemble-methods-tutorial-from-de-to-pe-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="improved-ensemble-parameter-efficiency-with-packed-ensembles">
<span id="sphx-glr-auto-tutorials-ensemble-methods-tutorial-from-de-to-pe-py"></span><h1>Improved Ensemble parameter-efficiency with Packed-Ensembles<a class="headerlink" href="#improved-ensemble-parameter-efficiency-with-packed-ensembles" title="Link to this heading">#</a></h1>
<p><em>This tutorial is adapted from a notebook part of a lecture given at the</em> <a class="reference external" href="https://haicon24.de/"><em>Helmholtz AI Conference</em></a> <em>by Sebastian Starke, Peter Steinbach, Gianni Franchi, and Olivier Laurent.</em></p>
<p>In this notebook will work on the MNIST dataset that was introduced by Corinna Cortes, Christopher J.C. Burges, and later modified by Yann LeCun in the foundational paper:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. “Gradient-based learning applied to document recognition.” Proceedings of the IEEE.</a>.</p></li>
</ul>
<p>The MNIST dataset consists of 70 000 images of handwritten digits from 0 to 9. The images are grayscale and 28x28-pixel sized. The task is to classify the images into their respective digits. The dataset can be automatically downloaded using the <cite>torchvision</cite> library.</p>
<p>In this notebook, we will train a model and an ensemble on this task and evaluate their performance. The performance will consist in the following metrics:</p>
<ul class="simple">
<li><p>Accuracy: the proportion of correctly classified images,</p></li>
<li><p>Brier score: a measure of the quality of the predicted probabilities,</p></li>
<li><p>Calibration error: a measure of the calibration of the predicted probabilities,</p></li>
<li><p>Negative Log-Likelihood: the value of the loss on the test set.</p></li>
</ul>
<p>Throughout this notebook, we abstract the training and evaluation process using <a class="reference external" href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>
and <a class="reference external" href="https://torch-uncertainty.github.io/">TorchUncertainty</a>.</p>
<p>Similarly to keras for tensorflow, PyTorch Lightning is a high-level interface for PyTorch that simplifies the training and evaluation process using a Trainer.
TorchUncertainty is partly built on top of PyTorch Lightning and provides tools to train and evaluate models with uncertainty quantification.</p>
<p>TorchUncertainty includes datamodules that handle the data loading and preprocessing. We don’t use them here for tutorial purposes.</p>
<section id="download-instantiate-and-visualize-the-datasets">
<h2>1. Download, instantiate and visualize the datasets<a class="headerlink" href="#download-instantiate-and-visualize-the-datasets" title="Link to this heading">#</a></h2>
<p>The dataset is automatically downloaded using torchvision. We then visualize a few images to see a bit what we are working with.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">T</span>

<span class="c1"># We set the number of epochs to some very low value for the sake of time</span>
<span class="n">MAX_EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Create the transforms for the images</span>
<span class="n">train_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">T</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="c1"># We perform random cropping as data augmentation</span>
        <span class="n">T</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="c1"># As for the MNIST1d dataset, we normalize the data</span>
        <span class="n">T</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,)),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">test_transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">T</span><span class="o">.</span><span class="n">Grayscale</span><span class="p">(</span><span class="n">num_output_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">T</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">T</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">28</span><span class="p">),</span>
        <span class="n">T</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,)),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Download and instantiate the dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Subset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">MNIST</span><span class="p">,</span> <span class="n">FashionMNIST</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;./data/&quot;</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">train_transform</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;./data/&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">test_transform</span><span class="p">)</span>
<span class="c1"># We only take the first 10k images to have the same number of samples as the test set using torch Subsets</span>
<span class="n">ood_data</span> <span class="o">=</span> <span class="n">Subset</span><span class="p">(</span>
    <span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;./data/&quot;</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">test_transform</span><span class="p">),</span>
    <span class="n">indices</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Create the corresponding dataloaders</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">test_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">ood_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">ood_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>  0%|          | 0.00/9.91M [00:00&lt;?, ?B/s]
  1%|          | 98.3k/9.91M [00:00&lt;00:15, 618kB/s]
  4%|▎         | 360k/9.91M [00:00&lt;00:07, 1.22MB/s]
  9%|▊         | 852k/9.91M [00:00&lt;00:03, 2.39MB/s]
 14%|█▍        | 1.38M/9.91M [00:00&lt;00:02, 3.11MB/s]
 28%|██▊       | 2.75M/9.91M [00:00&lt;00:01, 5.93MB/s]
 42%|████▏     | 4.13M/9.91M [00:00&lt;00:00, 7.75MB/s]
 75%|███████▍  | 7.41M/9.91M [00:00&lt;00:00, 14.0MB/s]
100%|██████████| 9.91M/9.91M [00:00&lt;00:00, 10.2MB/s]

  0%|          | 0.00/28.9k [00:00&lt;?, ?B/s]
100%|██████████| 28.9k/28.9k [00:00&lt;00:00, 367kB/s]

  0%|          | 0.00/1.65M [00:00&lt;?, ?B/s]
  4%|▍         | 65.5k/1.65M [00:00&lt;00:03, 419kB/s]
 20%|█▉        | 328k/1.65M [00:00&lt;00:01, 1.15MB/s]
 46%|████▌     | 754k/1.65M [00:00&lt;00:00, 1.86MB/s]
 85%|████████▌ | 1.41M/1.65M [00:00&lt;00:00, 2.77MB/s]
100%|██████████| 1.65M/1.65M [00:00&lt;00:00, 2.61MB/s]

  0%|          | 0.00/4.54k [00:00&lt;?, ?B/s]
100%|██████████| 4.54k/4.54k [00:00&lt;00:00, 9.91MB/s]

  0%|          | 0.00/26.4M [00:00&lt;?, ?B/s]
 16%|█▌        | 4.10M/26.4M [00:00&lt;00:00, 40.9MB/s]
 60%|██████    | 15.9M/26.4M [00:00&lt;00:00, 86.0MB/s]
100%|██████████| 26.4M/26.4M [00:00&lt;00:00, 91.1MB/s]

  0%|          | 0.00/29.5k [00:00&lt;?, ?B/s]
100%|██████████| 29.5k/29.5k [00:00&lt;00:00, 3.34MB/s]

  0%|          | 0.00/4.42M [00:00&lt;?, ?B/s]
 62%|██████▏   | 2.72M/4.42M [00:00&lt;00:00, 27.1MB/s]
100%|██████████| 4.42M/4.42M [00:00&lt;00:00, 36.4MB/s]

  0%|          | 0.00/5.15k [00:00&lt;?, ?B/s]
100%|██████████| 5.15k/5.15k [00:00&lt;00:00, 24.9MB/s]
</pre></div>
</div>
<p>You could replace all this cell by simply loading the MNIST datamodule from TorchUncertainty.
Now, let’s visualize a few images from the dataset. For this task, we use the viz_data dataset that applies no transformation to the images.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Datasets without transformation to visualize the unchanged data</span>
<span class="n">viz_data</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;./data/&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ood_viz_data</span> <span class="o">=</span> <span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;./data/&quot;</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;In distribution data:&quot;</span><span class="p">)</span>
<span class="n">viz_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>In distribution data:

&lt;PIL.Image.Image image mode=L size=28x28 at 0x72F2097AACB0&gt;
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Out of distribution data:&quot;</span><span class="p">)</span>
<span class="n">ood_viz_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Out of distribution data:

&lt;PIL.Image.Image image mode=L size=28x28 at 0x72F2097A8730&gt;
</pre></div>
</div>
</section>
<section id="create-train-the-model">
<h2>2. Create &amp; train the model<a class="headerlink" href="#create-train-the-model" title="Link to this heading">#</a></h2>
<p>We will create a simple convolutional neural network (CNN): the LeNet model (also introduced by LeCun).</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooling</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># No softmax in the model!</span>


<span class="c1"># Instantiate the model, the images are in grayscale so the number of channels is 1</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>We now need to define the optimization recipe:
- the optimizer, here the standard stochastic gradient descent (SGD) with a learning rate of 0.05
- the scheduler, here cosine annealing.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">optim_recipe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lr_mult</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span> <span class="o">*</span> <span class="n">lr_mult</span><span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">scheduler</span><span class="p">}</span>
</pre></div>
</div>
<p>To train the model, we use <a class="reference external" href="https://torch-uncertainty.github.io/">TorchUncertainty</a>, a library that we have developed to ease
the training and evaluation of models with uncertainty.</p>
<p><strong>Note:</strong> To train supervised classification models we most often use the cross-entropy loss.
With weight-decay, minimizing this loss amounts to finding a Maximum a posteriori (MAP) estimate of the model parameters.
This means that the model is trained to predict the most likely class for each input given a diagonal Gaussian prior on the weights.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch_uncertainty</span><span class="w"> </span><span class="kn">import</span> <span class="n">TUTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_uncertainty.routines</span><span class="w"> </span><span class="kn">import</span> <span class="n">ClassificationRoutine</span>

<span class="c1"># Create the trainer that will handle the training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TUTrainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="n">MAX_EPOCHS</span><span class="p">,</span> <span class="n">enable_progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># The routine is a wrapper of the model that contains the training logic with the metrics, etc</span>
<span class="n">routine</span> <span class="o">=</span> <span class="n">ClassificationRoutine</span><span class="p">(</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
    <span class="n">optim_recipe</span><span class="o">=</span><span class="n">optim_recipe</span><span class="p">(</span><span class="n">model</span><span class="p">),</span>
    <span class="n">eval_ood</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># In practice, avoid performing the validation on the test set (if you do model selection)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">routine</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">val_dataloaders</span><span class="o">=</span><span class="n">test_dl</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓
┃   ┃ Name             ┃ Type                ┃ Params ┃ Mode  ┃ FLOPs ┃
┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩
│ 0 │ ood_criterion    │ MaxSoftmaxCriterion │      0 │ train │     0 │
│ 1 │ model            │ LeNet               │ 44.4 K │ train │     0 │
│ 2 │ loss             │ CrossEntropyLoss    │      0 │ train │     0 │
│ 3 │ format_batch_fn  │ Identity            │      0 │ train │     0 │
│ 4 │ val_cls_metrics  │ MetricCollection    │      0 │ train │     0 │
│ 5 │ test_cls_metrics │ MetricCollection    │      0 │ train │     0 │
│ 6 │ test_id_entropy  │ Entropy             │      0 │ train │     0 │
│ 7 │ test_ood_metrics │ MetricCollection    │      0 │ train │     0 │
│ 8 │ test_ood_entropy │ Entropy             │      0 │ train │     0 │
│ 9 │ mixup            │ Identity            │      0 │ train │     0 │
└───┴──────────────────┴─────────────────────┴────────┴───────┴───────┘
Trainable params: 44.4 K
Non-trainable params: 0
Total params: 44.4 K
Total estimated model params size (MB): 0
Modules in train mode: 37
Modules in eval mode: 0
Total FLOPs: 0
</pre></div>
</div>
<p>Evaluate the trained model on the test set - pay attention to the cls/Acc metric</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">perf</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">routine</span><span class="p">,</span> <span class="n">dataloaders</span><span class="o">=</span><span class="p">[</span><span class="n">test_dl</span><span class="p">,</span> <span class="n">ood_dl</span><span class="p">])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃      Classification       ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     Acc      │          89.870%          │
│    Brier     │          0.16752          │
│   Entropy    │          0.59468          │
│     NLL      │          0.37914          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃        Calibration        ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     ECE      │          9.188%           │
│     aECE     │          9.106%           │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃       OOD Detection       ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     AUPR     │          83.454%          │
│    AUROC     │          81.939%          │
│   Entropy    │          0.59468          │
│    FPR95     │          74.360%          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃ Selective Classification  ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│    AUGRC     │          1.407%           │
│     AURC     │          1.667%           │
│  Cov@5Risk   │          87.860%          │
│  Risk@80Cov  │          2.963%           │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃        Complexity         ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│    flops     │          1.15 G           │
│    params    │          44.43 K          │
└──────────────┴───────────────────────────┘
</pre></div>
</div>
<p>This table provides a lot of information:</p>
<p><strong>OOD Detection: Binary Classification MNIST vs. FashionMNIST</strong></p>
<ul class="simple">
<li><p>AUPR/AUROC/FPR95: Measures the quality of the OOD detection. The higher the better for AUPR and AUROC, the lower the better for FPR95.</p></li>
</ul>
<p><strong>Calibration: Reliability of the Predictions</strong></p>
<ul class="simple">
<li><p>ECE: Expected Calibration Error. The lower the better.</p></li>
<li><p>aECE: Adaptive Expected Calibration Error. The lower the better. (~More precise version of the ECE)</p></li>
</ul>
<p><strong>Classification Performance</strong></p>
<ul class="simple">
<li><p>Accuracy: The ratio of correctly classified images. The higher the better.</p></li>
<li><p>Brier: The quality of the predicted probabilities (Mean Squared Error of the predictions vs. ground-truth). The lower the better.</p></li>
<li><p>Negative Log-Likelihood: The value of the loss on the test set. The lower the better.</p></li>
</ul>
<p><strong>Selective Classification &amp; Grouping Loss</strong>
- We talk about these points later in the “To go further” section.</p>
<p>By setting <cite>eval_shift</cite> to True, we could also evaluate the performance of the models on MNIST-C, a dataset close to MNIST but with perturbations.</p>
</section>
<section id="training-an-ensemble-of-models-with-torchuncertainty">
<h2>3. Training an ensemble of models with TorchUncertainty<a class="headerlink" href="#training-an-ensemble-of-models-with-torchuncertainty" title="Link to this heading">#</a></h2>
<p>You have two options here, you can either train the ensemble directly if you have enough memory,
otherwise, you can train independent models and do the ensembling during the evaluation (sometimes called inference).</p>
<p>In this case, we will do it sequentially. In this tutorial, you have the choice between training multiple models,
which will take time if you have no GPU, or downloading the pre-trained models that we have prepared for you.</p>
<p><strong>Training the ensemble</strong></p>
<p>To train the ensemble, you will have to use the “deep_ensembles” function from TorchUncertainty, which will
replicate and change the initialization of your networks to ensure diversity.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch_uncertainty.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">deep_ensembles</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_uncertainty.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">RepeatTarget</span>

<span class="c1"># Create the ensemble model</span>
<span class="n">ensemble</span> <span class="o">=</span> <span class="n">deep_ensembles</span><span class="p">(</span>
    <span class="n">LeNet</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">num_estimators</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span>
    <span class="n">reset_model_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TUTrainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="n">MAX_EPOCHS</span><span class="p">)</span>
<span class="n">ens_routine</span> <span class="o">=</span> <span class="n">ClassificationRoutine</span><span class="p">(</span>
    <span class="n">is_ensemble</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">ensemble</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>  <span class="c1"># The loss for the training</span>
    <span class="n">format_batch_fn</span><span class="o">=</span><span class="n">RepeatTarget</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>  <span class="c1"># How to handle the targets when comparing the predictions</span>
    <span class="n">optim_recipe</span><span class="o">=</span><span class="n">optim_recipe</span><span class="p">(</span>
        <span class="n">ensemble</span><span class="p">,</span> <span class="mf">2.0</span>
    <span class="p">),</span>  <span class="c1"># The optimization scheme with the optimizer and the scheduler as a dictionnary</span>
    <span class="n">eval_ood</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># We want to evaluate the OOD-related metrics</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ens_routine</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">val_dataloaders</span><span class="o">=</span><span class="n">test_dl</span><span class="p">)</span>
<span class="n">ens_perf</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">ens_routine</span><span class="p">,</span> <span class="n">dataloaders</span><span class="o">=</span><span class="p">[</span><span class="n">test_dl</span><span class="p">,</span> <span class="n">ood_dl</span><span class="p">])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓
┃    ┃ Name                 ┃ Type                ┃ Params ┃ Mode  ┃ FLOPs ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩
│ 0  │ ood_criterion        │ MaxSoftmaxCriterion │      0 │ train │     0 │
│ 1  │ model                │ _DeepEnsembles      │ 88.9 K │ train │     0 │
│ 2  │ loss                 │ CrossEntropyLoss    │      0 │ train │     0 │
│ 3  │ format_batch_fn      │ RepeatTarget        │      0 │ train │     0 │
│ 4  │ val_cls_metrics      │ MetricCollection    │      0 │ train │     0 │
│ 5  │ test_cls_metrics     │ MetricCollection    │      0 │ train │     0 │
│ 6  │ test_id_entropy      │ Entropy             │      0 │ train │     0 │
│ 7  │ test_ood_metrics     │ MetricCollection    │      0 │ train │     0 │
│ 8  │ test_ood_entropy     │ Entropy             │      0 │ train │     0 │
│ 9  │ test_id_ens_metrics  │ MetricCollection    │      0 │ train │     0 │
│ 10 │ test_ood_ens_metrics │ MetricCollection    │      0 │ train │     0 │
│ 11 │ mixup                │ Identity            │      0 │ train │     0 │
└────┴──────────────────────┴─────────────────────┴────────┴───────┴───────┘
Trainable params: 88.9 K
Non-trainable params: 0
Total params: 88.9 K
Total estimated model params size (MB): 0
Modules in train mode: 54
Modules in eval mode: 0
Total FLOPs: 0
Epoch 2/2  ━━━━━━━━━━━━━━━━ 118/118 0:00:01 •       119.07it/s v_num: 1.000
                                    0:00:00                    train_loss: 0.846
                                                               Acc: 77.210
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃      Classification       ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     Acc      │          77.210%          │
│    Brier     │          0.34634          │
│   Entropy    │          0.92447          │
│     NLL      │          0.73960          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃        Calibration        ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     ECE      │          11.496%          │
│     aECE     │          11.647%          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃       OOD Detection       ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     AUPR     │          58.252%          │
│    AUROC     │          62.143%          │
│   Entropy    │          0.92447          │
│    FPR95     │          71.990%          │
│ ens_Disagre… │          0.57480          │
│ ens_Entropy  │          0.97280          │
│    ens_MI    │          0.23191          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃ Selective Classification  ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│    AUGRC     │          5.117%           │
│     AURC     │          6.637%           │
│  Cov@5Risk   │          52.230%          │
│  Risk@80Cov  │          12.775%          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃        Complexity         ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│    flops     │          2.31 G           │
│    params    │          88.85 K          │
└──────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:00 • 0:00:00 41.48it/s
</pre></div>
</div>
<p>Feel free to run the notebook on your machine for a longer duration.</p>
<p>We need to multiply the learning rate by 2 to account for the fact that we have 2 models
in the ensemble and that we average the loss over all the predictions.</p>
<p><strong>Downloading the pre-trained models</strong></p>
<p>We have put the pre-trained models on Hugging Face that you can download with the utility function
“hf_hub_download” imported just below. These models are trained for 75 epochs and are therefore not
comparable to the all the others trained in this notebook. The pretrained models can be seen
on <a class="reference external" href="https://huggingface.co/ENSTA-U2IS/tutorial-models">HuggingFace</a> and TorchUncertainty’s are <a class="reference external" href="https://huggingface.co/torch-uncertainty">there</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch_uncertainty.utils.hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">hf_hub_download</span>

<span class="n">all_models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
    <span class="n">hf_hub_download</span><span class="p">(</span>
        <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;ENSTA-U2IS/tutorial-models&quot;</span><span class="p">,</span>
        <span class="n">filename</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;version_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ckpt&quot;</span><span class="p">,</span>
        <span class="n">local_dir</span><span class="o">=</span><span class="s2">&quot;./models/&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;./models/version_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ckpt&quot;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span>
        <span class="s2">&quot;state_dict&quot;</span>
    <span class="p">]</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;model.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
    <span class="n">all_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch_uncertainty.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">deep_ensembles</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_uncertainty.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">RepeatTarget</span>

<span class="n">ensemble</span> <span class="o">=</span> <span class="n">deep_ensembles</span><span class="p">(</span>
    <span class="n">all_models</span><span class="p">,</span>
    <span class="n">num_estimators</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span>
    <span class="n">reset_model_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ens_routine</span> <span class="o">=</span> <span class="n">ClassificationRoutine</span><span class="p">(</span>
    <span class="n">is_ensemble</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">ensemble</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>  <span class="c1"># The loss for the training</span>
    <span class="n">format_batch_fn</span><span class="o">=</span><span class="n">RepeatTarget</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span>  <span class="c1"># How to handle the targets when comparing the predictions</span>
    <span class="n">optim_recipe</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># No optim recipe as the model is already trained</span>
    <span class="n">eval_ood</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># We want to evaluate the OOD-related metrics</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TUTrainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="n">MAX_EPOCHS</span><span class="p">)</span>

<span class="n">ens_perf</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">ens_routine</span><span class="p">,</span> <span class="n">dataloaders</span><span class="o">=</span><span class="p">[</span><span class="n">test_dl</span><span class="p">,</span> <span class="n">ood_dl</span><span class="p">])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃      Classification       ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     Acc      │          99.610%          │
│    Brier     │          0.00677          │
│   Entropy    │          0.02816          │
│     NLL      │          0.01454          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃        Calibration        ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     ECE      │          0.459%           │
│     aECE     │          0.451%           │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃       OOD Detection       ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     AUPR     │          98.980%          │
│    AUROC     │          99.205%          │
│   Entropy    │          0.02816          │
│    FPR95     │          2.640%           │
│ ens_Disagre… │          0.38780          │
│ ens_Entropy  │          1.01786          │
│    ens_MI    │          0.23445          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃ Selective Classification  ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│    AUGRC     │          0.004%           │
│     AURC     │          0.004%           │
│  Cov@5Risk   │         100.000%          │
│  Risk@80Cov  │          0.000%           │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃        Complexity         ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│    flops     │          9.23 G           │
│    params    │         355.41 K          │
└──────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:00 • 0:00:00 43.02it/s
</pre></div>
</div>
</section>
<section id="from-deep-ensembles-to-packed-ensembles">
<h2>4. From Deep Ensembles to Packed-Ensembles<a class="headerlink" href="#from-deep-ensembles-to-packed-ensembles" title="Link to this heading">#</a></h2>
<p>In the paper <a class="reference external" href="https://arxiv.org/abs/2210.09184">Packed-Ensembles for Efficient Uncertainty Quantification</a>
published at the International Conference on Learning Representations (ICLR) in 2023, we introduced a
modification of Deep Ensembles to make it more computationally-efficient. The idea is to pack the ensemble
members into a single model, which allows us to train the ensemble in a single forward pass.
This modification is particularly useful when the ensemble size is large, as it is often the case in practice.</p>
<p>We will need to update the model and replace the layers with their Packed equivalents. You can find the
documentation of the Packed-Linear layer using this <a class="reference external" href="https://torch-uncertainty.github.io/generated/torch_uncertainty.layers.PackedLinear.html">link</a>,
and the Packed-Conv2D, <a class="reference external" href="https://torch-uncertainty.github.io/generated/torch_uncertainty.layers.PackedLinear.html">here</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch_uncertainty.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PackedConv2d</span><span class="p">,</span> <span class="n">PackedLinear</span>


<span class="k">class</span><span class="w"> </span><span class="nc">PackedLeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_estimators</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_estimators</span> <span class="o">=</span> <span class="n">num_estimators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">PackedConv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="mi">6</span><span class="p">,</span>
            <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">num_estimators</span><span class="o">=</span><span class="n">num_estimators</span><span class="p">,</span>
            <span class="n">first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">PackedConv2d</span><span class="p">(</span>
            <span class="mi">6</span><span class="p">,</span>
            <span class="mi">16</span><span class="p">,</span>
            <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">num_estimators</span><span class="o">=</span><span class="n">num_estimators</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooling</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">PackedLinear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">num_estimators</span><span class="o">=</span><span class="n">num_estimators</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">PackedLinear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">num_estimators</span><span class="o">=</span><span class="n">num_estimators</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">PackedLinear</span><span class="p">(</span>
            <span class="mi">84</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">num_estimators</span><span class="o">=</span><span class="n">num_estimators</span><span class="p">,</span>
            <span class="n">last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>  <span class="c1"># Again, no softmax in the model</span>


<span class="c1"># Instantiate the model, the images are in grayscale so the number of channels is 1</span>
<span class="n">packed_model</span> <span class="o">=</span> <span class="n">PackedLeNet</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_estimators</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Create the trainer that will handle the training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TUTrainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="n">MAX_EPOCHS</span><span class="p">)</span>

<span class="c1"># The routine is a wrapper of the model that contains the training logic with the metrics, etc</span>
<span class="n">packed_routine</span> <span class="o">=</span> <span class="n">ClassificationRoutine</span><span class="p">(</span>
    <span class="n">is_ensemble</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">packed_model</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
    <span class="n">format_batch_fn</span><span class="o">=</span><span class="n">RepeatTarget</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">optim_recipe</span><span class="o">=</span><span class="n">optim_recipe</span><span class="p">(</span><span class="n">packed_model</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">),</span>
    <span class="n">eval_ood</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># In practice, avoid performing the validation on the test set</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">packed_routine</span><span class="p">,</span> <span class="n">train_dataloaders</span><span class="o">=</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">val_dataloaders</span><span class="o">=</span><span class="n">test_dl</span><span class="p">)</span>

<span class="n">packed_perf</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">packed_routine</span><span class="p">,</span> <span class="n">dataloaders</span><span class="o">=</span><span class="p">[</span><span class="n">test_dl</span><span class="p">,</span> <span class="n">ood_dl</span><span class="p">])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓
┃    ┃ Name                 ┃ Type                ┃ Params ┃ Mode  ┃ FLOPs ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩
│ 0  │ ood_criterion        │ MaxSoftmaxCriterion │      0 │ train │     0 │
│ 1  │ model                │ PackedLeNet         │ 45.7 K │ train │     0 │
│ 2  │ loss                 │ CrossEntropyLoss    │      0 │ train │     0 │
│ 3  │ format_batch_fn      │ RepeatTarget        │      0 │ train │     0 │
│ 4  │ val_cls_metrics      │ MetricCollection    │      0 │ train │     0 │
│ 5  │ test_cls_metrics     │ MetricCollection    │      0 │ train │     0 │
│ 6  │ test_id_entropy      │ Entropy             │      0 │ train │     0 │
│ 7  │ test_ood_metrics     │ MetricCollection    │      0 │ train │     0 │
│ 8  │ test_ood_entropy     │ Entropy             │      0 │ train │     0 │
│ 9  │ test_id_ens_metrics  │ MetricCollection    │      0 │ train │     0 │
│ 10 │ test_ood_ens_metrics │ MetricCollection    │      0 │ train │     0 │
│ 11 │ mixup                │ Identity            │      0 │ train │     0 │
└────┴──────────────────────┴─────────────────────┴────────┴───────┴───────┘
Trainable params: 45.7 K
Non-trainable params: 0
Total params: 45.7 K
Total estimated model params size (MB): 0
Modules in train mode: 47
Modules in eval mode: 0
Total FLOPs: 0
Epoch 2/2  ━━━━━━━━━━━━━━━━ 118/118 0:00:01 •       126.83it/s v_num: 3.000
                                    0:00:00                    train_loss: 1.286
                                                               Acc: 85.500
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃      Classification       ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     Acc      │          85.500%          │
│    Brier     │          0.33266          │
│   Entropy    │          1.53609          │
│     NLL      │          0.78450          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃        Calibration        ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     ECE      │          31.763%          │
│     aECE     │          31.763%          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃       OOD Detection       ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│     AUPR     │          78.175%          │
│    AUROC     │          83.061%          │
│   Entropy    │          1.53609          │
│    FPR95     │          43.610%          │
│ ens_Disagre… │          0.74562          │
│ ens_Entropy  │          1.69121          │
│    ens_MI    │          0.26824          │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃ Selective Classification  ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│    AUGRC     │          2.436%           │
│     AURC     │          2.887%           │
│  Cov@5Risk   │          76.080%          │
│  Risk@80Cov  │          6.225%           │
└──────────────┴───────────────────────────┘
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Test metric  ┃        Complexity         ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│    flops     │          1.51 G           │
│    params    │          45.67 K          │
└──────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5/5 0:00:00 • 0:00:00 42.22it/s
</pre></div>
</div>
<p>The training time should be approximately similar to the one of the single model that you trained before. However, please note that we are working with very small models, hence completely underusing your GPU. As such, the training time is not representative of what you would observe with larger models.</p>
<p>You can read more on Packed-Ensembles in the <a class="reference external" href="https://arxiv.org/abs/2210.09184">paper</a> or the <a class="reference external" href="https://medium.com/&#64;adrien.lafage/make-your-neural-networks-more-reliable-with-packed-ensembles-7ad0b737a873">Medium</a> post.</p>
</section>
<section id="to-go-further-more-concepts-of-uncertainty-in-ml">
<h2>To Go Further &amp; More Concepts of Uncertainty in ML<a class="headerlink" href="#to-go-further-more-concepts-of-uncertainty-in-ml" title="Link to this heading">#</a></h2>
<p><strong>Question 1:</strong> Have a look at the models in the “lightning_logs”. If you are on your own machine, try to visualize the learning curves with <cite>tensorboard –logdir lightning_logs</cite>.</p>
<p><strong>Question 2:</strong> Add a cell below and try to find the errors made by packed-ensembles on the test set. Visualize the errors and their labels and look at the predictions of the different sub-models. Are they similar? Can you think of uncertainty scores that could help you identify these errors?</p>
<section id="selective-classification">
<h3>Selective Classification<a class="headerlink" href="#selective-classification" title="Link to this heading">#</a></h3>
<p>Selective classification or “prediction with rejection” is a paradigm in uncertainty-aware machine learning where the model can decide not to make a prediction if the confidence score given by the model is below some pre-computed threshold. This can be useful in real-world applications where the cost of making a wrong prediction is high.</p>
<p>In constrast to calibration, the values of the confidence scores are not important, only the order of the scores. <em>Ideally, the best model will order all the correct predictions first, and all the incorrect predictions last.</em> In this case, there will be a threshold so that all the predictions above the threshold are correct, and all the predictions below the threshold are incorrect.</p>
<p>In TorchUncertainty, we look at 3 different metrics for selective classification:</p>
<ul class="simple">
<li><p><strong>AURC</strong>: The area under the Risk (% of errors) vs. Coverage (% of classified samples) curve. This curve expresses how the risk of the model evolves as we increase the coverage (the proportion of predictions that are above the selection threshold). This metric will be minimized by a model able to perfectly separate the correct and incorrect predictions.</p></li>
</ul>
<p>The following metrics are computed at a fixed risk and coverage level and that have practical interests. The idea of these metrics is that you can set the selection threshold to achieve a certain level of risk and coverage, as required by the technical constraints of your application:</p>
<ul class="simple">
<li><p><strong>Coverage at 5% Risk</strong>: The proportion of predictions that are above the selection threshold when it is set for the risk to egal 5%. Set the risk threshold to your application constraints. The higher the better.</p></li>
<li><p><strong>Risk at 80% Coverage</strong>: The proportion of errors when the coverage is set to 80%. Set the coverage threshold to your application constraints. The lower the better.</p></li>
</ul>
</section>
<section id="grouping-loss">
<h3>Grouping Loss<a class="headerlink" href="#grouping-loss" title="Link to this heading">#</a></h3>
<p>The grouping loss is a measure of uncertainty orthogonal to calibration. Have a look at <a class="reference external" href="https://arxiv.org/abs/2210.16315">this paper</a> to learn about it. Check out their small library <a class="reference external" href="https://github.com/aperezlebel/glest">GLest</a>. TorchUncertainty includes a wrapper of the library to compute the grouping loss with eval_grouping_loss parameter.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 30.137 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-tutorials-ensemble-methods-tutorial-from-de-to-pe-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/a24826754d6ced80ce40d102f0dc0731/tutorial_from_de_to_pe.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tutorial_from_de_to_pe.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/52a043c39064b31e6fd2a9d670a369fb/tutorial_from_de_to_pe.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tutorial_from_de_to_pe.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/9970d33187659754c103e29f8f5297e9/tutorial_from_de_to_pe.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">tutorial_from_de_to_pe.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Ensemble Methods</p>
      </div>
    </a>
    <a class="right-next"
       href="../Segmentation/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Segmentation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#download-instantiate-and-visualize-the-datasets">1. Download, instantiate and visualize the datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-train-the-model">2. Create &amp; train the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-an-ensemble-of-models-with-torchuncertainty">3. Training an ensemble of models with TorchUncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-deep-ensembles-to-packed-ensembles">4. From Deep Ensembles to Packed-Ensembles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#to-go-further-more-concepts-of-uncertainty-in-ml">To Go Further &amp; More Concepts of Uncertainty in ML</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selective-classification">Selective Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#grouping-loss">Grouping Loss</a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/auto_tutorials/Ensemble_Methods/tutorial_from_de_to_pe.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
  
    <div class="footer-items__center">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2026, Adrien Lafage and Olivier Laurent.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
</div>

  </footer>
  </body>
</html>