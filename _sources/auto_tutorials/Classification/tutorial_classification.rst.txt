
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/Classification/tutorial_classification.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_Classification_tutorial_classification.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_Classification_tutorial_classification.py:


Training a LeNet for Image Classification with TorchUncertainty
===============================================================

In this tutorial, we will train a LeNet classifier on the MNIST dataset using TorchUncertainty.
You will discover two of the core tools from TorchUncertainty, namely

- the routine: a model wrapper, which handles the training and evaluation logics, here for classification
- the datamodules: python classes, which provide the dataloaders used by the routine


1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

First, we have to load the following utilities from TorchUncertainty:

- the TUTrainer which mostly handles the link with the hardware (accelerators, precision, etc)
- the classification training & evaluation routine from torch_uncertainty.routines
- the datamodule handling dataloaders: MNISTDataModule from torch_uncertainty.datamodules
- the model: lenet from torch_uncertainty.models
- an optimization recipe in the torch_uncertainty.optim_recipes module.

.. GENERATED FROM PYTHON SOURCE LINES 26-36

.. code-block:: Python

    from pathlib import Path

    from torch import nn

    from torch_uncertainty import TUTrainer
    from torch_uncertainty.datamodules import MNISTDataModule
    from torch_uncertainty.models.classification.lenet import lenet
    from torch_uncertainty.optim_recipes import optim_cifar10_resnet18
    from torch_uncertainty.routines import ClassificationRoutine








.. GENERATED FROM PYTHON SOURCE LINES 37-42

2. Creating the Trainer and the DataModule
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the following, we first create the trainer and instantiate the datamodule that handles the MNIST dataset,
dataloaders and transforms.

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: Python


    trainer = TUTrainer(accelerator="gpu", devices=1, max_epochs=2, enable_progress_bar=False)

    # datamodule providing the dataloaders to the trainer
    root = Path("data")
    datamodule = MNISTDataModule(root=root, batch_size=128)








.. GENERATED FROM PYTHON SOURCE LINES 50-54

3. Instantiating the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~

We create the model easily using the blueprint from torch_uncertainty.models.

.. GENERATED FROM PYTHON SOURCE LINES 54-61

.. code-block:: Python


    model = lenet(
        in_channels=datamodule.num_channels,
        num_classes=datamodule.num_classes,
        dropout_rate=0.4,
    )








.. GENERATED FROM PYTHON SOURCE LINES 62-69

4. The Loss and the Routine
~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is a classification problem, and we use CrossEntropyLoss as the (negative-log-)likelihood.
We define the training routine using the classification routine from torch_uncertainty.routines.
We provide the number of classes, the model, the optimization recipe, the loss, and tell the routine
that our model is an ensemble at evaluation time with the `is_ensemble` flag.

.. GENERATED FROM PYTHON SOURCE LINES 69-78

.. code-block:: Python


    routine = ClassificationRoutine(
        num_classes=datamodule.num_classes,
        model=model,
        loss=nn.CrossEntropyLoss(),
        optim_recipe=optim_cifar10_resnet18(model),
        is_ensemble=True,
    )








.. GENERATED FROM PYTHON SOURCE LINES 79-85

5. Gathering Everything and Training the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can now train the model using the trainer. We pass the routine and the datamodule
to the fit and test methods of the trainer. It will automatically evaluate uncertainty
metrics that you will find in the table below.

.. GENERATED FROM PYTHON SOURCE LINES 85-89

.. code-block:: Python


    trainer.fit(model=routine, datamodule=datamodule)
    results = trainer.test(model=routine, datamodule=datamodule)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0.00/9.91M [00:00<?, ?B/s]      0%|          | 32.8k/9.91M [00:52<4:24:04, 624B/s]      1%|          | 65.5k/9.91M [00:52<1:49:04, 1.50kB/s]      1%|▏         | 131k/9.91M [00:52<40:40, 4.01kB/s]         2%|▏         | 164k/9.91M [00:53<28:14, 5.75kB/s]      2%|▏         | 229k/9.91M [00:53<15:04, 10.7kB/s]      3%|▎         | 295k/9.91M [00:53<09:07, 17.6kB/s]      4%|▎         | 360k/9.91M [00:53<05:55, 26.9kB/s]      4%|▍         | 426k/9.91M [00:53<03:58, 39.8kB/s]      5%|▍         | 492k/9.91M [00:53<02:46, 56.7kB/s]      6%|▌         | 590k/9.91M [00:54<01:43, 90.4kB/s]      7%|▋         | 655k/9.91M [00:54<01:17, 119kB/s]       8%|▊         | 754k/9.91M [00:54<00:52, 174kB/s]      9%|▊         | 852k/9.91M [00:54<00:38, 233kB/s]     10%|▉         | 983k/9.91M [00:54<00:27, 327kB/s]     11%|█         | 1.08M/9.91M [00:54<00:21, 403kB/s]     12%|█▏        | 1.18M/9.91M [00:54<00:18, 470kB/s]     13%|█▎        | 1.31M/9.91M [00:54<00:14, 602kB/s]     15%|█▍        | 1.44M/9.91M [00:55<00:12, 701kB/s]     16%|█▌        | 1.57M/9.91M [00:55<00:10, 809kB/s]     17%|█▋        | 1.70M/9.91M [00:55<00:09, 868kB/s]     19%|█▉        | 1.90M/9.91M [00:55<00:07, 1.08MB/s]     20%|██        | 2.03M/9.91M [00:55<00:07, 1.08MB/s]     22%|██▏       | 2.23M/9.91M [00:55<00:06, 1.27MB/s]     24%|██▍       | 2.42M/9.91M [00:55<00:05, 1.35MB/s]     27%|██▋       | 2.65M/9.91M [00:55<00:04, 1.55MB/s]     29%|██▉       | 2.85M/9.91M [00:56<00:04, 1.54MB/s]     31%|███▏      | 3.11M/9.91M [00:56<00:03, 1.77MB/s]     34%|███▎      | 3.34M/9.91M [00:56<00:03, 1.82MB/s]     37%|███▋      | 3.67M/9.91M [00:56<00:02, 2.10MB/s]     40%|███▉      | 3.93M/9.91M [00:56<00:02, 2.08MB/s]     43%|████▎     | 4.29M/9.91M [00:56<00:02, 2.41MB/s]     47%|████▋     | 4.62M/9.91M [00:56<00:02, 2.46MB/s]     51%|█████     | 5.01M/9.91M [00:56<00:01, 2.77MB/s]     54%|█████▍    | 5.37M/9.91M [00:56<00:01, 2.85MB/s]     59%|█████▉    | 5.87M/9.91M [00:57<00:01, 3.22MB/s]     63%|██████▎   | 6.29M/9.91M [00:57<00:01, 3.33MB/s]     69%|██████▉   | 6.85M/9.91M [00:57<00:00, 3.73MB/s]     74%|███████▎  | 7.31M/9.91M [00:57<00:00, 3.80MB/s]     81%|████████  | 8.00M/9.91M [00:57<00:00, 4.35MB/s]     86%|████████▋ | 8.55M/9.91M [00:57<00:00, 4.45MB/s]     94%|█████████▎| 9.27M/9.91M [00:57<00:00, 4.92MB/s]    100%|██████████| 9.91M/9.91M [00:57<00:00, 171kB/s] 
      0%|          | 0.00/28.9k [00:00<?, ?B/s]    100%|██████████| 28.9k/28.9k [00:00<00:00, 373kB/s]
      0%|          | 0.00/1.65M [00:00<?, ?B/s]      4%|▍         | 65.5k/1.65M [00:00<00:03, 417kB/s]     20%|█▉        | 328k/1.65M [00:00<00:00, 1.40MB/s]     34%|███▍      | 557k/1.65M [00:00<00:00, 1.58MB/s]     78%|███████▊  | 1.28M/1.65M [00:00<00:00, 3.36MB/s]    100%|██████████| 1.65M/1.65M [00:00<00:00, 2.97MB/s]
      0%|          | 0.00/4.54k [00:00<?, ?B/s]    100%|██████████| 4.54k/4.54k [00:00<00:00, 10.6MB/s]
    ┏━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓
    ┃   ┃ Name                ┃ Type                ┃ Params ┃ Mode  ┃ FLOPs ┃
    ┡━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩
    │ 0 │ ood_criterion       │ MaxSoftmaxCriterion │      0 │ train │     0 │
    │ 1 │ model               │ _LeNet              │ 44.4 K │ train │     0 │
    │ 2 │ loss                │ CrossEntropyLoss    │      0 │ train │     0 │
    │ 3 │ format_batch_fn     │ Identity            │      0 │ train │     0 │
    │ 4 │ val_cls_metrics     │ MetricCollection    │      0 │ train │     0 │
    │ 5 │ test_cls_metrics    │ MetricCollection    │      0 │ train │     0 │
    │ 6 │ test_id_entropy     │ Entropy             │      0 │ train │     0 │
    │ 7 │ test_id_ens_metrics │ MetricCollection    │      0 │ train │     0 │
    │ 8 │ mixup               │ Identity            │      0 │ train │     0 │
    └───┴─────────────────────┴─────────────────────┴────────┴───────┴───────┘
    Trainable params: 44.4 K                                                        
    Non-trainable params: 0                                                         
    Total params: 44.4 K                                                            
    Total estimated model params size (MB): 0                                       
    Modules in train mode: 41                                                       
    Modules in eval mode: 0                                                         
    Total FLOPs: 0                                                                  
    /home/chocolatine/actions-runner/_work/torch-uncertainty/torch-uncertainty/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
    /home/chocolatine/actions-runner/_work/torch-uncertainty/torch-uncertainty/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
    /home/chocolatine/actions-runner/_work/torch-uncertainty/torch-uncertainty/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:434: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃      Classification       ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     Acc      │          91.370%          │
    │    Brier     │          0.14599          │
    │   Entropy    │          0.63373          │
    │     NLL      │          0.34618          │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Calibration        ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     ECE      │          9.569%           │
    │     aECE     │          9.569%           │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃ Selective Classification  ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │    AUGRC     │          1.041%           │
    │     AURC     │          1.252%           │
    │  Cov@5Risk   │          92.500%          │
    │  Risk@80Cov  │          1.887%           │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Complexity         ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │    flops     │          72.10 M          │
    │    params    │          44.43 K          │
    └──────────────┴───────────────────────────┘




.. GENERATED FROM PYTHON SOURCE LINES 90-94

6. Evaluating the Model
~~~~~~~~~~~~~~~~~~~~~~~

Now that the model is trained, let's test it on MNIST.

.. GENERATED FROM PYTHON SOURCE LINES 94-126

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import torch
    import torchvision


    def imshow(img) -> None:
        npimg = img.numpy()
        npimg = npimg * 0.3081 + 0.1307  # unnormalize
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.axis("off")
        plt.tight_layout()
        plt.show()


    images, labels = next(iter(datamodule.val_dataloader()))

    # print images
    imshow(torchvision.utils.make_grid(images[:6, ...], padding=0))
    print("Ground truth labels: ", " ".join(f"{labels[j]}" for j in range(6)))

    routine.eval()
    logits = routine(images)

    probs = torch.nn.functional.softmax(logits, dim=-1)

    values, predicted = torch.max(probs, 1)
    print(
        "LeNet predictions for the first 6 images: ",
        " ".join([str(image_id.item()) for image_id in predicted[:6]]),
    )



.. image-sg:: /auto_tutorials/Classification/images/sphx_glr_tutorial_classification_001.png
   :alt: tutorial classification
   :srcset: /auto_tutorials/Classification/images/sphx_glr_tutorial_classification_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Ground truth labels:  7 2 1 0 4 1
    LeNet predictions for the first 6 images:  7 2 1 0 4 1





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 30.325 seconds)


.. _sphx_glr_download_auto_tutorials_Classification_tutorial_classification.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_classification.ipynb <tutorial_classification.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_classification.py <tutorial_classification.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: tutorial_classification.zip <tutorial_classification.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
