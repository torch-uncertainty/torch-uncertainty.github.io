
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/Classification/tutorial_evidential_classification.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_Classification_tutorial_evidential_classification.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_Classification_tutorial_evidential_classification.py:


Deep Evidential Classification on a Toy Example
===============================================

This tutorial aims to provide an introductory overview of Deep Evidential Classification (DEC) using a practical example.
We demonstrate an application of DEC by tackling the toy-problem of fitting the MNIST dataset using a Multi-Layer Perceptron (MLP)
neural network model. The output of the MLP is modeled as a Dirichlet distribution. The MLP is trained by minimizing the DEC loss
function, composed of a Bayesian risk square error loss and a regularization term based on KL Divergence.

DEC represents an evidential approach to quantifying uncertainty in neural network classification models. This method involves
introducing prior distributions over the parameters of the Categorical likelihood function. Then, the MLP model estimates the
parameters of the evidential distribution.

Training a LeNet with DEC using TorchUncertainty models
-------------------------------------------------------

In this part, we train a neural network, based on the model and routines already implemented in TU.

1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

To train a LeNet with the DEC loss function using TorchUncertainty, we have to load the following utilities from TorchUncertainty:

- our wrapper of the Lightning Trainer
- the model: lenet, which lies in torch_uncertainty.models.classification.lenet
- the classification training routine in the torch_uncertainty.routines
- the evidential objective: the DECLoss from torch_uncertainty.losses
- the datamodule that handles dataloaders & transforms: MNISTDataModule from torch_uncertainty.datamodules

We also need to define an optimizer using torch.optim, the neural network utils within torch.nn.

.. GENERATED FROM PYTHON SOURCE LINES 35-51

.. code-block:: Python

    from pathlib import Path

    import torch
    from torch import optim

    from torch_uncertainty import TUTrainer
    from torch_uncertainty.datamodules import MNISTDataModule
    from torch_uncertainty.losses import DECLoss
    from torch_uncertainty.models.classification import lenet
    from torch_uncertainty.routines import ClassificationRoutine

    # We also define the main hyperparameters.
    # We set the number of epochs to some very low value for the sake of time.
    MAX_EPOCHS = 3
    BATCH_SIZE = 512








.. GENERATED FROM PYTHON SOURCE LINES 52-58

2. Creating the necessary variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the following, we need to define the root of the logs, and to
We use the same MNIST classification example as that used in the
original DEC paper.

.. GENERATED FROM PYTHON SOURCE LINES 58-69

.. code-block:: Python

    trainer = TUTrainer(accelerator="gpu", devices=1, max_epochs=MAX_EPOCHS, enable_progress_bar=False)

    # datamodule
    root = Path() / "data"
    datamodule = MNISTDataModule(root=root, batch_size=BATCH_SIZE, num_workers=8)

    model = lenet(
        in_channels=datamodule.num_channels,
        num_classes=datamodule.num_classes,
    )








.. GENERATED FROM PYTHON SOURCE LINES 70-80

3. The Loss and the Training Routine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Next, we need to define the loss to be used during training.
After that, we define the training routine using
the single classification model training routine from
torch_uncertainty.routines.ClassificationRoutine.
In this routine, we provide the model, the DEC loss, the optimizer,
and all the default arguments.
We follow the official implementation in DEC, use the Adam optimizer
with the default learning rate of 0.002 and a weight decay of 0.005.

.. GENERATED FROM PYTHON SOURCE LINES 80-90

.. code-block:: Python


    loss = DECLoss(reg_weight=1e-2)

    routine = ClassificationRoutine(
        model=model,
        num_classes=datamodule.num_classes,
        loss=loss,
        optim_recipe=optim.Adam(model.parameters(), lr=2e-2, weight_decay=0.005),
    )








.. GENERATED FROM PYTHON SOURCE LINES 91-93

4. Gathering Everything and Training the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 93-96

.. code-block:: Python


    trainer.fit(model=routine, datamodule=datamodule)
    trainer.test(model=routine, datamodule=datamodule)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃      Classification       ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     Acc      │          72.540%          │
    │    Brier     │          0.48181          │
    │   Entropy    │          0.11676          │
    │     NLL      │          1.96484          │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Calibration        ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     ECE      │          23.454%          │
    │     aECE     │          23.454%          │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃ Selective Classification  ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │    AUGRC     │          5.783%           │
    │     AURC     │          8.204%           │
    │  Cov@5Risk   │          64.290%          │
    │  Risk@80Cov  │          14.613%          │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Complexity         ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │    flops     │         288.40 M          │
    │    params    │          44.43 K          │
    └──────────────┴───────────────────────────┘

    [{'test/cal/ECE': 0.23453570902347565, 'test/cal/aECE': 0.2345350682735443, 'test/cls/Acc': 0.7253999710083008, 'test/cls/Brier': 0.481812447309494, 'test/cls/NLL': 1.964837908744812, 'test/sc/AUGRC': 0.05783192068338394, 'test/sc/AURC': 0.08203836530447006, 'test/sc/Cov@5Risk': 0.6428999900817871, 'test/sc/Risk@80Cov': 0.14612500369548798, 'test/cls/Entropy': 0.11675838381052017, 'test/cplx/flops': 288399360.0, 'test/cplx/params': 44426.0}]



.. GENERATED FROM PYTHON SOURCE LINES 97-100

5. Testing the Model
~~~~~~~~~~~~~~~~~~~~
Now that the model is trained, let's test it on MNIST.

.. GENERATED FROM PYTHON SOURCE LINES 100-147

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import torchvision
    import torchvision.transforms.functional as F


    def imshow(img) -> None:
        npimg = img.numpy()
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.show()


    def rotated_mnist(angle: int) -> None:
        """Rotate MNIST images and show images and confidence.

        Args:
            angle: Rotation angle in degrees.
        """
        rotated_images = F.rotate(images, angle)
        # print rotated images
        plt.axis("off")
        imshow(torchvision.utils.make_grid(rotated_images[:4, ...], padding=0))
        print("Ground truth: ", " ".join(f"{labels[j]}" for j in range(4)))

        evidence = routine(rotated_images)
        alpha = torch.relu(evidence) + 1
        strength = torch.sum(alpha, dim=1, keepdim=True)
        probs = alpha / strength
        entropy = -1 * torch.sum(probs * torch.log(probs), dim=1, keepdim=True)
        for j in range(4):
            predicted = torch.argmax(probs[j, :])
            print(
                f"Predicted digits for the image {j}: {predicted} with strength "
                f"{strength[j, 0]:.3f} and entropy {entropy[j, 0]:.3f}."
            )


    dataiter = iter(datamodule.val_dataloader())
    images, labels = next(dataiter)

    with torch.no_grad():
        routine.eval()
        rotated_mnist(0)
        rotated_mnist(45)
        rotated_mnist(90)




.. image-sg:: /auto_tutorials/Classification/images/sphx_glr_tutorial_evidential_classification_001.png
   :alt: tutorial evidential classification
   :srcset: /auto_tutorials/Classification/images/sphx_glr_tutorial_evidential_classification_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Ground truth:  7 2 1 0
    Predicted digits for the image 0: 7 with strength 117.447 and entropy 0.439.
    Predicted digits for the image 1: 4 with strength 19.292 and entropy 1.716.
    Predicted digits for the image 2: 1 with strength 89.678 and entropy 0.546.
    Predicted digits for the image 3: 0 with strength 59.278 and entropy 0.759.
    Ground truth:  7 2 1 0
    Predicted digits for the image 0: 1 with strength 37.731 and entropy 1.073.
    Predicted digits for the image 1: 4 with strength 12.301 and entropy 2.199.
    Predicted digits for the image 2: 4 with strength 10.810 and entropy 2.281.
    Predicted digits for the image 3: 0 with strength 10.000 and entropy 2.303.
    Ground truth:  7 2 1 0
    Predicted digits for the image 0: 7 with strength 35.862 and entropy 1.496.
    Predicted digits for the image 1: 4 with strength 13.919 and entropy 2.147.
    Predicted digits for the image 2: 7 with strength 66.477 and entropy 0.694.
    Predicted digits for the image 3: 0 with strength 34.456 and entropy 1.148.




.. GENERATED FROM PYTHON SOURCE LINES 148-152

References
----------

- **Deep Evidential Classification:** Murat Sensoy, Lance Kaplan, & Melih Kandemir (2018). Evidential Deep Learning to Quantify Classification Uncertainty `NeurIPS 2018 <https://arxiv.org/pdf/1806.01768>`_.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 5.979 seconds)


.. _sphx_glr_download_auto_tutorials_Classification_tutorial_evidential_classification.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_evidential_classification.ipynb <tutorial_evidential_classification.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_evidential_classification.py <tutorial_evidential_classification.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: tutorial_evidential_classification.zip <tutorial_evidential_classification.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
