
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/tutorial_probabilistic_regression.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_tutorial_probabilistic_regression.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_tutorial_probabilistic_regression.py:


Deep Probabilistic Regression
=============================

This tutorial aims to provide an overview of some utilities in TorchUncertainty for probabilistic regression.

Building a MLP for Probabilistic Regression using TorchUncertainty distribution layers
--------------------------------------------------------------------------------------

In this section we cover the building of a very simple MLP outputting Normal distribution parameters.

1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

We disable some logging and warnings to keep the output clean.

.. GENERATED FROM PYTHON SOURCE LINES 18-27

.. code-block:: Python

    import torch
    from torch import nn

    import logging
    logging.getLogger("lightning.pytorch.utilities.rank_zero").setLevel(logging.WARNING)

    import warnings
    warnings.filterwarnings("ignore")








.. GENERATED FROM PYTHON SOURCE LINES 28-34

2. Building the MLP model
~~~~~~~~~~~~~~~~~~~~~~~~~

To create a MLP model estimating a Normal distribution, we use the NormalLinear layer.
This layer is a wrapper around the nn.Linear layer, which outputs the location and scale of a Normal distribution.
Note that any other distribution layer from TU can be used in the same way.

.. GENERATED FROM PYTHON SOURCE LINES 34-51

.. code-block:: Python

    from torch_uncertainty.layers.distributions import NormalLinear


    class MLP(nn.Module):
        def __init__(self, in_features: int, out_features: int):
            super().__init__()
            self.fc1 = nn.Linear(in_features, 50)
            self.fc2 = NormalLinear(
                base_layer=nn.Linear,
                event_dim=out_features,
                in_features=50,
            )

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            return self.fc2(x)








.. GENERATED FROM PYTHON SOURCE LINES 52-56

3. Setting up the data
~~~~~~~~~~~~~~~~~~~~~~

We use the UCI Kin8nm dataset, which is a regression dataset with 8 features and 8192 samples.

.. GENERATED FROM PYTHON SOURCE LINES 56-65

.. code-block:: Python

    from torch_uncertainty.datamodules import UCIRegressionDataModule

    # datamodule
    datamodule = UCIRegressionDataModule(
        root="data",
        batch_size=32,
        dataset_name="kin8nm",
    )








.. GENERATED FROM PYTHON SOURCE LINES 66-68

4. Setting up the model and trainer
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 68-80

.. code-block:: Python


    from torch_uncertainty import TUTrainer

    trainer = TUTrainer(
        accelerator="cpu",
        max_epochs=5,
        enable_progress_bar=False,
    )

    model = MLP(in_features=8, out_features=1)









.. GENERATED FROM PYTHON SOURCE LINES 81-88

5. The Loss, the Optimizer and the Training Routine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We use the DistributionNLLLoss to compute the negative log-likelihood of the Normal distribution.
Note that this loss can be used with any Distribution from torch.distributions.
For the optimizer, we use the Adam optimizer with a learning rate of 5e-3.
Finally, we create a RegressionRoutine to train the model. We indicate that the output dimension is 1 and the distribution family is "normal".

.. GENERATED FROM PYTHON SOURCE LINES 88-113

.. code-block:: Python


    from torch_uncertainty.losses import DistributionNLLLoss
    from torch_uncertainty.routines import RegressionRoutine

    loss = DistributionNLLLoss()

    def optim_regression(
        model: nn.Module,
        learning_rate: float = 5e-3,
    ):
        return torch.optim.Adam(
            model.parameters(),
            lr=learning_rate,
            weight_decay=0,
        )

    routine = RegressionRoutine(
        output_dim=1,
        model=model,
        loss=loss,
        optim_recipe=optim_regression(model),
        dist_family="normal",
    )









.. GENERATED FROM PYTHON SOURCE LINES 114-116

6. Training the model
~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 116-120

.. code-block:: Python


    trainer.fit(model=routine, datamodule=datamodule)
    results = trainer.test(model=routine, datamodule=datamodule)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0.00/1.13M [00:00<?, ?B/s]    100%|██████████| 1.13M/1.13M [00:00<00:00, 11.8MB/s]
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Regression         ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     MAE      │          0.31242          │
    │     MSE      │          0.18321          │
    │     NLL      │          0.42442          │
    │     RMSE     │          0.42803          │
    └──────────────┴───────────────────────────┘




.. GENERATED FROM PYTHON SOURCE LINES 121-127

7. Benchmarking different distributions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Our MLP model assumes a Normal distribution as the output. However, we could be interested in comparing the performance of different distributions.
TorchUncertainty provides a simple way to do this using the get_dist_linear_layer() function.
Let us rewrite the MLP model to use it.

.. GENERATED FROM PYTHON SOURCE LINES 127-145

.. code-block:: Python


    from torch_uncertainty.layers.distributions import get_dist_linear_layer

    class MLP(nn.Module):
        def __init__(self, in_features: int, out_features: int, dist_family: str):
            super().__init__()
            self.fc1 = nn.Linear(in_features, 50)
            dist_layer = get_dist_linear_layer(dist_family)
            self.fc2 = dist_layer(
                base_layer=nn.Linear,
                event_dim=out_features,
                in_features=50,
            )

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            return self.fc2(x)








.. GENERATED FROM PYTHON SOURCE LINES 146-150

We can now train the model with different distributions.
Let us train the model with a Normal, Laplace, Student's t, and Cauchy distribution.
Note that we use the mode as the point-wise estimate of the distribution as the mean
is not defined for the Cauchy distribution.

.. GENERATED FROM PYTHON SOURCE LINES 150-171

.. code-block:: Python

    for dist_family in ["normal", "laplace", "student", "cauchy"]:
        print("#" * 50)
        print(f">>> Training with {dist_family} distribution")
        print("#" * 50)
        trainer = TUTrainer(
            accelerator="cpu",
            max_epochs=10,
            enable_model_summary=False,
            enable_progress_bar=False,
        )
        model = MLP(in_features=8, out_features=1, dist_family=dist_family)
        routine = RegressionRoutine(
            output_dim=1,
            model=model,
            loss=loss,
            optim_recipe=optim_regression(model),
            dist_family=dist_family,
            dist_estimate="mode",
        )
        trainer.fit(model=routine, datamodule=datamodule)
        trainer.test(model=routine, datamodule=datamodule)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ##################################################
    >>> Training with normal distribution
    ##################################################
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Regression         ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     MAE      │          0.28080          │
    │     MSE      │          0.14585          │
    │     NLL      │          0.30855          │
    │     RMSE     │          0.38190          │
    └──────────────┴───────────────────────────┘
    ##################################################
    >>> Training with laplace distribution
    ##################################################
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Regression         ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     MAE      │          0.32945          │
    │     MSE      │          0.23991          │
    │     NLL      │          0.44365          │
    │     RMSE     │          0.48981          │
    └──────────────┴───────────────────────────┘
    ##################################################
    >>> Training with student distribution
    ##################################################
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Regression         ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     MAE      │          0.29378          │
    │     MSE      │          0.17029          │
    │     NLL      │          0.39866          │
    │     RMSE     │          0.41266          │
    └──────────────┴───────────────────────────┘
    ##################################################
    >>> Training with cauchy distribution
    ##################################################
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Regression         ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     MAE      │          0.27116          │
    │     MSE      │          0.14282          │
    │     NLL      │          0.43879          │
    │     RMSE     │          0.37792          │
    └──────────────┴───────────────────────────┘





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 19.365 seconds)


.. _sphx_glr_download_auto_tutorials_tutorial_probabilistic_regression.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_probabilistic_regression.ipynb <tutorial_probabilistic_regression.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_probabilistic_regression.py <tutorial_probabilistic_regression.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: tutorial_probabilistic_regression.zip <tutorial_probabilistic_regression.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
