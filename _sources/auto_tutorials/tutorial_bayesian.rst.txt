
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/tutorial_bayesian.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_tutorial_bayesian.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_tutorial_bayesian.py:


Train a Bayesian Neural Network in Three Minutes
================================================

In this tutorial, we will train a Bayesian Neural Network (BNN) LeNet classifier on the MNIST dataset.

Foreword on Bayesian Neural Networks
------------------------------------

Bayesian Neural Networks (BNNs) are a class of neural networks that can estimate the uncertainty of their predictions via uncertainty on their weights. This is achieved by considering the weights of the neural network as random variables, and by learning their posterior distribution. This is in contrast to standard neural networks, which only learn a single set of weights, which can be seen as Dirac distributions on the weights.

For more information on Bayesian Neural Networks, we refer the reader to the following resources:

- Weight Uncertainty in Neural Networks `ICML2015 <https://arxiv.org/pdf/1505.05424.pdf>`_
- Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users `IEEE Computational Intelligence Magazine <https://arxiv.org/pdf/2007.06823.pdf>`_

Training a Bayesian LeNet using TorchUncertainty models and PyTorch Lightning
-----------------------------------------------------------------------------

In this part, we train a bayesian LeNet, based on the model and routines already implemented in TU.

1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

To train a BNN using TorchUncertainty, we have to load the following utilities from TorchUncertainty:

- the cli handler: cli_main and argument parser: init_args
- the model: bayesian_lenet, which lies in the torch_uncertainty.model module
- the classification training routine in the torch_uncertainty.training.classification module
- the bayesian objective: the ELBOLoss, which lies in the torch_uncertainty.losses file
- the datamodule that handles dataloaders: MNISTDataModule, which lies in the torch_uncertainty.datamodule

.. GENERATED FROM PYTHON SOURCE LINES 36-43

.. code-block:: default


    from torch_uncertainty import cli_main, init_args
    from torch_uncertainty.datamodules import MNISTDataModule
    from torch_uncertainty.losses import ELBOLoss
    from torch_uncertainty.models.lenet import bayesian_lenet
    from torch_uncertainty.routines.classification import ClassificationSingle








.. GENERATED FROM PYTHON SOURCE LINES 44-50

We will also need to define an optimizer using torch.optim as well as the 
neural network utils withing torch.nn, as well as the partial util to provide
the modified default arguments for the ELBO loss.

We also import ArgvContext to avoid using the jupyter arguments as cli
arguments, and therefore avoid errors.

.. GENERATED FROM PYTHON SOURCE LINES 50-59

.. code-block:: default


    import os
    from functools import partial
    from pathlib import Path

    import torch.nn as nn
    import torch.optim as optim
    from cli_test_helpers import ArgvContext








.. GENERATED FROM PYTHON SOURCE LINES 60-63

2. Creating the Optimizer Wrapper
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
We will use the Adam optimizer with the default learning rate of 0.001.

.. GENERATED FROM PYTHON SOURCE LINES 63-71

.. code-block:: default


    def optim_lenet(model: nn.Module) -> dict:
        optimizer = optim.Adam(
            model.parameters(),
            lr=1e-3,
        )
        return {"optimizer": optimizer}








.. GENERATED FROM PYTHON SOURCE LINES 72-80

3. Creating the necessary variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the following, we will need to define the root of the datasets and the
logs, and to fake-parse the arguments needed for using the PyTorch Lightning
Trainer. We also create the datamodule that handles the MNIST dataset,
dataloaders and transforms. Finally, we create the model using the
blueprint from torch_uncertainty.models. 

.. GENERATED FROM PYTHON SOURCE LINES 80-98

.. code-block:: default


    root = Path(os.path.abspath(""))

    with ArgvContext("--max_epochs 1"):
        args = init_args(datamodule=MNISTDataModule)
        args.enable_progress_bar = False
        args.verbose = False
        args.max_epochs = 1

    net_name = "bayesian-lenet-mnist"

    # datamodule
    args.root = str(root / "data")
    dm = MNISTDataModule(**vars(args))

    # model
    model = bayesian_lenet(dm.num_channels, dm.num_classes)








.. GENERATED FROM PYTHON SOURCE LINES 99-109

4. The Loss and the Training Routine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Then, we just have to define the loss to be used during training. To do this,
we redefine the default parameters from the ELBO loss using the partial
function from functools. We use the hyperparameters proposed in the blitz
library. As we are train a classification model, we use the CrossEntropyLoss
as the likelihood.
We then define the training routine using the classification training routine
from torch_uncertainty.training.classification. We provide the model, the ELBO
loss and the optimizer, as well as all the default arguments.

.. GENERATED FROM PYTHON SOURCE LINES 109-127

.. code-block:: default


    loss = partial(
        ELBOLoss,
        model=model,
        criterion=nn.CrossEntropyLoss(),
        kl_weight=1 / 50000,
        num_samples=3,
    )

    baseline = ClassificationSingle(
        model=model,
        num_classes=dm.num_classes,
        in_channels=dm.num_channels,
        loss=loss,
        optimization_procedure=optim_lenet,
        **vars(args),
    )








.. GENERATED FROM PYTHON SOURCE LINES 128-138

5. Gathering Everything and Training the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now that we have prepared all of this, we just have to gather everything in
the main function and to train the model using the PyTorch Lightning Trainer.
Specifically, it needs the baseline, that includes the model as well as the
training routine, the datamodule, the root for the datasets and the logs, the
name of the model for the logs and all the training arguments.
The dataset will be downloaded automatically in the root/data folder, and the
logs will be saved in the root/logs folder.

.. GENERATED FROM PYTHON SOURCE LINES 138-141

.. code-block:: default


    results = cli_main(baseline, dm, root, net_name, args)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw/train-images-idx3-ubyte.gz
      0%|          | 0/9912422 [00:00<?, ?it/s]     66%|██████▋   | 6586368/9912422 [00:00<00:00, 65189974.75it/s]    100%|██████████| 9912422/9912422 [00:00<00:00, 84350378.77it/s]
    Extracting /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw/train-labels-idx1-ubyte.gz
      0%|          | 0/28881 [00:00<?, ?it/s]    100%|██████████| 28881/28881 [00:00<00:00, 35471652.66it/s]
    Extracting /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw/t10k-images-idx3-ubyte.gz
      0%|          | 0/1648877 [00:00<?, ?it/s]    100%|██████████| 1648877/1648877 [00:00<00:00, 19162318.12it/s]
    Extracting /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw/t10k-labels-idx1-ubyte.gz
      0%|          | 0/4542 [00:00<?, ?it/s]    100%|██████████| 4542/4542 [00:00<00:00, 29218602.40it/s]
    Extracting /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/MNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw/train-images-idx3-ubyte.gz
      0%|          | 0/26421880 [00:00<?, ?it/s]      0%|          | 32768/26421880 [00:00<01:37, 271707.62it/s]      0%|          | 65536/26421880 [00:00<01:37, 270929.73it/s]      0%|          | 131072/26421880 [00:00<01:06, 394081.95it/s]      1%|          | 229376/26421880 [00:00<00:46, 559007.14it/s]      2%|▏         | 491520/26421880 [00:00<00:22, 1137649.37it/s]      4%|▎         | 950272/26421880 [00:00<00:12, 2036711.84it/s]      7%|▋         | 1933312/26421880 [00:00<00:06, 4024186.82it/s]     15%|█▍        | 3833856/26421880 [00:00<00:02, 7737519.23it/s]     26%|██▋       | 6979584/26421880 [00:01<00:01, 13428466.24it/s]     38%|███▊      | 10092544/26421880 [00:01<00:00, 17215857.92it/s]     50%|█████     | 13238272/26421880 [00:01<00:00, 19876047.34it/s]     61%|██████    | 16121856/26421880 [00:01<00:00, 21029838.13it/s]     72%|███████▏  | 19136512/26421880 [00:01<00:00, 22184412.05it/s]     84%|████████▍ | 22249472/26421880 [00:01<00:00, 23218310.42it/s]     96%|█████████▌| 25395200/26421880 [00:01<00:00, 24041281.32it/s]    100%|██████████| 26421880/26421880 [00:01<00:00, 14501409.69it/s]
    Extracting /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz
      0%|          | 0/29515 [00:00<?, ?it/s]    100%|██████████| 29515/29515 [00:00<00:00, 241408.74it/s]    100%|██████████| 29515/29515 [00:00<00:00, 240782.83it/s]
    Extracting /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz
      0%|          | 0/4422102 [00:00<?, ?it/s]      1%|          | 32768/4422102 [00:00<00:16, 273381.42it/s]      1%|▏         | 65536/4422102 [00:00<00:15, 273219.34it/s]      3%|▎         | 131072/4422102 [00:00<00:10, 397822.55it/s]      5%|▌         | 229376/4422102 [00:00<00:07, 563758.99it/s]     11%|█         | 491520/4422102 [00:00<00:03, 1147309.97it/s]     21%|██▏       | 950272/4422102 [00:00<00:01, 2056171.52it/s]     44%|████▎     | 1933312/4422102 [00:00<00:00, 4057633.56it/s]     87%|████████▋ | 3833856/4422102 [00:00<00:00, 7798545.11it/s]    100%|██████████| 4422102/4422102 [00:00<00:00, 4596439.53it/s]
    Extracting /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
      0%|          | 0/5148 [00:00<?, ?it/s]    100%|██████████| 5148/5148 [00:00<00:00, 40510838.63it/s]
    Extracting /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/runner/work/torch-uncertainty/torch-uncertainty/auto_tutorials_source/data/FashionMNIST/raw

    /home/runner/work/torch-uncertainty/torch-uncertainty/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(_create_warning_msg(
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
           Test metric             DataLoader 0
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
           hp/test_acc          0.9420999884605408
          hp/test_brier         0.09126735478639603
           hp/test_ece         0.025074046105146408
       hp/test_entropy_id       0.25903555750846863
           hp/test_nll          0.19729553163051605
    ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────




.. GENERATED FROM PYTHON SOURCE LINES 142-146

6. Testing the Model
~~~~~~~~~~~~~~~~~~~~

Now that the model is trained, let's test it on MNIST

.. GENERATED FROM PYTHON SOURCE LINES 146-175

.. code-block:: default


    import matplotlib.pyplot as plt
    import torch
    import torchvision

    import numpy as np


    def imshow(img):
        npimg = img.numpy()
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.show()

    dataiter = iter(dm.val_dataloader())
    images, labels = next(dataiter)

    # print images
    imshow(torchvision.utils.make_grid(images[:4, ...]))
    print('Ground truth: ', ' '.join(f'{labels[j]}' for j in range(4)))

    logits = model(images)
    probs = torch.nn.functional.softmax(logits, dim=-1)

    _, predicted = torch.max(probs, 1)

    print(
        'Predicted digits: ', ' '.join(f'{predicted[j]}' for j in range(4))
    )




.. image-sg:: /auto_tutorials/images/sphx_glr_tutorial_bayesian_001.png
   :alt: tutorial bayesian
   :srcset: /auto_tutorials/images/sphx_glr_tutorial_bayesian_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    Ground truth:  7 2 1 0
    Predicted digits:  7 2 1 0




.. GENERATED FROM PYTHON SOURCE LINES 176-183

References
----------

- **LeNet & MNIST:** LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. `Proceedings of the IEEE <http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf>`_
- **Bayesian Neural Networks:** Weight Uncertainty in Neural Networks `ICML2015 <https://arxiv.org/pdf/1505.05424.pdf>`_
- **The Adam optimizer:** Kingma, Diederik P., and Jimmy Ba. "Adam: A method for stochastic optimization." `ICLR 2015 <https://arxiv.org/pdf/1412.6980.pdf>`_
- **The Blitz** `library <https://github.com/piEsposito/blitz-bayesian-deep-learning>`_ (for the hyperparameters)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  39.400 seconds)


.. _sphx_glr_download_auto_tutorials_tutorial_bayesian.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_bayesian.py <tutorial_bayesian.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_bayesian.ipynb <tutorial_bayesian.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
