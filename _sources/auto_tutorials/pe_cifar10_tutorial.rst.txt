
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/pe_cifar10_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_pe_cifar10_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_pe_cifar10_tutorial.py:


From a Vanilla Classifier to a Packed-Ensemble
==============================================

Let's dive step by step into the process to modify a vanilla classifier into a
packed-ensemble classifier.

Dataset
-------

In this tutorial we will use the CIFAR10 dataset available in the torchvision
package. The CIFAR10 dataset consists of 60000 32x32 colour images in 10
classes, with 6000 images per class. There are 50000 training images and 10000
test images.

Here is an example of what the data looks like:

.. figure:: /_static/img/cifar10.png
   :alt: cifar10

   cifar10

Training a image Packed-Ensemble classifier
-------------------------------------------

Here is the outline of the process:

1. Load and normalizing the CIFAR10 training and test datasets using
   ``torchvision``
2. Define a Packed-Ensemble from a vanilla classifier
3. Define a loss function
4. Train the Packed-Ensemble on the training data
5. Test the Packed-Ensemble on the test data and evaluate its performance
   w.r.t. uncertainty quantification and OOD detection

1. Load and normalize CIFAR10
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 42-46

.. code-block:: default

    import torch
    import torchvision
    import torchvision.transforms as transforms








.. GENERATED FROM PYTHON SOURCE LINES 47-49

The output of torchvision datasets are PILImage images of range [0, 1].
We transform them to Tensors of normalized range [-1, 1].

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. note::
    If running on Windows and you get a BrokenPipeError, try setting
    the num_worker of torch.utils.data.DataLoader() to 0.

.. GENERATED FROM PYTHON SOURCE LINES 54-74

.. code-block:: default


    transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    batch_size = 4

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                            download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                              shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                           download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                             shuffle=False, num_workers=2)

    classes = ('plane', 'car', 'bird', 'cat',
               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
      0%|          | 0/170498071 [00:00<?, ?it/s]      0%|          | 688128/170498071 [00:00<00:24, 6846144.23it/s]      2%|2         | 4161536/170498071 [00:00<00:07, 22920351.43it/s]      4%|4         | 7176192/170498071 [00:00<00:06, 25962090.33it/s]      6%|6         | 10813440/170498071 [00:00<00:05, 29881187.14it/s]      8%|8         | 13828096/170498071 [00:00<00:05, 29285715.49it/s]     10%|#         | 17432576/170498071 [00:00<00:04, 31396849.17it/s]     12%|#2        | 20578304/170498071 [00:00<00:05, 29980083.89it/s]     14%|#4        | 24084480/170498071 [00:00<00:04, 31437652.72it/s]     16%|#5        | 27262976/170498071 [00:00<00:04, 29938134.83it/s]     18%|#7        | 30572544/170498071 [00:01<00:04, 30691305.09it/s]     20%|#9        | 33685504/170498071 [00:01<00:04, 29166664.17it/s]     21%|##1       | 36634624/170498071 [00:01<00:04, 27683689.16it/s]     23%|##3       | 39452672/170498071 [00:01<00:05, 25378336.80it/s]     25%|##4       | 42041344/170498071 [00:01<00:05, 23537510.22it/s]     26%|##6       | 44466176/170498071 [00:01<00:05, 21471214.80it/s]     27%|##7       | 46661632/170498071 [00:01<00:06, 20230997.45it/s]     29%|##8       | 48726016/170498071 [00:01<00:06, 19606326.35it/s]     30%|##9       | 50724864/170498071 [00:02<00:06, 18664737.17it/s]     31%|###       | 52625408/170498071 [00:02<00:06, 18547284.51it/s]     32%|###1      | 54493184/170498071 [00:02<00:06, 18012875.56it/s]     33%|###3      | 56328192/170498071 [00:02<00:06, 17978556.19it/s]     34%|###4      | 58130432/170498071 [00:02<00:06, 17635731.87it/s]     35%|###5      | 59965440/170498071 [00:02<00:06, 17705152.09it/s]     36%|###6      | 61767680/170498071 [00:02<00:06, 17483456.79it/s]     37%|###7      | 63537152/170498071 [00:02<00:06, 17541878.98it/s]     38%|###8      | 65306624/170498071 [00:02<00:06, 17473266.17it/s]     39%|###9      | 67076096/170498071 [00:03<00:05, 17298040.26it/s]     40%|####      | 68943872/170498071 [00:03<00:05, 17521060.21it/s]     42%|####1     | 71008256/170498071 [00:03<00:05, 18403607.63it/s]     43%|####2     | 73170944/170498071 [00:03<00:05, 19214914.14it/s]     44%|####4     | 75169792/170498071 [00:03<00:04, 19396881.86it/s]     45%|####5     | 77135872/170498071 [00:03<00:04, 18807328.23it/s]     46%|####6     | 79036416/170498071 [00:03<00:04, 18389281.04it/s]     47%|####7     | 80904192/170498071 [00:03<00:04, 18034320.92it/s]     49%|####8     | 82739200/170498071 [00:03<00:04, 17793481.29it/s]     50%|####9     | 84541440/170498071 [00:03<00:04, 17758181.65it/s]     51%|#####     | 86343680/170498071 [00:04<00:04, 17578841.86it/s]     52%|#####1    | 88113152/170498071 [00:04<00:04, 17494660.42it/s]     53%|#####2    | 89882624/170498071 [00:04<00:04, 17408578.46it/s]     54%|#####3    | 91652096/170498071 [00:04<00:04, 17490000.56it/s]     55%|#####4    | 93421568/170498071 [00:04<00:04, 17352080.57it/s]     56%|#####5    | 95158272/170498071 [00:04<00:04, 17333615.67it/s]     57%|#####6    | 96894976/170498071 [00:04<00:04, 17275838.71it/s]     58%|#####7    | 98631680/170498071 [00:04<00:04, 17267723.04it/s]     59%|#####8    | 100368384/170498071 [00:04<00:04, 17269086.16it/s]     60%|#####9    | 102137856/170498071 [00:04<00:03, 17216412.63it/s]     61%|######    | 103907328/170498071 [00:05<00:03, 17257040.84it/s]     62%|######1   | 105644032/170498071 [00:05<00:03, 16802972.70it/s]     63%|######2   | 107347968/170498071 [00:05<00:03, 16226258.14it/s]     64%|######3   | 108986368/170498071 [00:05<00:03, 15714735.38it/s]     65%|######4   | 110592000/170498071 [00:05<00:03, 15487095.03it/s]     66%|######5   | 112164864/170498071 [00:05<00:03, 15371414.75it/s]     67%|######6   | 113704960/170498071 [00:05<00:03, 15223431.82it/s]     68%|######7   | 115245056/170498071 [00:05<00:03, 15116224.34it/s]     68%|######8   | 116785152/170498071 [00:05<00:03, 14678217.53it/s]     69%|######9   | 118259712/170498071 [00:06<00:03, 14285852.53it/s]     70%|#######   | 119701504/170498071 [00:06<00:03, 13791379.18it/s]     71%|#######1  | 121110528/170498071 [00:06<00:03, 13561221.80it/s]     72%|#######1  | 122486784/170498071 [00:06<00:03, 13496804.09it/s]     73%|#######2  | 123863040/170498071 [00:06<00:03, 13266086.10it/s]     73%|#######3  | 125206528/170498071 [00:06<00:03, 13174785.41it/s]     74%|#######4  | 126550016/170498071 [00:06<00:03, 13137558.13it/s]     75%|#######5  | 127893504/170498071 [00:06<00:03, 13057554.20it/s]     76%|#######5  | 129236992/170498071 [00:06<00:03, 13064323.87it/s]     77%|#######6  | 130580480/170498071 [00:06<00:03, 13025572.36it/s]     77%|#######7  | 131923968/170498071 [00:07<00:02, 13036666.68it/s]     78%|#######8  | 133267456/170498071 [00:07<00:02, 12991575.49it/s]     79%|#######8  | 134610944/170498071 [00:07<00:02, 13032794.77it/s]     80%|#######9  | 135954432/170498071 [00:07<00:02, 13013589.26it/s]     81%|########  | 137396224/170498071 [00:07<00:02, 13406058.05it/s]     81%|########1 | 138870784/170498071 [00:07<00:02, 13757999.58it/s]     82%|########2 | 140345344/170498071 [00:07<00:02, 14020343.84it/s]     83%|########3 | 141819904/170498071 [00:07<00:02, 14200677.51it/s]     84%|########4 | 143294464/170498071 [00:07<00:01, 14320785.88it/s]     85%|########4 | 144736256/170498071 [00:08<00:01, 14332599.94it/s]     86%|########5 | 146243584/170498071 [00:08<00:01, 14496327.79it/s]     87%|########6 | 147718144/170498071 [00:08<00:01, 14528454.21it/s]     88%|########7 | 149192704/170498071 [00:08<00:01, 14552318.93it/s]     88%|########8 | 150667264/170498071 [00:08<00:01, 14580857.71it/s]     89%|########9 | 152141824/170498071 [00:08<00:01, 14594143.83it/s]     90%|######### | 153616384/170498071 [00:08<00:01, 14593680.49it/s]     91%|#########1| 155189248/170498071 [00:08<00:01, 14894658.70it/s]     92%|#########2| 156893184/170498071 [00:08<00:00, 15531206.59it/s]     93%|#########3| 158597120/170498071 [00:08<00:00, 15816680.44it/s]     94%|#########4| 160301056/170498071 [00:09<00:00, 16052488.88it/s]     95%|#########5| 162004992/170498071 [00:09<00:00, 16314923.86it/s]     96%|#########5| 163676160/170498071 [00:09<00:00, 16429777.74it/s]     97%|#########6| 165347328/170498071 [00:09<00:00, 16448670.53it/s]     98%|#########7| 167018496/170498071 [00:09<00:00, 15881534.32it/s]     99%|#########8| 168624128/170498071 [00:09<00:00, 15490008.21it/s]    100%|#########9| 170196992/170498071 [00:09<00:00, 15157034.07it/s]    100%|##########| 170498071/170498071 [00:09<00:00, 17608285.13it/s]
    Extracting ./data/cifar-10-python.tar.gz to ./data
    Files already downloaded and verified




.. GENERATED FROM PYTHON SOURCE LINES 75-76

Let us show some of the training images, for fun.

.. GENERATED FROM PYTHON SOURCE LINES 76-101

.. code-block:: default


    import matplotlib.pyplot as plt

    import numpy as np

    # functions to show an image


    def imshow(img):
        img = img / 2 + 0.5     # unnormalize
        npimg = img.numpy()
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.show()


    # get some random training images
    dataiter = iter(trainloader)
    images, labels = next(dataiter)

    # show images
    imshow(torchvision.utils.make_grid(images))
    # print labels
    print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))





.. image-sg:: /auto_tutorials/images/sphx_glr_pe_cifar10_tutorial_001.png
   :alt: pe cifar10 tutorial
   :srcset: /auto_tutorials/images/sphx_glr_pe_cifar10_tutorial_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    horse car   horse dog  




.. GENERATED FROM PYTHON SOURCE LINES 102-106

2. Define a Packed-Ensemble from a vanilla classifier
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
First we define a vanilla classifier for CIFAR10 for reference. We will use a
convolutional neural network.

.. GENERATED FROM PYTHON SOURCE LINES 106-132

.. code-block:: default


    import torch.nn as nn
    import torch.nn.functional as F


    class Net(nn.Module):
          def __init__(self):
             super(Net, self).__init__()
             self.conv1 = nn.Conv2d(3, 6, 5)
             self.pool = nn.MaxPool2d(2, 2)
             self.conv2 = nn.Conv2d(6, 16, 5)
             self.fc1 = nn.Linear(16 * 5 * 5, 120)
             self.fc2 = nn.Linear(120, 84)
             self.fc3 = nn.Linear(84, 10)
   
          def forward(self, x):
             x = self.pool(F.relu(self.conv1(x)))
             x = self.pool(F.relu(self.conv2(x)))
             x = x.flatten(1)
             x = F.relu(self.fc1(x))
             x = F.relu(self.fc2(x))
             x = self.fc3(x)
             return x

    net = Net()








.. GENERATED FROM PYTHON SOURCE LINES 133-135

Let's modify the vanilla classifier into a Packed-Ensemble classifier of 
parameters :math:`M=4,\ \alpha=2\text{ and }\gamma=1`.

.. GENERATED FROM PYTHON SOURCE LINES 135-170

.. code-block:: default


    from einops import rearrange
    from torch_uncertainty.layers import PackedConv2d, PackedLinear

    class PackedNet(nn.Module):
        def __init__(self) -> None:
            super().__init__()
            M = 4
            alpha = 2
            gamma = 1
            # The first layer is left as is since all the subnetworks have the
            # input.
            self.conv1 = nn.Conv2d(3, 6*alpha, 5)
            self.pool = nn.MaxPool2d(2, 2)
            self.conv2 = PackedConv2d(6*alpha, 16*alpha, 5, num_estimators=M, groups=gamma)
            self.fc1 = PackedLinear(16 * 5 * 5 * alpha, 120 * alpha, num_estimators=M, groups=gamma)
            self.fc2 = PackedLinear(120 * alpha, 84 * alpha, num_estimators=M, groups=gamma)
            self.fc3 = PackedLinear(84 * alpha, 10 * M, num_estimators=M, groups=gamma)

            self.num_estimators = M

        def forward(self, x):
             x = self.pool(F.relu(self.conv1(x)))
             x = self.pool(F.relu(self.conv2(x)))
             x = rearrange(
                x, "e (m c) h w -> (m e) c h w", m=self.num_estimators
            )
             x = x.flatten(1)
             x = F.relu(self.fc1(x))
             x = F.relu(self.fc2(x))
             x = self.fc3(x)
             return x

    packed_net = PackedNet()








.. GENERATED FROM PYTHON SOURCE LINES 171-174

3. Define a Loss function and optimizer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Let's use a Classification Cross-Entropy loss and SGD with momentum.

.. GENERATED FROM PYTHON SOURCE LINES 174-180

.. code-block:: default


    import torch.optim as optim

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(packed_net.parameters(), lr=0.001, momentum=0.9)








.. GENERATED FROM PYTHON SOURCE LINES 181-184

4. Train the Packed-Ensemble on the training data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Let's train the Packed-Ensemble on the training data.

.. GENERATED FROM PYTHON SOURCE LINES 184-208

.. code-block:: default


    for epoch in range(2):  # loop over the dataset multiple times
        
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data

            # zero the parameter gradients
            optimizer.zero_grad()
            # forward + backward + optimize
            outputs = packed_net(inputs)
            loss = criterion(outputs, labels.repeat(packed_net.num_estimators))
            loss.backward()
            optimizer.step()

            # print statistics
            running_loss += loss.item()
            if i % 2000 == 1999:    # print every 2000 mini-batches
                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
                running_loss = 0.0

    print('Finished Training')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [1,  2000] loss: 2.302
    [1,  4000] loss: 2.279
    [1,  6000] loss: 2.186
    [1,  8000] loss: 2.110
    [1, 10000] loss: 2.050
    [1, 12000] loss: 1.994
    [2,  2000] loss: 1.929
    [2,  4000] loss: 1.848
    [2,  6000] loss: 1.807
    [2,  8000] loss: 1.747
    [2, 10000] loss: 1.724
    [2, 12000] loss: 1.692
    Finished Training




.. GENERATED FROM PYTHON SOURCE LINES 209-210

Save our trained model:

.. GENERATED FROM PYTHON SOURCE LINES 210-214

.. code-block:: default


    PATH = './cifar_packed_net.pth'
    torch.save(packed_net.state_dict(), PATH)








.. GENERATED FROM PYTHON SOURCE LINES 215-218

5. Test the Packed-Ensemble on the test data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Let us display an image from the test set to get familiar.

.. GENERATED FROM PYTHON SOURCE LINES 218-226

.. code-block:: default


    dataiter = iter(testloader)
    images, labels = next(dataiter)

    # print images
    imshow(torchvision.utils.make_grid(images))
    print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))




.. image-sg:: /auto_tutorials/images/sphx_glr_pe_cifar10_tutorial_002.png
   :alt: pe cifar10 tutorial
   :srcset: /auto_tutorials/images/sphx_glr_pe_cifar10_tutorial_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    GroundTruth:  cat   ship  ship  plane




.. GENERATED FROM PYTHON SOURCE LINES 227-229

Next, let us load back in our saved model (note: saving and re-loading the
model wasn't necessary here, we only did it to illustrate how to do so):

.. GENERATED FROM PYTHON SOURCE LINES 229-233

.. code-block:: default


    packed_net = PackedNet()
    packed_net.load_state_dict(torch.load(PATH))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <All keys matched successfully>



.. GENERATED FROM PYTHON SOURCE LINES 234-235

Let us see what the Packed-Ensemble thinks these examples above are:

.. GENERATED FROM PYTHON SOURCE LINES 235-247

.. code-block:: default


    logits = packed_net(images)
    logits = rearrange(logits, "(n b) c -> b n c", n=packed_net.num_estimators)
    probs_per_est = F.softmax(logits, dim=-1)
    outputs = probs_per_est.mean(dim=1)

    _, predicted = torch.max(outputs, 1)

    print(
        'Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(batch_size))
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Predicted:  cat   car   ship  ship 




.. GENERATED FROM PYTHON SOURCE LINES 248-248

The results seem pretty good.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  35.944 seconds)


.. _sphx_glr_download_auto_tutorials_pe_cifar10_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: pe_cifar10_tutorial.py <pe_cifar10_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: pe_cifar10_tutorial.ipynb <pe_cifar10_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
