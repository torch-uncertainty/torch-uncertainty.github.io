
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/tutorial_scaler.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_tutorial_scaler.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_tutorial_scaler.py:


Improve Top-label Calibration with Temperature Scaling
======================================================

In this tutorial, we use *TorchUncertainty* to improve the calibration
of the top-label predictions and the reliability of the underlying neural network.

This tutorial provides extensive details on how to use the TemperatureScaler
class, however, this is done automatically in the classification routine when setting
the `calibration_set` to val or test.

Through this tutorial, we also see how to use the datamodules outside any Lightning trainers,
and how to use TorchUncertainty's models.

1. Loading the Utilities
~~~~~~~~~~~~~~~~~~~~~~~~

In this tutorial, we will need:

- TorchUncertainty's Calibration Error metric to compute to evaluate the top-label calibration with ECE and plot the reliability diagrams
- the CIFAR-100 datamodule to handle the data
- a ResNet 18 as starting model
- the temperature scaler to improve the top-label calibration
- a utility function to download HF models easily

If you use the classification routine, the plots will be automatically available in the tensorboard logs if you use the `log_plots` flag.

.. GENERATED FROM PYTHON SOURCE LINES 28-35

.. code-block:: Python


    from torch_uncertainty.datamodules import CIFAR100DataModule
    from torch_uncertainty.metrics import CalibrationError
    from torch_uncertainty.models.resnet import resnet
    from torch_uncertainty.post_processing import TemperatureScaler
    from torch_uncertainty.utils import load_hf








.. GENERATED FROM PYTHON SOURCE LINES 36-41

2. Loading a model from TorchUncertainty's HF
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To avoid training a model on CIFAR-100 from scratch, we load a model from Hugging Face.
This can be done in a one liner:

.. GENERATED FROM PYTHON SOURCE LINES 41-51

.. code-block:: Python


    # Build the model
    model = resnet(in_channels=3, num_classes=100, arch=18, style="cifar", conv_bias=False)

    # Download the weights (the config is not used here)
    weights, config = load_hf("resnet18_c100")

    # Load the weights in the pre-built model
    model.load_state_dict(weights)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <All keys matched successfully>



.. GENERATED FROM PYTHON SOURCE LINES 52-59

3. Setting up the Datamodule and Dataloaders
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To get the dataloader from the datamodule, just call prepare_data, setup, and
extract the first element of the test dataloader list. There are more than one
element if eval_ood is True: the dataloader of in-distribution data and the dataloader
of out-of-distribution data. Otherwise, it is a list of 1 element.

.. GENERATED FROM PYTHON SOURCE LINES 59-67

.. code-block:: Python


    dm = CIFAR100DataModule(root="./data", eval_ood=False, batch_size=32)
    dm.prepare_data()
    dm.setup("test")

    # Get the full test dataloader (unused in this tutorial)
    dataloader = dm.test_dataloader()[0]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz
      0%|          | 0/169001437 [00:00<?, ?it/s]      0%|          | 32768/169001437 [00:00<11:07, 253288.13it/s]      0%|          | 196608/169001437 [00:00<03:03, 918161.20it/s]      0%|          | 557056/169001437 [00:00<01:23, 2019260.30it/s]      1%|          | 1703936/169001437 [00:00<00:32, 5189254.58it/s]      3%|▎         | 4685824/169001437 [00:00<00:12, 13351866.73it/s]      5%|▌         | 8454144/169001437 [00:00<00:07, 21141529.12it/s]      7%|▋         | 12517376/169001437 [00:00<00:05, 27234419.85it/s]     10%|▉         | 16416768/169001437 [00:00<00:04, 30801990.14it/s]     12%|█▏        | 20447232/169001437 [00:00<00:04, 33697096.11it/s]     15%|█▍        | 24576000/169001437 [00:01<00:04, 35980829.63it/s]     17%|█▋        | 28540928/169001437 [00:01<00:03, 36996978.37it/s]     19%|█▉        | 32571392/169001437 [00:01<00:03, 37930328.53it/s]     22%|██▏       | 36667392/169001437 [00:01<00:03, 38786750.19it/s]     24%|██▍       | 40665088/169001437 [00:01<00:03, 39119561.27it/s]     26%|██▋       | 44695552/169001437 [00:01<00:03, 39396536.88it/s]     29%|██▉       | 48726016/169001437 [00:01<00:03, 39640072.41it/s]     31%|███       | 52723712/169001437 [00:01<00:02, 39481981.19it/s]     34%|███▎      | 56688640/169001437 [00:01<00:02, 39160024.65it/s]     36%|███▌      | 60620800/169001437 [00:01<00:02, 38040657.91it/s]     38%|███▊      | 64454656/169001437 [00:02<00:02, 35051818.55it/s]     40%|████      | 68026368/169001437 [00:02<00:03, 32465340.08it/s]     42%|████▏     | 71335936/169001437 [00:02<00:03, 31508758.19it/s]     44%|████▍     | 74612736/169001437 [00:02<00:02, 31755802.92it/s]     46%|████▌     | 77824000/169001437 [00:02<00:02, 30424886.32it/s]     48%|████▊     | 81690624/169001437 [00:02<00:02, 32575107.14it/s]     51%|█████     | 85852160/169001437 [00:02<00:02, 34979114.68it/s]     53%|█████▎    | 89948160/169001437 [00:02<00:02, 36664860.59it/s]     56%|█████▌    | 93945856/169001437 [00:02<00:01, 37605362.72it/s]     58%|█████▊    | 97845248/169001437 [00:03<00:01, 37952751.72it/s]     60%|██████    | 101744640/169001437 [00:03<00:01, 38256102.69it/s]     63%|██████▎   | 105775104/169001437 [00:03<00:01, 38828855.86it/s]     65%|██████▌   | 109903872/169001437 [00:03<00:01, 39513884.50it/s]     67%|██████▋   | 114032640/169001437 [00:03<00:01, 39927793.50it/s]     70%|██████▉   | 118063104/169001437 [00:03<00:01, 39780413.86it/s]     72%|███████▏  | 122060800/169001437 [00:03<00:01, 39612194.88it/s]     75%|███████▍  | 126091264/169001437 [00:03<00:01, 39719814.13it/s]     77%|███████▋  | 130088960/169001437 [00:03<00:00, 39168896.82it/s]     79%|███████▉  | 134348800/169001437 [00:03<00:00, 40083157.87it/s]     82%|████████▏ | 138412032/169001437 [00:04<00:00, 40098164.55it/s]     84%|████████▍ | 142508032/169001437 [00:04<00:00, 40201278.86it/s]     87%|████████▋ | 146538496/169001437 [00:04<00:00, 40063410.14it/s]     89%|████████▉ | 150568960/169001437 [00:04<00:00, 39192960.51it/s]     91%|█████████▏| 154501120/169001437 [00:04<00:00, 39095417.37it/s]     94%|█████████▍| 158695424/169001437 [00:04<00:00, 39931739.23it/s]     97%|█████████▋| 163479552/169001437 [00:04<00:00, 41909016.21it/s]    100%|█████████▉| 168165376/169001437 [00:04<00:00, 43342048.18it/s]    100%|██████████| 169001437/169001437 [00:04<00:00, 34995427.28it/s]
    Extracting data/cifar-100-python.tar.gz to data
    Files already downloaded and verified




.. GENERATED FROM PYTHON SOURCE LINES 68-78

4. Iterating on the Dataloader and Computing the ECE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We first split the original test set into a calibration set and a test set for proper evaluation.

When computing the ECE, you need to provide the likelihoods associated with the inputs.
To do this, just call PyTorch's softmax.

To avoid lengthy computations (without GPU), we restrict the calibration computation to a subset
of the test set.

.. GENERATED FROM PYTHON SOURCE LINES 78-100

.. code-block:: Python


    from torch.utils.data import DataLoader, random_split

    # Split datasets
    dataset = dm.test
    cal_dataset, test_dataset, other = random_split(
        dataset, [1000, 1000, len(dataset) - 2000]
    )
    test_dataloader = DataLoader(test_dataset, batch_size=32)

    # Initialize the ECE
    ece = CalibrationError(task="multiclass", num_classes=100)

    # Iterate on the calibration dataloader
    for sample, target in test_dataloader:
        logits = model(sample)
        probs = logits.softmax(-1)
        ece.update(probs, target)

    # Compute & print the calibration error
    print(f"ECE before scaling - {ece.compute():.3%}.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ECE before scaling - 10.233%.




.. GENERATED FROM PYTHON SOURCE LINES 101-103

We also compute and plot the top-label calibration figure. We see that the
model is not well calibrated.

.. GENERATED FROM PYTHON SOURCE LINES 103-106

.. code-block:: Python

    fig, ax = ece.plot()
    fig.show()




.. image-sg:: /auto_tutorials/images/sphx_glr_tutorial_scaler_001.png
   :alt: tutorial scaler
   :srcset: /auto_tutorials/images/sphx_glr_tutorial_scaler_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 107-114

5. Fitting the Scaler to Improve the Calibration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The TemperatureScaler has one parameter that can be used to temper the softmax.
We minimize the tempered cross-entropy on a calibration set that we define here as
a subset of the test set and containing 1000 data. Look at the code run by TemperatureScaler
`fit` method for more details.

.. GENERATED FROM PYTHON SOURCE LINES 114-119

.. code-block:: Python


    # Fit the scaler on the calibration dataset
    scaled_model = TemperatureScaler(model=model)
    scaled_model = scaled_model.fit(calibration_set=cal_dataset)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/32 [00:00<?, ?it/s]      3%|▎         | 1/32 [00:00<00:08,  3.67it/s]      6%|▋         | 2/32 [00:00<00:07,  3.90it/s]      9%|▉         | 3/32 [00:00<00:07,  3.99it/s]     12%|█▎        | 4/32 [00:01<00:06,  4.05it/s]     16%|█▌        | 5/32 [00:01<00:06,  4.06it/s]     19%|█▉        | 6/32 [00:01<00:06,  4.08it/s]     22%|██▏       | 7/32 [00:01<00:06,  4.09it/s]     25%|██▌       | 8/32 [00:01<00:05,  4.10it/s]     28%|██▊       | 9/32 [00:02<00:05,  4.09it/s]     31%|███▏      | 10/32 [00:02<00:05,  4.10it/s]     34%|███▍      | 11/32 [00:02<00:05,  4.10it/s]     38%|███▊      | 12/32 [00:02<00:04,  4.10it/s]     41%|████      | 13/32 [00:03<00:04,  4.07it/s]     44%|████▍     | 14/32 [00:03<00:04,  4.08it/s]     47%|████▋     | 15/32 [00:03<00:04,  4.09it/s]     50%|█████     | 16/32 [00:03<00:03,  4.09it/s]     53%|█████▎    | 17/32 [00:04<00:03,  4.07it/s]     56%|█████▋    | 18/32 [00:04<00:03,  4.08it/s]     59%|█████▉    | 19/32 [00:04<00:03,  4.09it/s]     62%|██████▎   | 20/32 [00:04<00:02,  4.09it/s]     66%|██████▌   | 21/32 [00:05<00:02,  4.07it/s]     69%|██████▉   | 22/32 [00:05<00:02,  4.08it/s]     72%|███████▏  | 23/32 [00:05<00:02,  4.09it/s]     75%|███████▌  | 24/32 [00:05<00:01,  4.09it/s]     78%|███████▊  | 25/32 [00:06<00:01,  4.10it/s]     81%|████████▏ | 26/32 [00:06<00:01,  4.09it/s]     84%|████████▍ | 27/32 [00:06<00:01,  4.09it/s]     88%|████████▊ | 28/32 [00:06<00:00,  4.10it/s]     91%|█████████ | 29/32 [00:07<00:00,  4.10it/s]     94%|█████████▍| 30/32 [00:07<00:00,  4.10it/s]     97%|█████████▋| 31/32 [00:07<00:00,  4.10it/s]    100%|██████████| 32/32 [00:07<00:00,  4.18it/s]




.. GENERATED FROM PYTHON SOURCE LINES 120-127

6. Iterating Again to Compute the Improved ECE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can directly use the scaler as a calibrated model.

Note that you will need to first reset the ECE metric to avoid mixing the scores of
the previous and current iterations.

.. GENERATED FROM PYTHON SOURCE LINES 127-139

.. code-block:: Python


    # Reset the ECE
    ece.reset()

    # Iterate on the test dataloader
    for sample, target in test_dataloader:
        logits = scaled_model(sample)
        probs = logits.softmax(-1)
        ece.update(probs, target)

    print(f"ECE after scaling - {ece.compute():.3%}.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ECE after scaling - 3.450%.




.. GENERATED FROM PYTHON SOURCE LINES 140-142

We finally compute and plot the scaled top-label calibration figure. We see
that the model is now better calibrated.

.. GENERATED FROM PYTHON SOURCE LINES 142-145

.. code-block:: Python

    fig, ax = ece.plot()
    fig.show()




.. image-sg:: /auto_tutorials/images/sphx_glr_tutorial_scaler_002.png
   :alt: tutorial scaler
   :srcset: /auto_tutorials/images/sphx_glr_tutorial_scaler_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 146-154

The top-label calibration should be improved.

Notes
~~~~~

Temperature scaling is very efficient when the calibration set is representative of the test set.
In this case, we say that the calibration and test set are drawn from the same distribution.
However, this may not hold true in real-world cases where dataset shift could happen.

.. GENERATED FROM PYTHON SOURCE LINES 156-161

References
~~~~~~~~~~

- **Expected Calibration Error:** Naeini, M. P., Cooper, G. F., & Hauskrecht, M. (2015). Obtaining Well Calibrated Probabilities Using Bayesian Binning. In `AAAI 2015 <https://arxiv.org/pdf/1411.0160.pdf>`_.
- **Temperature Scaling:** Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. In `ICML 2017 <https://arxiv.org/pdf/1706.04599.pdf>`_.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 33.908 seconds)


.. _sphx_glr_download_auto_tutorials_tutorial_scaler.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_scaler.ipynb <tutorial_scaler.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_scaler.py <tutorial_scaler.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
