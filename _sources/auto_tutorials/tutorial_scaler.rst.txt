
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/tutorial_scaler.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_tutorial_scaler.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_tutorial_scaler.py:


Improve Top-label Calibration with Temperature Scaling
======================================================

In this tutorial, we use torch-uncertainty to improve the calibration of the top-label predictions
to improve the reliability of the underlying neural network.

We also see how to use the datamodules outside any Lightning Trainer.

1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

In this tutorial, we will need:

- torch to download the pretrained model
- the Calibration Error metric to compute the ECE and evaluate the top-label calibration
- the CIFAR-100 datamodule to handle the data
- the Temperature Scaler to improve the top-label calibration

.. GENERATED FROM PYTHON SOURCE LINES 23-30

.. code-block:: default


    import torch
    from torchmetrics import CalibrationError

    from torch_uncertainty.datamodules import CIFAR100DataModule
    from torch_uncertainty.post_processing import TemperatureScaler








.. GENERATED FROM PYTHON SOURCE LINES 31-36

2. Downloading a Pre-trained Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To avoid training a model on CIFAR-100 from scratch, we will use here a model from https://github.com/chenyaofo/pytorch-cifar-models (thank you!)
This can be done in a one liner:

.. GENERATED FROM PYTHON SOURCE LINES 36-39

.. code-block:: default


    model = torch.hub.load("chenyaofo/pytorch-cifar-models", "cifar100_resnet20", pretrained=True)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/torch-uncertainty/torch-uncertainty/.venv/lib/python3.10/site-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour
      warnings.warn(
    Downloading: "https://github.com/chenyaofo/pytorch-cifar-models/zipball/master" to /home/runner/.cache/torch/hub/master.zip
    Downloading: "https://github.com/chenyaofo/pytorch-cifar-models/releases/download/resnet/cifar100_resnet20-23dac2f1.pt" to /home/runner/.cache/torch/hub/checkpoints/cifar100_resnet20-23dac2f1.pt
      0%|          | 0.00/1.11M [00:00<?, ?B/s]    100%|██████████| 1.11M/1.11M [00:00<00:00, 17.6MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 40-46

3. Setting up the Datamodule and Dataloader
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To get the dataloader from the datamodule, just call prepare_data, setup, and 
extract the first element of the test dataloader list. There are more than one 
element if `:attr:ood_detection` is True.

.. GENERATED FROM PYTHON SOURCE LINES 46-54

.. code-block:: default


    dm = CIFAR100DataModule(root="./data", ood_detection=False, batch_size=32)
    dm.prepare_data()
    dm.setup("test")

    # Get the full test dataloader (unused in this tutorial)
    dataloader = dm.test_dataloader()[0]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz
      0%|          | 0/169001437 [00:00<?, ?it/s]      0%|          | 229376/169001437 [00:00<01:13, 2293177.18it/s]      2%|▏         | 3014656/169001437 [00:00<00:09, 17296367.14it/s]      5%|▍         | 8355840/169001437 [00:00<00:04, 33642049.87it/s]      8%|▊         | 13697024/169001437 [00:00<00:03, 41396767.65it/s]     11%|█▏        | 19070976/169001437 [00:00<00:03, 45820802.92it/s]     14%|█▍        | 24379392/169001437 [00:00<00:03, 48163422.76it/s]     18%|█▊        | 29949952/169001437 [00:00<00:02, 50482039.38it/s]     21%|██        | 35389440/169001437 [00:00<00:02, 51630006.20it/s]     24%|██▍       | 40796160/169001437 [00:00<00:02, 52350252.40it/s]     27%|██▋       | 46202880/169001437 [00:01<00:02, 52859469.98it/s]     31%|███       | 51609600/169001437 [00:01<00:02, 53104005.74it/s]     34%|███▍      | 57049088/169001437 [00:01<00:02, 53460006.74it/s]     37%|███▋      | 62488576/169001437 [00:01<00:01, 53572162.97it/s]     40%|████      | 67960832/169001437 [00:01<00:01, 53808462.45it/s]     43%|████▎     | 73400320/169001437 [00:01<00:01, 53908908.64it/s]     47%|████▋     | 78905344/169001437 [00:01<00:01, 54131531.97it/s]     50%|████▉     | 84344832/169001437 [00:01<00:01, 54100101.54it/s]     53%|█████▎    | 89849856/169001437 [00:01<00:01, 54237853.38it/s]     56%|█████▋    | 95289344/169001437 [00:01<00:01, 54219431.51it/s]     60%|█████▉    | 100761600/169001437 [00:02<00:01, 54348583.11it/s]     63%|██████▎   | 106201088/169001437 [00:02<00:01, 54279325.56it/s]     66%|██████▌   | 111640576/169001437 [00:02<00:01, 54313825.67it/s]     69%|██████▉   | 117112832/169001437 [00:02<00:00, 54333125.33it/s]     73%|███████▎  | 122552320/169001437 [00:02<00:00, 54217003.69it/s]     76%|███████▌  | 128057344/169001437 [00:02<00:00, 54222443.64it/s]     79%|███████▉  | 133562368/169001437 [00:02<00:00, 54306051.25it/s]     82%|████████▏ | 139001856/169001437 [00:02<00:00, 54303213.93it/s]     85%|████████▌ | 144474112/169001437 [00:02<00:00, 54238891.30it/s]     89%|████████▊ | 149913600/169001437 [00:02<00:00, 54106200.78it/s]     93%|█████████▎| 156336128/169001437 [00:03<00:00, 57056210.02it/s]     99%|█████████▊| 166756352/169001437 [00:03<00:00, 71085460.07it/s]    100%|██████████| 169001437/169001437 [00:03<00:00, 53870529.50it/s]
    Extracting data/cifar-100-python.tar.gz to data
    Files already downloaded and verified




.. GENERATED FROM PYTHON SOURCE LINES 55-62

4. Iterate on the Dataloader and compute the ECE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We first split the original test set into a calibration set and a test set for proper evaluation.

When computing the ECE, you need to provide the likelihoods associated with the inputs.
To do this, just call PyTorch's softmax.

.. GENERATED FROM PYTHON SOURCE LINES 62-83

.. code-block:: default


    from torch.utils.data import DataLoader, random_split

    # Split datasets
    dataset = dm.test
    cal_dataset, test_dataset = random_split(dataset, [1000, len(dataset)-1000])
    cal_dataloader, test_dataloader = DataLoader(cal_dataset, batch_size=32), DataLoader(test_dataset, batch_size=32)

    # Initialize the ECE
    ece = CalibrationError(task="multiclass", num_classes=100)

    # Iterate on the calibration dataloader
    for sample, target in test_dataloader:
        logits = model(sample)
        ece.update(logits, target)

    # Compute & print the calibration error
    cal = ece.compute()

    print(f"ECE before scaling - {cal*100:.3}%.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ECE before scaling - 10.8%.




.. GENERATED FROM PYTHON SOURCE LINES 84-90

5. Fit the Scaler to Improve the Calibration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The TemperatureScaler has one parameter that can be used to temper the softmax.
We minimize the tempered cross-entropy on a calibration set that we define here as
a subset of the test set and containing 1000 data.

.. GENERATED FROM PYTHON SOURCE LINES 90-95

.. code-block:: default


    # Fit the scaler on the calibration dataset
    scaler = TemperatureScaler()
    scaler = scaler.fit(model=model, calib_loader=cal_dataloader)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/32 [00:00<?, ?it/s]      9%|▉         | 3/32 [00:00<00:01, 27.31it/s]     19%|█▉        | 6/32 [00:00<00:00, 26.62it/s]     28%|██▊       | 9/32 [00:00<00:00, 26.87it/s]     38%|███▊      | 12/32 [00:00<00:00, 26.64it/s]     47%|████▋     | 15/32 [00:00<00:00, 26.21it/s]     56%|█████▋    | 18/32 [00:00<00:00, 26.63it/s]     66%|██████▌   | 21/32 [00:00<00:00, 27.03it/s]     75%|███████▌  | 24/32 [00:00<00:00, 27.25it/s]     84%|████████▍ | 27/32 [00:01<00:00, 27.27it/s]     94%|█████████▍| 30/32 [00:01<00:00, 27.24it/s]    100%|██████████| 32/32 [00:01<00:00, 27.36it/s]




.. GENERATED FROM PYTHON SOURCE LINES 96-103

6. Iterate again to compute the improved ECE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We create a wrapper of the original model and the scaler using torch.nn.Sequential.
This is possible because the scaler is derived from nn.Module.

Note that you will need to first reset the ECE metric to avoid mixing the scores of the previous and current iterations.

.. GENERATED FROM PYTHON SOURCE LINES 103-119

.. code-block:: default


    # Create the calibrated model
    cal_model = torch.nn.Sequential(model, scaler)

    # Reset the ECE
    ece.reset()

    # Iterate on the test dataloader
    for sample, target in test_dataloader:
        logits = cal_model(sample)
        ece.update(logits.softmax(-1), target)

    cal = ece.compute()

    print(f"ECE after scaling - {cal*100:.3}%.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ECE after scaling - 1.4%.




.. GENERATED FROM PYTHON SOURCE LINES 120-128

The top-label calibration should be improved.

Notes
-----

Temperature scaling is very efficient when the calibration set is representative of the test set.
In this case, we say that the calibration and test set are drawn from the same distribution.
However, this may not be True in real-world cases where dataset shift could happen.

.. GENERATED FROM PYTHON SOURCE LINES 130-135

References
----------

- **Expected Calibration Error:** Naeini, M. P., Cooper, G. F., & Hauskrecht, M. (2015). Obtaining Well Calibrated Probabilities Using Bayesian Binning. In `AAAI 2015 <https://arxiv.org/pdf/1411.0160.pdf>`_
- **Temperature Scaling:** Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. In `ICML 2017 <https://arxiv.org/pdf/1706.04599.pdf>`_


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 33.398 seconds)


.. _sphx_glr_download_auto_tutorials_tutorial_scaler.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_scaler.py <tutorial_scaler.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_scaler.ipynb <tutorial_scaler.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
