
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/Bayesian_Methods/tutorial_mc_batch_norm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_Bayesian_Methods_tutorial_mc_batch_norm.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_Bayesian_Methods_tutorial_mc_batch_norm.py:


Training a LeNet with Monte Carlo Batch Normalization
=====================================================

In this tutorial, we train a LeNet classifier on the MNIST dataset using Monte-Carlo Batch Normalization (MCBN), a post-hoc Bayesian approximation method.

Training a LeNet with MCBN using TorchUncertainty models and PyTorch Lightning
------------------------------------------------------------------------------
In this part, we train a LeNet with batch normalization layers, based on the model and routines already implemented in TU.

1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

First, we have to load the following utilities from TorchUncertainty:

- the TUTrainer from our framework
- the datamodule handling dataloaders: MNISTDataModule from torch_uncertainty.datamodules
- the model: LeNet, which lies in torch_uncertainty.models
- the MC Batch Normalization wrapper: mc_batch_norm, which lies in torch_uncertainty.post_processing
- the classification training routine in the torch_uncertainty.routines
- an optimization recipe in the torch_uncertainty.optim_recipes module.

We also need import the neural network utils within `torch.nn`.

.. GENERATED FROM PYTHON SOURCE LINES 28-39

.. code-block:: Python

    from pathlib import Path

    from torch import nn

    from torch_uncertainty import TUTrainer
    from torch_uncertainty.datamodules import MNISTDataModule
    from torch_uncertainty.models.classification import lenet
    from torch_uncertainty.optim_recipes import optim_cifar10_resnet18
    from torch_uncertainty.post_processing.mc_batch_norm import MCBatchNorm
    from torch_uncertainty.routines import ClassificationRoutine








.. GENERATED FROM PYTHON SOURCE LINES 40-45

2. Creating the necessary variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In the following, we define the root of the datasets and the
logs. We also create the datamodule that handles the MNIST dataset
dataloaders and transforms.

.. GENERATED FROM PYTHON SOURCE LINES 45-59

.. code-block:: Python


    trainer = TUTrainer(accelerator="gpu", devices=1, max_epochs=2, enable_progress_bar=False)

    # datamodule
    root = Path("data")
    datamodule = MNISTDataModule(root, batch_size=128)


    model = lenet(
        in_channels=datamodule.num_channels,
        num_classes=datamodule.num_classes,
        norm=nn.BatchNorm2d,
    )








.. GENERATED FROM PYTHON SOURCE LINES 60-66

3. The Loss and the Training Routine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This is a classification problem, and we use CrossEntropyLoss as likelihood.
We define the training routine using the classification training routine from
torch_uncertainty.training.classification. We provide the number of classes,
and the optimization recipe.

.. GENERATED FROM PYTHON SOURCE LINES 66-74

.. code-block:: Python


    routine = ClassificationRoutine(
        num_classes=datamodule.num_classes,
        model=model,
        loss=nn.CrossEntropyLoss(),
        optim_recipe=optim_cifar10_resnet18(model),
    )








.. GENERATED FROM PYTHON SOURCE LINES 75-79

4. Gathering Everything and Training the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
You can also save the results in a variable by saving the output of
`trainer.test`.

.. GENERATED FROM PYTHON SOURCE LINES 79-83

.. code-block:: Python


    trainer.fit(model=routine, datamodule=datamodule)
    perf = trainer.test(model=routine, datamodule=datamodule)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓
    ┃   ┃ Name             ┃ Type                ┃ Params ┃ Mode  ┃ FLOPs ┃
    ┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩
    │ 0 │ ood_criterion    │ MaxSoftmaxCriterion │      0 │ train │     0 │
    │ 1 │ model            │ _LeNet              │ 44.5 K │ train │     0 │
    │ 2 │ loss             │ CrossEntropyLoss    │      0 │ train │     0 │
    │ 3 │ format_batch_fn  │ Identity            │      0 │ train │     0 │
    │ 4 │ val_cls_metrics  │ MetricCollection    │      0 │ train │     0 │
    │ 5 │ test_cls_metrics │ MetricCollection    │      0 │ train │     0 │
    │ 6 │ test_id_entropy  │ Entropy             │      0 │ train │     0 │
    │ 7 │ mixup            │ Identity            │      0 │ train │     0 │
    └───┴──────────────────┴─────────────────────┴────────┴───────┴───────┘
    Trainable params: 44.5 K                                                        
    Non-trainable params: 0                                                         
    Total params: 44.5 K                                                            
    Total estimated model params size (MB): 0                                       
    Modules in train mode: 37                                                       
    Modules in eval mode: 0                                                         
    Total FLOPs: 0                                                                  
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃      Classification       ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     Acc      │          97.160%          │
    │    Brier     │          0.04240          │
    │   Entropy    │          0.10402          │
    │     NLL      │          0.09136          │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Calibration        ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │     ECE      │          0.451%           │
    │     aECE     │          0.414%           │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃ Selective Classification  ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │    AUGRC     │          0.150%           │
    │     AURC     │          0.162%           │
    │  Cov@5Risk   │         100.000%          │
    │  Risk@80Cov  │          0.112%           │
    └──────────────┴───────────────────────────┘
    ┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Test metric  ┃        Complexity         ┃
    ┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │    flops     │          72.10 M          │
    │    params    │          44.47 K          │
    └──────────────┴───────────────────────────┘




.. GENERATED FROM PYTHON SOURCE LINES 84-93

5. Wrapping the Model in a MCBatchNorm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
We can now wrap the model in a MCBatchNorm to add stochasticity to the
predictions. We specify that the BatchNorm layers are to be converted to
MCBatchNorm layers, and that we want to use 8 stochastic estimators.
The amount of stochasticity is controlled by the ``mc_batch_size`` argument.
The larger the ``mc_batch_size``, the less stochastic the predictions will be.
The authors suggest 32 as a good value for ``mc_batch_size`` but we use 16 here
to highlight the effect of stochasticity on the predictions.

.. GENERATED FROM PYTHON SOURCE LINES 93-98

.. code-block:: Python


    routine.model = MCBatchNorm(routine.model, num_estimators=8, convert=True, mc_batch_size=16)
    routine.model.fit(datamodule.train_dataloader())
    routine = routine.eval()  # To avoid prints








.. GENERATED FROM PYTHON SOURCE LINES 99-108

6. Testing the Model
~~~~~~~~~~~~~~~~~~~~
Now that the model is trained, let's test it on MNIST. Don't forget to call
.eval() to enable Monte Carlo batch normalization at evaluation (sometimes called inference).
In this tutorial, we plot the most uncertain images, i.e. the images for which
the variance of the predictions is the highest.
Please note that we apply a reshape to the logits to determine the dimension corresponding to the ensemble
and to the batch. As for TorchUncertainty 2.0, the ensemble dimension is merged with the batch dimension
in this order (num_estimator x batch, classes).

.. GENERATED FROM PYTHON SOURCE LINES 108-143

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import torch
    import torchvision


    def imshow(img) -> None:
        npimg = img.numpy()
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.axis("off")
        plt.tight_layout()
        plt.show()


    dataiter = iter(datamodule.val_dataloader())
    images, labels = next(dataiter)

    routine.eval()
    logits = routine(images).reshape(8, 128, 10)  # num_estimators, batch_size, num_classes

    probs = torch.nn.functional.softmax(logits, dim=-1)
    most_uncertain = sorted(probs.var(0).sum(-1).topk(4).indices)

    # print images
    imshow(torchvision.utils.make_grid(images[most_uncertain, ...]))
    print("Ground truth: ", " ".join(f"{labels[j]}" for j in range(4)))

    for j in most_uncertain:
        values, predicted = torch.max(probs[:, j], 1)
        print(
            f"Predicted digits for the image {j}: ",
            " ".join([str(image_id.item()) for image_id in predicted]),
        )




.. image-sg:: /auto_tutorials/Bayesian_Methods/images/sphx_glr_tutorial_mc_batch_norm_001.png
   :alt: tutorial mc batch norm
   :srcset: /auto_tutorials/Bayesian_Methods/images/sphx_glr_tutorial_mc_batch_norm_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Ground truth:  7 2 1 0
    Predicted digits for the image 20:  9 4 9 9 4 9 9 4
    Predicted digits for the image 80:  7 7 7 7 7 7 7 1
    Predicted digits for the image 111:  7 7 7 7 7 7 7 1
    Predicted digits for the image 119:  2 2 2 2 7 2 2 2




.. GENERATED FROM PYTHON SOURCE LINES 144-148

The predictions are mostly erroneous, which is expected since we selected
the most uncertain images. We also see that there stochasticity in the
predictions, as the predictions for the same image differ depending on the
stochastic estimator used.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 18.174 seconds)


.. _sphx_glr_download_auto_tutorials_Bayesian_Methods_tutorial_mc_batch_norm.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_mc_batch_norm.ipynb <tutorial_mc_batch_norm.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_mc_batch_norm.py <tutorial_mc_batch_norm.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: tutorial_mc_batch_norm.zip <tutorial_mc_batch_norm.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
