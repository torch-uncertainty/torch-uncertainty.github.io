
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/tutorial_mc_batch_norm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_tutorial_mc_batch_norm.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_tutorial_mc_batch_norm.py:


Training a LeNet with Monte Carlo Batch Normalization
=====================================================

In this tutorial, we will train a LeNet classifier on the MNIST dataset using Monte-Carlo Batch Normalization (MCBN), a post-hoc Bayesian approximation method. 

Training a LeNet with MCBN using TorchUncertainty models and PyTorch Lightning
------------------------------------------------------------------------------
In this part, we train a LeNet with batch normalization layers, based on the model and routines already implemented in TU.

1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

First, we have to load the following utilities from TorchUncertainty:

- the Trainer from Lightning
- the datamodule handling dataloaders: MNISTDataModule from torch_uncertainty.datamodules
- the model: LeNet, which lies in torch_uncertainty.models
- the MC Batch Normalization wrapper: mc_batch_norm, which lies in torch_uncertainty.post_processing
- the classification training routine in the torch_uncertainty.routines
- an optimization recipe in the torch_uncertainty.optim_recipes module.

We also need import the neural network utils within `torch.nn`.

.. GENERATED FROM PYTHON SOURCE LINES 25-36

.. code-block:: Python

    from pathlib import Path

    from lightning import Trainer
    from torch import nn

    from torch_uncertainty.datamodules import MNISTDataModule
    from torch_uncertainty.models.lenet import lenet
    from torch_uncertainty.optim_recipes import optim_cifar10_resnet18
    from torch_uncertainty.post_processing.mc_batch_norm import MCBatchNorm
    from torch_uncertainty.routines import ClassificationRoutine








.. GENERATED FROM PYTHON SOURCE LINES 37-42

2. Creating the necessary variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In the following, we define the root of the datasets and the
logs. We also create the datamodule that handles the MNIST dataset
dataloaders and transforms.

.. GENERATED FROM PYTHON SOURCE LINES 42-56

.. code-block:: Python


    trainer = Trainer(accelerator="cpu", max_epochs=2, enable_progress_bar=False)

    # datamodule
    root = Path("data")
    datamodule = MNISTDataModule(root, batch_size=128)


    model = lenet(
        in_channels=datamodule.num_channels,
        num_classes=datamodule.num_classes,
        norm=nn.BatchNorm2d,
    )








.. GENERATED FROM PYTHON SOURCE LINES 57-63

3. The Loss and the Training Routine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This is a classification problem, and we use CrossEntropyLoss as likelihood.
We define the training routine using the classification training routine from
torch_uncertainty.training.classification. We provide the number of classes,
and the optimization recipe.

.. GENERATED FROM PYTHON SOURCE LINES 63-71

.. code-block:: Python


    routine = ClassificationRoutine(
        num_classes=datamodule.num_classes,
        model=model,
        loss=nn.CrossEntropyLoss(),
        optim_recipe=optim_cifar10_resnet18(model),
    )








.. GENERATED FROM PYTHON SOURCE LINES 72-76

4. Gathering Everything and Training the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
You can also save the results in a variable by saving the output of
`trainer.test`.

.. GENERATED FROM PYTHON SOURCE LINES 76-80

.. code-block:: Python


    trainer.fit(model=routine, datamodule=datamodule)
    perf = trainer.test(model=routine, datamodule=datamodule)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃        Test metric        ┃       DataLoader 0        ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │       test/cal/ECE        │   0.0031606173142790794   │
    │       test/cal/aECE       │   0.004584523383527994    │
    │       test/cls/Acc        │    0.9760000109672546     │
    │      test/cls/Brier       │    0.03674852475523949    │
    │       test/cls/NLL        │    0.07853303849697113    │
    │     test/cls/entropy      │    0.08711287379264832    │
    │       test/sc/AURC        │   0.0016468340763822198   │
    │    test/sc/CovAt5Risk     │            1.0            │
    │    test/sc/RiskAt80Cov    │   0.0021250001154839993   │
    └───────────────────────────┴───────────────────────────┘




.. GENERATED FROM PYTHON SOURCE LINES 81-90

5. Wrapping the Model in a MCBatchNorm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
We can now wrap the model in a MCBatchNorm to add stochasticity to the
predictions. We specify that the BatchNorm layers are to be converted to
MCBatchNorm layers, and that we want to use 8 stochastic estimators.
The amount of stochasticity is controlled by the ``mc_batch_size`` argument.
The larger the ``mc_batch_size``, the more stochastic the predictions will be.
The authors suggest 32 as a good value for ``mc_batch_size`` but we use 4 here
to highlight the effect of stochasticity on the predictions.

.. GENERATED FROM PYTHON SOURCE LINES 90-97

.. code-block:: Python


    routine.model = MCBatchNorm(
        routine.model, num_estimators=8, convert=True, mc_batch_size=16
    )
    routine.model.fit(datamodule.train)
    routine = routine.eval()  # To avoid prints








.. GENERATED FROM PYTHON SOURCE LINES 98-107

6. Testing the Model
~~~~~~~~~~~~~~~~~~~~
Now that the model is trained, let's test it on MNIST. Don't forget to call
.eval() to enable Monte Carlo batch normalization at inference.
In this tutorial, we plot the most uncertain images, i.e. the images for which
the variance of the predictions is the highest.
Please note that we apply a reshape to the logits to determine the dimension corresponding to the ensemble
and to the batch. As for TorchUncertainty 2.0, the ensemble dimension is merged with the batch dimension
in this order (num_estimator x batch, classes).

.. GENERATED FROM PYTHON SOURCE LINES 107-142

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import torch
    import torchvision


    def imshow(img):
        npimg = img.numpy()
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.axis("off")
        plt.tight_layout()
        plt.show()


    dataiter = iter(datamodule.val_dataloader())
    images, labels = next(dataiter)

    routine.eval()
    logits = routine(images).reshape(8, 128, 10)  # num_estimators, batch_size, num_classes

    probs = torch.nn.functional.softmax(logits, dim=-1)
    most_uncertain = sorted(probs.var(0).sum(-1).topk(4).indices)

    # print images
    imshow(torchvision.utils.make_grid(images[most_uncertain, ...]))
    print("Ground truth: ", " ".join(f"{labels[j]}" for j in range(4)))

    for j in most_uncertain:
        values, predicted = torch.max(probs[:, j], 1)
        print(
            f"Predicted digits for the image {j}: ",
            " ".join([str(image_id.item()) for image_id in predicted]),
        )




.. image-sg:: /auto_tutorials/images/sphx_glr_tutorial_mc_batch_norm_001.png
   :alt: tutorial mc batch norm
   :srcset: /auto_tutorials/images/sphx_glr_tutorial_mc_batch_norm_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    Ground truth:  7 2 1 0
    Predicted digits for the image 18:  3 5 3 3 5 5 3 3
    Predicted digits for the image 19:  4 4 1 4 4 4 4 4
    Predicted digits for the image 36:  7 7 7 7 2 2 7 2
    Predicted digits for the image 111:  7 7 1 7 7 7 7 7




.. GENERATED FROM PYTHON SOURCE LINES 143-147

The predictions are mostly erroneous, which is expected since we selected
the most uncertain images. We also see that there stochasticity in the
predictions, as the predictions for the same image differ depending on the
stochastic estimator used.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 38.262 seconds)


.. _sphx_glr_download_auto_tutorials_tutorial_mc_batch_norm.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_mc_batch_norm.ipynb <tutorial_mc_batch_norm.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_mc_batch_norm.py <tutorial_mc_batch_norm.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
