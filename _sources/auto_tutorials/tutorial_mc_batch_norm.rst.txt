
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/tutorial_mc_batch_norm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_tutorial_mc_batch_norm.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_tutorial_mc_batch_norm.py:


Training a LeNet with Monte Carlo Batch Normalization
=====================================================

In this tutorial, we will train a LeNet classifier on the MNIST dataset using Monte-Carlo Batch Normalization (MCBN), a post-hoc Bayesian approximation method. 

Training a LeNet with MCBN using TorchUncertainty models and PyTorch Lightning
------------------------------------------------------------------------------
In this part, we train a LeNet with batch normalization layers, based on the model and routines already implemented in TU.

1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

First, we have to load the following utilities from TorchUncertainty:

- the Trainer from Lightning
- the datamodule handling dataloaders: MNISTDataModule from torch_uncertainty.datamodules
- the model: LeNet, which lies in torch_uncertainty.models
- the MC Batch Normalization wrapper: mc_batch_norm, which lies in torch_uncertainty.post_processing
- the classification training routine in the torch_uncertainty.routines
- an optimization recipe in the torch_uncertainty.optim_recipes module.

We also need import the neural network utils within `torch.nn`.

.. GENERATED FROM PYTHON SOURCE LINES 26-37

.. code-block:: Python

    from pathlib import Path

    from lightning import Trainer
    from torch import nn

    from torch_uncertainty.datamodules import MNISTDataModule
    from torch_uncertainty.models.lenet import lenet
    from torch_uncertainty.optim_recipes import optim_cifar10_resnet18
    from torch_uncertainty.post_processing.mc_batch_norm import MCBatchNorm
    from torch_uncertainty.routines import ClassificationRoutine








.. GENERATED FROM PYTHON SOURCE LINES 38-43

2. Creating the necessary variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In the following, we define the root of the datasets and the
logs. We also create the datamodule that handles the MNIST dataset
dataloaders and transforms.

.. GENERATED FROM PYTHON SOURCE LINES 43-57

.. code-block:: Python


    trainer = Trainer(accelerator="cpu", max_epochs=2, enable_progress_bar=False)

    # datamodule
    root = Path("") / "data"
    datamodule = MNISTDataModule(root, batch_size=128)


    model = lenet(
        in_channels=datamodule.num_channels,
        num_classes=datamodule.num_classes,
        norm=nn.BatchNorm2d,
    )








.. GENERATED FROM PYTHON SOURCE LINES 58-64

3. The Loss and the Training Routine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This is a classification problem, and we use CrossEntropyLoss as likelihood.
We define the training routine using the classification training routine from
torch_uncertainty.training.classification. We provide the number of classes,
and the optimization recipe.

.. GENERATED FROM PYTHON SOURCE LINES 64-72

.. code-block:: Python


    routine = ClassificationRoutine(
        num_classes=datamodule.num_classes,
        model=model,
        loss=nn.CrossEntropyLoss(),
        optim_recipe=optim_cifar10_resnet18(model),
    )








.. GENERATED FROM PYTHON SOURCE LINES 73-75

4. Gathering Everything and Training the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 75-79

.. code-block:: Python


    trainer.fit(model=routine, datamodule=datamodule)
    trainer.test(model=routine, datamodule=datamodule)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃        Test metric        ┃       DataLoader 0        ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │       cls_test/Acc        │    0.9812999963760376     │
    │      cls_test/Brier       │   0.028409384191036224    │
    │       cls_test/ECE        │   0.0051252031698822975   │
    │       cls_test/NLL        │   0.059990596026182175    │
    │     cls_test/entropy      │    0.08191443234682083    │
    └───────────────────────────┴───────────────────────────┘

    [{'cls_test/Acc': 0.9812999963760376, 'cls_test/Brier': 0.028409384191036224, 'cls_test/ECE': 0.0051252031698822975, 'cls_test/NLL': 0.059990596026182175, 'cls_test/entropy': 0.08191443234682083}]



.. GENERATED FROM PYTHON SOURCE LINES 80-89

5. Wrapping the Model in a MCBatchNorm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
We can now wrap the model in a MCBatchNorm to add stochasticity to the
predictions. We specify that the BatchNorm layers are to be converted to
MCBatchNorm layers, and that we want to use 8 stochastic estimators.
The amount of stochasticity is controlled by the ``mc_batch_size`` argument.
The larger the ``mc_batch_size``, the more stochastic the predictions will be.
The authors suggest 32 as a good value for ``mc_batch_size`` but we use 4 here
to highlight the effect of stochasticity on the predictions.

.. GENERATED FROM PYTHON SOURCE LINES 89-96

.. code-block:: Python


    routine.model = MCBatchNorm(
        routine.model, num_estimators=8, convert=True, mc_batch_size=4
    )
    routine.model.fit(datamodule.train)
    routine.eval()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ClassificationRoutine(
      (model): MCBatchNorm(
        (model): _LeNet(
          (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
          (norm1): MCBatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
          (norm2): MCBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (pooling): AdaptiveAvgPool2d(output_size=(4, 4))
          (fc1): Linear(in_features=256, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=84, bias=True)
          (fc3): Linear(in_features=84, out_features=10, bias=True)
        )
      )
      (loss): CrossEntropyLoss()
      (format_batch_fn): Identity()
      (val_cls_metrics): MetricCollection(
        (Acc): MulticlassAccuracy()
        (Brier): BrierScore()
        (ECE): MulticlassCE()
        (NLL): CategoricalNLL(),
        prefix=cls_val/
      )
      (test_cls_metrics): MetricCollection(
        (Acc): MulticlassAccuracy()
        (Brier): BrierScore()
        (ECE): MulticlassCE()
        (NLL): CategoricalNLL(),
        prefix=cls_test/
      )
      (test_entropy_id): Entropy()
      (mixup): Identity()
    )



.. GENERATED FROM PYTHON SOURCE LINES 97-103

6. Testing the Model
~~~~~~~~~~~~~~~~~~~~
Now that the model is trained, let's test it on MNIST. Don't forget to call
.eval() to enable Monte Carlo batch normalization at inference.
In this tutorial, we plot the most uncertain images, i.e. the images for which
the variance of the predictions is the highest.

.. GENERATED FROM PYTHON SOURCE LINES 103-138

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import torch
    import torchvision


    def imshow(img):
        npimg = img.numpy()
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.axis("off")
        plt.tight_layout()
        plt.show()


    dataiter = iter(datamodule.val_dataloader())
    images, labels = next(dataiter)

    # print images
    imshow(torchvision.utils.make_grid(images[:4, ...]))
    print("Ground truth: ", " ".join(f"{labels[j]}" for j in range(4)))

    routine.eval()
    logits = routine(images).reshape(8, 128, 10)

    probs = torch.nn.functional.softmax(logits, dim=-1)


    for j in sorted(probs.var(0).sum(-1).topk(4).indices):
        values, predicted = torch.max(probs[:, j], 1)
        print(
            f"Predicted digits for the image {j}: ",
            " ".join([str(image_id.item()) for image_id in predicted]),
        )




.. image-sg:: /auto_tutorials/images/sphx_glr_tutorial_mc_batch_norm_001.png
   :alt: tutorial mc batch norm
   :srcset: /auto_tutorials/images/sphx_glr_tutorial_mc_batch_norm_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    Ground truth:  7 2 1 0
    Predicted digits for the image 62:  5 9 9 9 5 5 9 5
    Predicted digits for the image 89:  7 1 8 1 1 1 1 1
    Predicted digits for the image 94:  8 1 8 1 8 1 1 1
    Predicted digits for the image 95:  6 4 6 4 6 4 4 4




.. GENERATED FROM PYTHON SOURCE LINES 139-143

The predictions are mostly erroneous, which is expected since we selected
the most uncertain images. We also see that there stochasticity in the
predictions, as the predictions for the same image differ depending on the
stochastic estimator used.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 36.826 seconds)


.. _sphx_glr_download_auto_tutorials_tutorial_mc_batch_norm.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_mc_batch_norm.ipynb <tutorial_mc_batch_norm.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_mc_batch_norm.py <tutorial_mc_batch_norm.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
