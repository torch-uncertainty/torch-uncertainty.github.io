
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_tutorials/tutorial_mc_dropout.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_tutorials_tutorial_mc_dropout.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_tutorials_tutorial_mc_dropout.py:


Training a LeNet with Monte-Carlo Dropout
=========================================

In this tutorial, we will train a LeNet classifier on the MNIST dataset using Monte-Carlo Dropout (MC Dropout), a computationally efficient Bayesian approximation method. To estimate the predictive mean and uncertainty (variance), we perform multiple forward passes through the network with dropout layers enabled in ``train`` mode.

For more information on Monte-Carlo Dropout, we refer the reader to the following resources:

- Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning `ICML 2016 <https://browse.arxiv.org/pdf/1506.02142.pdf>`_
- What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? `NeurIPS 2017 <https://browse.arxiv.org/pdf/1703.04977.pdf>`_

Training a LeNet with MC Dropout using TorchUncertainty models and PyTorch Lightning
-------------------------------------------------------------------------------------

In this part, we train a LeNet with dropout layers, based on the model and routines already implemented in TU.

1. Loading the utilities
~~~~~~~~~~~~~~~~~~~~~~~~

First, we have to load the following utilities from TorchUncertainty:

- the Trainer from Lightning
- the datamodule handling dataloaders: MNISTDataModule from torch_uncertainty.datamodules
- the model: LeNet, which lies in torch_uncertainty.models
- the MC Dropout wrapper: mc_dropout, from torch_uncertainty.models.wrappers
- the classification training & evaluation routine in the torch_uncertainty.routines
- an optimization recipe in the torch_uncertainty.optim_recipes module.

We also need import the neural network utils within `torch.nn`.

.. GENERATED FROM PYTHON SOURCE LINES 33-44

.. code-block:: Python

    from pathlib import Path

    from lightning.pytorch import Trainer
    from torch import nn

    from torch_uncertainty.datamodules import MNISTDataModule
    from torch_uncertainty.models.lenet import lenet
    from torch_uncertainty.models import mc_dropout
    from torch_uncertainty.optim_recipes import optim_cifar10_resnet18
    from torch_uncertainty.routines import ClassificationRoutine








.. GENERATED FROM PYTHON SOURCE LINES 45-55

2. Creating the necessary variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the following, we will need to define the root of the datasets and the
logs, and to fake-parse the arguments needed for using the PyTorch Lightning
Trainer. We also create the datamodule that handles the MNIST dataset,
dataloaders and transforms. We create the model using the
blueprint from torch_uncertainty.models and we wrap it into mc_dropout.

It is important to add a ``dropout_rate`` argument in your model to use Monte Carlo dropout.

.. GENERATED FROM PYTHON SOURCE LINES 55-71

.. code-block:: Python


    trainer = Trainer(accelerator="cpu", max_epochs=2, enable_progress_bar=False)

    # datamodule
    root = Path("data")
    datamodule = MNISTDataModule(root=root, batch_size=128)


    model = lenet(
        in_channels=datamodule.num_channels,
        num_classes=datamodule.num_classes,
        dropout_rate=0.5,
    )

    mc_model = mc_dropout(model, num_estimators=16, last_layer=False)








.. GENERATED FROM PYTHON SOURCE LINES 72-78

3. The Loss and the Training Routine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This is a classification problem, and we use CrossEntropyLoss as the likelihood.
We define the training routine using the classification training routine from
torch_uncertainty.routines.classification. We provide the number of classes
and channels, the optimizer wrapper, and the dropout rate.

.. GENERATED FROM PYTHON SOURCE LINES 78-87

.. code-block:: Python


    routine = ClassificationRoutine(
        num_classes=datamodule.num_classes,
        model=mc_model,
        loss=nn.CrossEntropyLoss(),
        optim_recipe=optim_cifar10_resnet18(mc_model),
        is_ensemble=True,
    )








.. GENERATED FROM PYTHON SOURCE LINES 88-90

4. Gathering Everything and Training the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 90-94

.. code-block:: Python


    trainer.fit(model=routine, datamodule=datamodule)
    trainer.test(model=routine, datamodule=datamodule)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
    Failed to download (trying next):
    HTTP Error 403: Forbidden

    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz
      0%|          | 0/9912422 [00:00<?, ?it/s]    100%|██████████| 9912422/9912422 [00:00<00:00, 127736608.22it/s]
    Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
    Failed to download (trying next):
    HTTP Error 403: Forbidden

    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz
      0%|          | 0/28881 [00:00<?, ?it/s]    100%|██████████| 28881/28881 [00:00<00:00, 4711984.36it/s]
    Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
    Failed to download (trying next):
    HTTP Error 403: Forbidden

    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz
      0%|          | 0/1648877 [00:00<?, ?it/s]    100%|██████████| 1648877/1648877 [00:00<00:00, 41840195.27it/s]
    Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw

    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
    Failed to download (trying next):
    HTTP Error 403: Forbidden

    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz
      0%|          | 0/4542 [00:00<?, ?it/s]    100%|██████████| 4542/4542 [00:00<00:00, 32902467.65it/s]
    Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw

    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃        Test metric        ┃       DataLoader 0        ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │       test/cal/ECE        │    0.2038881480693817     │
    │       test/cal/aECE       │    0.20388808846473694    │
    │       test/cls/Acc        │    0.9334999918937683     │
    │      test/cls/Brier       │    0.16512416303157806    │
    │       test/cls/NLL        │    0.4210292100906372     │
    │     test/cls/entropy      │    0.9684865474700928     │
    │   test/ens_Disagreement   │    0.22260916233062744    │
    │     test/ens_Entropy      │    0.7011492252349854     │
    │        test/ens_MI        │    0.26733729243278503    │
    │       test/sc/AURC        │   0.008880446227875599    │
    │    test/sc/CovAt5Risk     │    0.9643999934196472     │
    │    test/sc/RiskAt80Cov    │   0.013500000350177288    │
    └───────────────────────────┴───────────────────────────┘

    [{'test/cal/ECE': 0.2038881480693817, 'test/cal/aECE': 0.20388808846473694, 'test/cls/Acc': 0.9334999918937683, 'test/cls/Brier': 0.16512416303157806, 'test/cls/NLL': 0.4210292100906372, 'test/sc/AURC': 0.008880446227875599, 'test/sc/CovAt5Risk': 0.9643999934196472, 'test/sc/RiskAt80Cov': 0.013500000350177288, 'test/cls/entropy': 0.9684865474700928, 'test/ens_Disagreement': 0.22260916233062744, 'test/ens_Entropy': 0.7011492252349854, 'test/ens_MI': 0.26733729243278503}]



.. GENERATED FROM PYTHON SOURCE LINES 95-99

5. Testing the Model
~~~~~~~~~~~~~~~~~~~~
Now that the model is trained, let's test it on MNIST. Don't forget to call
.eval() to enable dropout at inference.

.. GENERATED FROM PYTHON SOURCE LINES 99-134

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import torch
    import torchvision


    def imshow(img):
        npimg = img.numpy()
        plt.imshow(np.transpose(npimg, (1, 2, 0)))
        plt.axis("off")
        plt.tight_layout()
        plt.show()


    dataiter = iter(datamodule.val_dataloader())
    images, labels = next(dataiter)

    # print images
    imshow(torchvision.utils.make_grid(images[:6, ...], padding=0))
    print("Ground truth labels: ", " ".join(f"{labels[j]}" for j in range(6)))

    routine.eval()
    logits = routine(images).reshape(16, 128, 10)

    probs = torch.nn.functional.softmax(logits, dim=-1)


    for j in range(6):
        values, predicted = torch.max(probs[:, j], 1)
        print(
            f"Predicted digits for the image {j+1}: ",
            " ".join([str(image_id.item()) for image_id in predicted]),
        )




.. image-sg:: /auto_tutorials/images/sphx_glr_tutorial_mc_dropout_001.png
   :alt: tutorial mc dropout
   :srcset: /auto_tutorials/images/sphx_glr_tutorial_mc_dropout_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
    Ground truth labels:  7 2 1 0 4 1
    Predicted digits for the image 1:  7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7
    Predicted digits for the image 2:  2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 8
    Predicted digits for the image 3:  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
    Predicted digits for the image 4:  0 0 0 0 8 0 8 8 0 0 0 0 0 6 0 8
    Predicted digits for the image 5:  4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
    Predicted digits for the image 6:  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1




.. GENERATED FROM PYTHON SOURCE LINES 135-137

Most of the time, we see that there is some disagreement between the samples of the dropout
approximation of the posterior distribution.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 47.058 seconds)


.. _sphx_glr_download_auto_tutorials_tutorial_mc_dropout.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_mc_dropout.ipynb <tutorial_mc_dropout.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_mc_dropout.py <tutorial_mc_dropout.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
