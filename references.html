


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>References &mdash; TorchUncertainty 0.2.0 documentation</title>
  

  <link rel="shortcut icon" href="_static/images/logo_torch_uncertainty.png" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/sphinx-codeautolink.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="index" title="Index" href="genindex.html" />
  <link rel="search" title="Search" href="search.html" />
  <link rel="prev" title="Contributing" href="contributing.html" />
  <!-- Google Analytics -->
  <script type="text/javascript">
    var collapsedSections = [];
  </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://torch-uncertainty.github.io/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/ENSTA-U2IS-AI/torch-uncertainty" target="_blank">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction_uncertainty.html">Introduction to Classification Uncertainty</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli_guide.html">CLI Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">References</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
    <li>
      <a href="index.html">
        Docs
      </a> &gt;
    </li>

    
    <li>References</li>
    
    <!-- 
    <li class="pytorch-breadcrumbs-aside">
      
      
      
      
      
      <a href="/zh_CN//references.html" class="fa fa-language"> 以中文阅读</a>
      
      
    </li>
    
     -->
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">

        

        <div class="pytorch-call-to-action-links">
          <div id="tutorial-type">references</div>

          <!-- <div id="google-colab-link">
            <img class="call-to-action-img" src="_static/images/pytorch-colab.svg" />
            <div class="call-to-action-desktop-view">Run in Google Colab</div>
            <div class="call-to-action-mobile-view">Colab</div>
          </div> -->
          <div id="download-notebook-link">
            <img class="call-to-action-notebook-img" src="_static/images/pytorch-download.svg" />
            <div class="call-to-action-desktop-view">Download Notebook</div>
            <div class="call-to-action-mobile-view">Notebook</div>
          </div>
          <div id="github-view-link">
            <img class="call-to-action-img" src="_static/images/pytorch-github.svg" />
            <div class="call-to-action-desktop-view">View on GitHub</div>
            <div class="call-to-action-mobile-view">GitHub</div>
          </div>
        </div>

        
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
                
  <section id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h1>
<p>Please find an exhaustive list of the references of the models, metrics, and datasets used in this library in the sections below.</p>
<section id="uncertainty-models">
<h2>Uncertainty Models<a class="headerlink" href="#uncertainty-models" title="Permalink to this heading">¶</a></h2>
<p>The following uncertainty models are implemented.</p>
<section id="deep-evidential-classification">
<h3>Deep Evidential Classification<a class="headerlink" href="#deep-evidential-classification" title="Permalink to this heading">¶</a></h3>
<p>For Deep Evidential Classification, consider citing:</p>
<p><strong>Evidential Deep Learning to Quantify Classification Uncertainty</strong></p>
<ul class="simple">
<li><p>Authors: <em>Murat Sensoy, Lance Kaplan, Melih Kandemir</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1806.01768">NeurIPS 2018</a>.</p></li>
</ul>
</section>
<section id="beta-nll-in-deep-regression">
<h3>Beta NLL in Deep Regression<a class="headerlink" href="#beta-nll-in-deep-regression" title="Permalink to this heading">¶</a></h3>
<p>For Beta NLL in Deep Regression, consider citing:</p>
<p><strong>On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks</strong></p>
<ul class="simple">
<li><p>Authors: <em>Maximilian Seitzer, Arash Tavakoli, Dimitrije Antic, Georg Martius</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/abs/2203.09168">ICLR 2022</a>.</p></li>
</ul>
</section>
<section id="deep-evidential-regression">
<h3>Deep Evidential Regression<a class="headerlink" href="#deep-evidential-regression" title="Permalink to this heading">¶</a></h3>
<p>For Deep Evidential Regression, consider citing:</p>
<p><strong>Deep Evidential Regression</strong></p>
<ul class="simple">
<li><p>Authors: <em>Alexander Amini, Wilko Schwarting, Ava Soleimany, Daniela Rus</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1910.02600">NeurIPS 2020</a>.</p></li>
</ul>
</section>
<section id="bayesian-neural-networks">
<h3>Bayesian Neural Networks<a class="headerlink" href="#bayesian-neural-networks" title="Permalink to this heading">¶</a></h3>
<p>For Bayesian Neural Networks, consider citing:</p>
<p><strong>Weight Uncertainty in Neural Networks</strong></p>
<ul class="simple">
<li><p>Authors: <em>Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1505.05424">ICML 2015</a>.</p></li>
</ul>
</section>
<section id="deep-ensembles">
<h3>Deep Ensembles<a class="headerlink" href="#deep-ensembles" title="Permalink to this heading">¶</a></h3>
<p>For Deep Ensembles, consider citing:</p>
<p><strong>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</strong></p>
<ul class="simple">
<li><p>Authors: <em>Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1612.01474.pdf">NeurIPS 2017</a>.</p></li>
</ul>
</section>
<section id="batchensemble">
<h3>BatchEnsemble<a class="headerlink" href="#batchensemble" title="Permalink to this heading">¶</a></h3>
<p>For BatchEnsemble, consider citing:</p>
<p><strong>BatchEnsemble: An alternative approach to Efficient Ensemble and Lifelong Learning</strong></p>
<ul class="simple">
<li><p>Authors: <em>Yeming Wen, Dustin Tran, and Jimmy Ba</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2002.06715.pdf">ICLR 2020</a>.</p></li>
</ul>
</section>
<section id="masksembles">
<h3>Masksembles<a class="headerlink" href="#masksembles" title="Permalink to this heading">¶</a></h3>
<p>For Masksembles, consider citing:</p>
<p><strong>Masksembles for Uncertainty Estimation</strong></p>
<ul class="simple">
<li><p>Authors: <em>Nikita Durasov, Timur Bagautdinov, Pierre Baque, and Pascal Fua</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2012.08334">CVPR 2021</a>.</p></li>
</ul>
</section>
<section id="mimo">
<h3>MIMO<a class="headerlink" href="#mimo" title="Permalink to this heading">¶</a></h3>
<p>For MIMO, consider citing:</p>
<p><strong>Training independent subnetworks for robust prediction</strong></p>
<ul class="simple">
<li><p>Authors: <em>Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan, Andrew M. Dai, and Dustin Tran</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2010.06610.pdf">ICLR 2021</a>.</p></li>
</ul>
</section>
<section id="packed-ensembles">
<h3>Packed-Ensembles<a class="headerlink" href="#packed-ensembles" title="Permalink to this heading">¶</a></h3>
<p>For Packed-Ensembles, consider citing:</p>
<p><strong>Packed-Ensembles for Efficient Uncertainty Estimation</strong></p>
<ul class="simple">
<li><p>Authors: <em>Olivier Laurent, Adrien Lafage, Enzo Tartaglione, Geoffrey Daniel, Jean-Marc Martinez, Andrei Bursuc, and Gianni Franchi</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/abs/2210.09184">ICLR 2023</a>.</p></li>
</ul>
</section>
<section id="monte-carlo-dropout">
<h3>Monte-Carlo Dropout<a class="headerlink" href="#monte-carlo-dropout" title="Permalink to this heading">¶</a></h3>
<p>For Monte-Carlo Dropout, consider citing:</p>
<p><strong>Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</strong></p>
<ul class="simple">
<li><p>Authors: <em>Yarin Gal and Zoubin Ghahramani</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1506.02142.pdf">ICML 2016</a>.</p></li>
</ul>
</section>
</section>
<section id="data-augmentation-methods">
<h2>Data Augmentation Methods<a class="headerlink" href="#data-augmentation-methods" title="Permalink to this heading">¶</a></h2>
<section id="mixup">
<h3>Mixup<a class="headerlink" href="#mixup" title="Permalink to this heading">¶</a></h3>
<p>For Mixup, consider citing:</p>
<p><strong>mixup: Beyond Empirical Risk Minimization</strong></p>
<ul class="simple">
<li><p>Authors: <em>Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1710.09412.pdf">ICLR 2018</a>.</p></li>
</ul>
</section>
<section id="regmixup">
<h3>RegMixup<a class="headerlink" href="#regmixup" title="Permalink to this heading">¶</a></h3>
<p>For RegMixup, consider citing:</p>
<p><strong>RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and Out Distribution Robustness</strong></p>
<ul class="simple">
<li><p>Authors: <em>Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip H.S. Torr, Puneet K. Dokania</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/abs/2206.14502">NeurIPS 2022</a>.</p></li>
</ul>
</section>
<section id="mixupio">
<h3>MixupIO<a class="headerlink" href="#mixupio" title="Permalink to this heading">¶</a></h3>
<p>For MixupIO, consider citing:</p>
<p><strong>On the Pitfall of Mixup for Uncertainty Calibration</strong></p>
<ul class="simple">
<li><p>Authors: <em>Deng-Bao Wang, Lanqing Li, Peilin Zhao, Pheng-Ann Heng, and Min-Ling Zhang</em></p></li>
<li><p>Paper: <cite>CVPR 2023 &lt;https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_On_the_Pitfall_of_Mixup_for_Uncertainty_Calibration_CVPR_2023_paper.pdf&gt;</cite></p></li>
</ul>
</section>
<section id="warping-mixup">
<h3>Warping Mixup<a class="headerlink" href="#warping-mixup" title="Permalink to this heading">¶</a></h3>
<p>For Warping Mixup, consider citing:</p>
<p><strong>Tailoring Mixup to Data using Kernel Warping functions</strong></p>
<ul class="simple">
<li><p>Authors: <em>Quentin Bouniot, Pavlo Mozharovskyi, and Florence d’Alché-Buc</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/abs/2311.01434">ArXiv 2023</a>.</p></li>
</ul>
</section>
</section>
<section id="post-processing-methods">
<h2>Post-Processing Methods<a class="headerlink" href="#post-processing-methods" title="Permalink to this heading">¶</a></h2>
<section id="temperature-vector-matrix-scaling">
<h3>Temperature, Vector, &amp; Matrix scaling<a class="headerlink" href="#temperature-vector-matrix-scaling" title="Permalink to this heading">¶</a></h3>
<p>For temperature, vector, &amp; matrix scaling, consider citing:</p>
<p><strong>On Calibration of Modern Neural Networks</strong></p>
<ul class="simple">
<li><p>Authors: <em>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1706.04599.pdf">ICML 2017</a>.</p></li>
</ul>
</section>
<section id="monte-carlo-batch-normalization">
<h3>Monte-Carlo Batch Normalization<a class="headerlink" href="#monte-carlo-batch-normalization" title="Permalink to this heading">¶</a></h3>
<p>For Monte-Carlo Batch Normalization, consider citing:</p>
<p><strong>Bayesian Uncertainty Estimation for Batch Normalized Deep Networks</strong></p>
<ul class="simple">
<li><p>Authors: <em>Mathias Teye, Hossein Azizpour, and Kevin Smith</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1802.06455.pdf">ICML 2018</a>.</p></li>
</ul>
</section>
</section>
<section id="metrics">
<h2>Metrics<a class="headerlink" href="#metrics" title="Permalink to this heading">¶</a></h2>
<p>The following metrics are used/implemented.</p>
<section id="expected-calibration-error">
<h3>Expected Calibration Error<a class="headerlink" href="#expected-calibration-error" title="Permalink to this heading">¶</a></h3>
<p>For the expected calibration error, consider citing:</p>
<p><strong>Obtaining Well Calibrated Probabilities Using Bayesian Binning</strong></p>
<ul class="simple">
<li><p>Authors: <em>Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht</em></p></li>
<li><p>Paper: <a class="reference external" href="https://www.dbmi.pitt.edu/wp-content/uploads/2022/10/Obtaining-well-calibrated-probabilities-using-Bayesian-binning.pdf">AAAI 2015</a>.</p></li>
</ul>
</section>
<section id="grouping-loss">
<h3>Grouping Loss<a class="headerlink" href="#grouping-loss" title="Permalink to this heading">¶</a></h3>
<p>For the grouping loss, consider citing:</p>
<p><strong>Beyond Calibration: Estimating the Grouping Loss of Modern Neural Networks</strong></p>
<ul class="simple">
<li><p>Authors: <em>Alexandre Perez-Lebel, Marine Le Morvan, and Gaël Varoquaux</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2210.16315.pdf">ICLR 2023</a>.</p></li>
</ul>
</section>
</section>
<section id="datasets">
<h2>Datasets<a class="headerlink" href="#datasets" title="Permalink to this heading">¶</a></h2>
<p>The following datasets are used/implemented.</p>
<section id="mnist">
<h3>MNIST<a class="headerlink" href="#mnist" title="Permalink to this heading">¶</a></h3>
<p><strong>Gradient-based learning applied to document recognition</strong></p>
<ul class="simple">
<li><p>Authors: <em>Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner</em></p></li>
<li><p>Paper: <a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">Proceedings of the IEEE 1998</a>.</p></li>
</ul>
</section>
<section id="mnist-c">
<h3>MNIST-C<a class="headerlink" href="#mnist-c" title="Permalink to this heading">¶</a></h3>
<p><strong>MNIST-C: A Robustness Benchmark for Computer Vision</strong></p>
<ul class="simple">
<li><p>Authors: <em>Norman Mu, and Justin Gilmer</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1906.02337.pdf">ICMLW 2019</a>.</p></li>
</ul>
</section>
<section id="not-mnist">
<h3>Not-MNIST<a class="headerlink" href="#not-mnist" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Author: <em>Yaroslav Bulatov</em></p></li>
</ul>
</section>
<section id="cifar-10-cifar-100">
<h3>CIFAR-10 &amp; CIFAR-100<a class="headerlink" href="#cifar-10-cifar-100" title="Permalink to this heading">¶</a></h3>
<p><strong>Learning multiple layers of features from tiny images</strong></p>
<ul class="simple">
<li><p>Authors: <em>Alex Krizhevsky</em></p></li>
<li><p>Paper: <a class="reference external" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">MIT Tech Report</a>.</p></li>
</ul>
</section>
<section id="cifar-c-tiny-imagenet-c-imagenet-c">
<h3>CIFAR-C, Tiny-ImageNet-C, ImageNet-C<a class="headerlink" href="#cifar-c-tiny-imagenet-c-imagenet-c" title="Permalink to this heading">¶</a></h3>
<p><strong>Benchmarking neural network robustness to common corruptions and perturbations</strong></p>
<ul class="simple">
<li><p>Authors: <em>Dan Hendrycks and Thomas Dietterich</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1903.12261.pdf">ICLR 2019</a>.</p></li>
</ul>
</section>
<section id="cifar-10-h">
<h3>CIFAR-10 H<a class="headerlink" href="#cifar-10-h" title="Permalink to this heading">¶</a></h3>
<p><strong>Human uncertainty makes classification more robust</strong></p>
<ul class="simple">
<li><p>Authors: <em>Joshua C. Peterson, Ruairidh M. Battleday, Thomas L. Griffiths, and Olga Russakovsky</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1908.07086.pdf">ICCV 2019</a>.</p></li>
</ul>
</section>
<section id="cifar-10-n-cifar-100-n">
<h3>CIFAR-10 N / CIFAR-100 N<a class="headerlink" href="#cifar-10-n-cifar-100-n" title="Permalink to this heading">¶</a></h3>
<p><strong>Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations</strong></p>
<ul class="simple">
<li><p>Authors: <em>Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, Yang Liu</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2110.12088.pdf">ICLR 2022</a>.</p></li>
</ul>
</section>
<section id="svhn">
<h3>SVHN<a class="headerlink" href="#svhn" title="Permalink to this heading">¶</a></h3>
<p><strong>Reading digits in natural images with unsupervised feature learning</strong></p>
<ul class="simple">
<li><p>Authors: <em>Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng</em></p></li>
<li><p>Paper: <a class="reference external" href="http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf">NeurIPS Workshops 2011</a>.</p></li>
</ul>
</section>
<section id="imagenet">
<h3>ImageNet<a class="headerlink" href="#imagenet" title="Permalink to this heading">¶</a></h3>
<p><strong>Imagenet: A large-scale hierarchical image database</strong></p>
<ul class="simple">
<li><p>Authors: <em>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei</em></p></li>
<li><p>Paper: <a class="reference external" href="https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf">CVPR 2009</a>.</p></li>
</ul>
</section>
<section id="imagenet-a-imagenet-0">
<h3>ImageNet-A &amp; ImageNet-0<a class="headerlink" href="#imagenet-a-imagenet-0" title="Permalink to this heading">¶</a></h3>
<p><strong>Natural adversarial examples</strong></p>
<ul class="simple">
<li><p>Authors: <em>Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1907.07174.pdf">CVPR 2021</a>.</p></li>
</ul>
</section>
<section id="imagenet-r">
<h3>ImageNet-R<a class="headerlink" href="#imagenet-r" title="Permalink to this heading">¶</a></h3>
<p><strong>The many faces of robustness: A critical analysis of out-of-distribution generalization</strong></p>
<ul class="simple">
<li><p>Authors: <em>Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al.</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2006.16241.pdf">ICCV 2021</a>.</p></li>
</ul>
</section>
<section id="textures">
<h3>Textures<a class="headerlink" href="#textures" title="Permalink to this heading">¶</a></h3>
<p><strong>ViM: Out-of-distribution with virtual-logit matching</strong></p>
<ul class="simple">
<li><p>Authors: <em>Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2203.10807.pdf">CVPR 2022</a>.</p></li>
</ul>
</section>
<section id="openimage-o">
<h3>OpenImage-O<a class="headerlink" href="#openimage-o" title="Permalink to this heading">¶</a></h3>
<p>Curation:</p>
<p><strong>ViM: Out-of-distribution with virtual-logit matching</strong></p>
<ul class="simple">
<li><p>Authors: <em>Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2203.10807.pdf">CVPR 2022</a>.</p></li>
</ul>
<p>Original Dataset:</p>
<p><strong>The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.</strong></p>
<ul class="simple">
<li><p>Authors: <em>Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, et al.</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1811.00982.pdf">IJCV 2020</a>.</p></li>
</ul>
</section>
<section id="muad">
<h3>MUAD<a class="headerlink" href="#muad" title="Permalink to this heading">¶</a></h3>
<p><strong>MUAD: Multiple Uncertainties for Autonomous Driving Dataset</strong></p>
<ul class="simple">
<li><p>Authors: Gianni Franchi, Xuanlong Yu, Andrei Bursuc, et al.*</p></li>
<li><p>Paper: <cite>BMVC 2022 &lt;https://arxiv.org/pdf/2203.01437.pdf&gt;</cite></p></li>
</ul>
</section>
</section>
<section id="architectures">
<h2>Architectures<a class="headerlink" href="#architectures" title="Permalink to this heading">¶</a></h2>
<section id="resnet">
<h3>ResNet<a class="headerlink" href="#resnet" title="Permalink to this heading">¶</a></h3>
<p><strong>Deep Residual Learning for Image Recognition</strong></p>
<ul class="simple">
<li><p>Authors: <em>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1512.03385.pdf">CVPR 2016</a>.</p></li>
</ul>
</section>
<section id="wide-resnet">
<h3>Wide-ResNet<a class="headerlink" href="#wide-resnet" title="Permalink to this heading">¶</a></h3>
<p><strong>Wide Residual Networks</strong></p>
<ul class="simple">
<li><p>Authors: <em>Sergey Zagoruyko and Nikos Komodakis</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1605.07146.pdf">BMVC 2016</a>.</p></li>
</ul>
</section>
<section id="vgg">
<h3>VGG<a class="headerlink" href="#vgg" title="Permalink to this heading">¶</a></h3>
<p><strong>Very Deep Convolutional Networks for Large-Scale Image Recognition</strong></p>
<ul class="simple">
<li><p>Authors: <em>Karen Simonyan and Andrew Zisserman</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">ICLR 2015</a>.</p></li>
</ul>
</section>
</section>
<section id="layers">
<h2>Layers<a class="headerlink" href="#layers" title="Permalink to this heading">¶</a></h2>
<p><strong>Filter Response Normalization Layer: Eliminating Batch Dependence in the
Training of Deep Neural Networks</strong></p>
<ul class="simple">
<li><p>Authors: <em>Saurabh Singh and Shankar Krishnan</em></p></li>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/1911.09737.pdf">CVPR 2020</a>.</p></li>
</ul>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    
    <a href="contributing.html" class="btn btn-neutral" title="Contributing" accesskey="p"
      rel="prev"><img src="_static/images/chevron-right-teal.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2024, Adrien Lafage and Olivier Laurent.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">References</a><ul>
<li><a class="reference internal" href="#uncertainty-models">Uncertainty Models</a><ul>
<li><a class="reference internal" href="#deep-evidential-classification">Deep Evidential Classification</a></li>
<li><a class="reference internal" href="#beta-nll-in-deep-regression">Beta NLL in Deep Regression</a></li>
<li><a class="reference internal" href="#deep-evidential-regression">Deep Evidential Regression</a></li>
<li><a class="reference internal" href="#bayesian-neural-networks">Bayesian Neural Networks</a></li>
<li><a class="reference internal" href="#deep-ensembles">Deep Ensembles</a></li>
<li><a class="reference internal" href="#batchensemble">BatchEnsemble</a></li>
<li><a class="reference internal" href="#masksembles">Masksembles</a></li>
<li><a class="reference internal" href="#mimo">MIMO</a></li>
<li><a class="reference internal" href="#packed-ensembles">Packed-Ensembles</a></li>
<li><a class="reference internal" href="#monte-carlo-dropout">Monte-Carlo Dropout</a></li>
</ul>
</li>
<li><a class="reference internal" href="#data-augmentation-methods">Data Augmentation Methods</a><ul>
<li><a class="reference internal" href="#mixup">Mixup</a></li>
<li><a class="reference internal" href="#regmixup">RegMixup</a></li>
<li><a class="reference internal" href="#mixupio">MixupIO</a></li>
<li><a class="reference internal" href="#warping-mixup">Warping Mixup</a></li>
</ul>
</li>
<li><a class="reference internal" href="#post-processing-methods">Post-Processing Methods</a><ul>
<li><a class="reference internal" href="#temperature-vector-matrix-scaling">Temperature, Vector, &amp; Matrix scaling</a></li>
<li><a class="reference internal" href="#monte-carlo-batch-normalization">Monte-Carlo Batch Normalization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#metrics">Metrics</a><ul>
<li><a class="reference internal" href="#expected-calibration-error">Expected Calibration Error</a></li>
<li><a class="reference internal" href="#grouping-loss">Grouping Loss</a></li>
</ul>
</li>
<li><a class="reference internal" href="#datasets">Datasets</a><ul>
<li><a class="reference internal" href="#mnist">MNIST</a></li>
<li><a class="reference internal" href="#mnist-c">MNIST-C</a></li>
<li><a class="reference internal" href="#not-mnist">Not-MNIST</a></li>
<li><a class="reference internal" href="#cifar-10-cifar-100">CIFAR-10 &amp; CIFAR-100</a></li>
<li><a class="reference internal" href="#cifar-c-tiny-imagenet-c-imagenet-c">CIFAR-C, Tiny-ImageNet-C, ImageNet-C</a></li>
<li><a class="reference internal" href="#cifar-10-h">CIFAR-10 H</a></li>
<li><a class="reference internal" href="#cifar-10-n-cifar-100-n">CIFAR-10 N / CIFAR-100 N</a></li>
<li><a class="reference internal" href="#svhn">SVHN</a></li>
<li><a class="reference internal" href="#imagenet">ImageNet</a></li>
<li><a class="reference internal" href="#imagenet-a-imagenet-0">ImageNet-A &amp; ImageNet-0</a></li>
<li><a class="reference internal" href="#imagenet-r">ImageNet-R</a></li>
<li><a class="reference internal" href="#textures">Textures</a></li>
<li><a class="reference internal" href="#openimage-o">OpenImage-O</a></li>
<li><a class="reference internal" href="#muad">MUAD</a></li>
</ul>
</li>
<li><a class="reference internal" href="#architectures">Architectures</a><ul>
<li><a class="reference internal" href="#resnet">ResNet</a></li>
<li><a class="reference internal" href="#wide-resnet">Wide-ResNet</a></li>
<li><a class="reference internal" href="#vgg">VGG</a></li>
</ul>
</li>
<li><a class="reference internal" href="#layers">Layers</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="./"
    src="_static/documentation_options.js"></script>
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
  <script src="_static/jquery.js"></script>
  <script src="_static/underscore.js"></script>
  <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
  <script src="_static/doctools.js"></script>
  <script src="_static/sphinx_highlight.js"></script>
  <script src="_static/clipboard.min.js"></script>
  <script src="_static/copybutton.js"></script>
  

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://torch-uncertainty.github.io/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/ENSTA-U2IS-AI/torch-uncertainty" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>