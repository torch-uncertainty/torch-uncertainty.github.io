{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Evidential Regression on a Toy Example\n\nThis tutorial provides an introduction to probabilistic regression in TorchUncertainty.\n\nMore specifically, we present Deep Evidential Regression (DER) using a practical example. We demonstrate an application of DER by tackling the toy-problem of fitting $y=x^3$ using a Multi-Layer Perceptron (MLP) neural network model. \nThe output layer of the MLP provides a NormalInverseGamma distribution which is used to optimize the model, through its negative log-likelihood. \n\nDER represents an evidential approach to quantifying epistemic and aleatoric uncertainty in neural network regression models. \nThis method involves introducing prior distributions over the parameters of the Gaussian likelihood function. \nThen, the MLP model estimates the parameters of this evidential distribution.\n\n## Training a MLP with DER using TorchUncertainty models and PyTorch Lightning\n\nIn this part, we train a neural network, based on the model and routines already implemented in TU.\n\n### 1. Loading the utilities\n\nTo train a MLP with the DER loss function using TorchUncertainty, we have to load the following modules:\n\n- our TUTrainer \n- the model: mlp from torch_uncertainty.models.mlp\n- the regression training routine from torch_uncertainty.routines\n- the evidential objective: the DERLoss from torch_uncertainty.losses. This loss contains the classic NLL loss and a regularization term.\n- a dataset that generates samples from a noisy cubic function: Cubic from torch_uncertainty.datasets.regression\n\nWe also need to define an optimizer using torch.optim and the neural network utils within torch.nn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom lightning import LightningDataModule\nfrom torch import nn, optim\n\nfrom torch_uncertainty import TUTrainer\nfrom torch_uncertainty.models.mlp import mlp\nfrom torch_uncertainty.datasets.regression.toy import Cubic\nfrom torch_uncertainty.losses import DERLoss\nfrom torch_uncertainty.routines import RegressionRoutine\nfrom torch_uncertainty.layers.distributions import NormalInverseGammaLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. The Optimization Recipe\nWe use the Adam optimizer with a rate of 5e-4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optim_regression(\n    model: nn.Module,\n    learning_rate: float = 5e-4,\n):\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=0,\n    )\n    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Creating the necessary variables\n\nIn the following, we create a trainer to train the model, the same synthetic regression \ndatasets as in the original DER paper and the model, a simple MLP with 2 hidden layers of 64 neurons each.\nPlease note that this MLP finishes with a NormalInverseGammaLayer that interpret the outputs of the model\nas the parameters of a Normal Inverse Gamma distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = TUTrainer(accelerator=\"cpu\", max_epochs=50) #, enable_progress_bar=False)\n\n# dataset\ntrain_ds = Cubic(num_samples=1000)\nval_ds = Cubic(num_samples=300)\n\n# datamodule\ndatamodule = LightningDataModule.from_datasets(\n    train_ds, val_dataset=val_ds, test_dataset=val_ds, batch_size=32\n)\ndatamodule.training_task = \"regression\"\n\n# model\nmodel = mlp(\n    in_features=1,\n    num_outputs=4,\n    hidden_dims=[64, 64],\n    final_layer=NormalInverseGammaLayer,\n    final_layer_args={\"dim\": 1},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. The Loss and the Training Routine\nNext, we need to define the loss to be used during training. To do this, we\nset the weight of the regularizer of the DER Loss. After that, we define the\ntraining routine using the probabilistic regression training routine from\ntorch_uncertainty.routines. In this routine, we provide the model, the DER\nloss, and the optimization recipe.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = DERLoss(reg_weight=1e-2)\n\nroutine = RegressionRoutine(\n    probabilistic=True,\n    output_dim=1,\n    model=model,\n    loss=loss,\n    optim_recipe=optim_regression(model),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Gathering Everything and Training the Model\nFinally, we train the model using the trainer and the regression routine. We also\ntest the model using the same trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=routine, datamodule=datamodule)\ntrainer.test(model=routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Testing the Model\nWe can now test the model by plotting the predictions and the uncertainty estimates.\nIn this specific case, we can reproduce the results of the paper.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nwith torch.no_grad():\n    x = torch.linspace(-7, 7, 1000)\n\n    dists = model(x.unsqueeze(-1))\n    means = dists.loc.squeeze(1)\n    variances = torch.sqrt(dists.variance_loc).squeeze(1)\n\nfig, ax = plt.subplots(1, 1)\nax.plot(x, x**3, \"--r\", label=\"ground truth\", zorder=3)\nax.plot(x, means, \"-k\", label=\"predictions\")\nfor k in torch.linspace(0, 4, 4):\n    ax.fill_between(\n        x,\n        means - k * variances,\n        means + k * variances,\n        linewidth=0,\n        alpha=0.3,\n        edgecolor=None,\n        facecolor=\"blue\",\n        label=\"epistemic uncertainty\" if not k else None,\n    )\n\nplt.gca().set_ylim(-150, 150)\nplt.gca().set_xlim(-7, 7)\nplt.legend(loc=\"upper left\")\nplt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reference\n\n- **Deep Evidential Regression:** Alexander Amini, Wilko Schwarting, Ava Soleimany, & Daniela Rus. [NeurIPS 2020](https://arxiv.org/pdf/1910.02600).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}