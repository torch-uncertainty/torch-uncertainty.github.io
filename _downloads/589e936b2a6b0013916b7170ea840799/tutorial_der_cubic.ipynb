{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Evidential Regression on a Toy Example\n\nThis tutorial aims to provide an introductory overview of Deep Evidential Regression (DER) using a practical example. We demonstrate an application of DER by tackling the toy-problem of fitting $y=x^3$ using a Multi-Layer Perceptron (MLP) neural network model. The output layer of the MLP has four outputs, and is trained by minimizing the Normal Inverse-Gamma (NIG) loss function.\n\nDER represents an evidential approach to quantifying uncertainty in neural network regression models. This method involves introducing prior distributions over the parameters of the Gaussian likelihood function. Then, the MLP model estimate the parameters of the evidential distribution.\n\n## Training a MLP with DER using TorchUncertainty models and PyTorch Lightning\n\nIn this part, we train a neural network, based on the model and routines already implemented in TU.\n\n### 1. Loading the utilities\n\nTo train a MLP with the NIG loss function using TorchUncertainty, we have to load the following utilities from TorchUncertainty:\n\n- the cli handler: cli_main and argument parser: init_args\n- the model: mlp, which lies in the torch_uncertainty.baselines.regression.mlp module.\n- the regression training routine in the torch_uncertainty.routines.regression module.\n- the evidential objective: the NIGLoss, which lies in the torch_uncertainty.losses file\n- a dataset that generates samples from a noisy cubic function: Cubic, which lies in the torch_uncertainty.datasets.regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import LightningDataModule\nfrom torch_uncertainty import cli_main, init_args\nfrom torch_uncertainty.baselines.regression.mlp import mlp\nfrom torch_uncertainty.datasets.regression.toy import Cubic\nfrom torch_uncertainty.losses import NIGLoss\nfrom torch_uncertainty.routines.regression import RegressionSingle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to define an optimizer using torch.optim as well as the\nneural network utils withing torch.nn, as well as the partial util to provide\nthe modified default arguments for the NIG loss.\n\nWe also import sys to override the command line arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport sys\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Creating the Optimizer Wrapper\nWe use the Adam optimizer with the default learning rate of 0.001.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optim_regression(\n    model: nn.Module,\n    learning_rate: float = 5e-4,\n) -> dict:\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=0,\n    )\n    return {\n        \"optimizer\": optimizer,\n    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Creating the necessary variables\n\nIn the following, we need to define the root of the logs, and to\nfake-parse the arguments needed for using the PyTorch Lightning Trainer. We\nalso use the same synthetic regression task example as that used in the\noriginal DER paper.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = Path(os.path.abspath(\"\"))\n\n# We mock the arguments for the trainer\nsys.argv = [\"file.py\", \"--max_epochs\", \"50\", \"--enable_progress_bar\", \"False\"]\nargs = init_args()\n\nnet_name = \"logs/der-mlp-cubic\"\n\n# dataset\ntrain_ds = Cubic(num_samples=1000)\nval_ds = Cubic(num_samples=300)\ntest_ds = train_ds\n\n# datamodule\ndatamodule = LightningDataModule.from_datasets(\n    train_ds, val_dataset=val_ds, test_dataset=test_ds, batch_size=32\n)\ndatamodule.training_task = \"regression\"\n\n# model\nmodel = mlp(in_features=1, num_outputs=4, hidden_dims=[64, 64])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. The Loss and the Training Routine\nNext, we need to define the loss to be used during training. To do this, we\nredefine the default parameters for the NIG loss using the partial\nfunction from functools. After that, we define the training routine using\nthe regression training routine from torch_uncertainty.routines.regression. In\nthis routine, we provide the model, the NIG loss, and the optimizer,\nalong with the dist_estimation parameter, which refers to the number of\ndistribution parameters, and all the default arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = partial(\n    NIGLoss,\n    reg_weight=1e-2,\n)\n\nbaseline = RegressionSingle(\n    model=model,\n    loss=loss,\n    optimization_procedure=optim_regression,\n    dist_estimation=4,\n    **vars(args),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Gathering Everything and Training the Model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = cli_main(baseline, datamodule, root, net_name, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Testing the Model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom torch.nn import functional as F\n\nwith torch.no_grad():\n    x = torch.linspace(-7, 7, 1000).unsqueeze(-1)\n\n    logits = model(x)\n    means, v, alpha, beta = logits.split(1, dim=-1)\n\n    v = F.softplus(v)\n    alpha = 1 + F.softplus(alpha)\n    beta = F.softplus(beta)\n\n    vars = torch.sqrt(beta / (v * (alpha - 1)))\n\n    means.squeeze_(1)\n    vars.squeeze_(1)\n    x.squeeze_(1)\n\nfig, ax = plt.subplots(1, 1)\nax.plot(x, x**3, \"--r\", label=\"ground truth\", zorder=3)\nax.plot(x, means, \"-k\", label=\"predictions\")\nfor k in torch.linspace(0, 4, 4):\n    ax.fill_between(\n        x,\n        means - k * vars,\n        means + k * vars,\n        linewidth=0,\n        alpha=0.3,\n        edgecolor=None,\n        facecolor=\"blue\",\n        label=\"epistemic uncertainty\" if not k else None,\n    )\n\nplt.gca().set_ylim(-150, 150)\nplt.gca().set_xlim(-7, 7)\nplt.legend(loc=\"upper left\")\nplt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reference\n\n- **Deep Evidential Regression:** Alexander Amini, Wilko Schwarting, Ava Soleimany, & Daniela Rus. [NeurIPS 2020](https://arxiv.org/pdf/1910.02600).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}