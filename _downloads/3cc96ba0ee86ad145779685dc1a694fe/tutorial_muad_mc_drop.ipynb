{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Monte Carlo Dropout for Semantic Segmentation on MUAD\n\nThis tutorial demonstrates how to train a segmentation model on the MUAD dataset using TorchUncertainty.\nMUAD is a synthetic dataset designed for evaluating autonomous driving under diverse uncertainties.\nIt includes **10,413 images** across training, validation, and test sets, featuring adverse weather,\nlighting conditions, and out-of-distribution (OOD) objects. The dataset supports tasks like semantic segmentation,\ndepth estimation, and object detection.\n\nFor details and access, visit the [MUAD Website](https://muad-dataset.github.io/).\n\n## 1. Loading the utilities\n\nFirst, we load the following utilities from TorchUncertainty:\n\n- the TUTrainer which mostly handles the link with the hardware (accelerators, precision, etc)\n- the segmentation training & evaluation routine from torch_uncertainty.routines\n- the datamodule handling dataloaders: MUADDataModule from torch_uncertainty.datamodules\n- the model: small_unet from torch_uncertainty.models.segmentation.unet\n- the MC Dropout wrapper from torch_uncertainty.models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\nimport torchvision.transforms.v2.functional as F\nfrom huggingface_hub import hf_hub_download\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import tv_tensors\nfrom torchvision.transforms import v2\nfrom torchvision.utils import draw_segmentation_masks\n\nfrom torch_uncertainty import TUTrainer\nfrom torch_uncertainty.datamodules.segmentation import MUADDataModule\nfrom torch_uncertainty.models import mc_dropout\nfrom torch_uncertainty.models.segmentation.unet import small_unet\nfrom torch_uncertainty.routines import SegmentationRoutine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initializing the DataModule\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "muad_mean = MUADDataModule.mean\nmuad_std = MUADDataModule.std\n\ntrain_transform = v2.Compose(\n    [\n        v2.Resize(size=(256, 512), antialias=True),\n        v2.RandomHorizontalFlip(),\n        v2.ToDtype(\n            dtype={\n                tv_tensors.Image: torch.float32,\n                tv_tensors.Mask: torch.int64,\n                \"others\": None,\n            },\n            scale=True,\n        ),\n        v2.Normalize(mean=muad_mean, std=muad_std),\n    ]\n)\n\ntest_transform = v2.Compose(\n    [\n        v2.Resize(size=(256, 512), antialias=True),\n        v2.ToDtype(\n            dtype={\n                tv_tensors.Image: torch.float32,\n                tv_tensors.Mask: torch.int64,\n                \"others\": None,\n            },\n            scale=True,\n        ),\n        v2.Normalize(mean=muad_mean, std=muad_std),\n    ]\n)\n\n# datamodule providing the dataloaders to the trainer\ndatamodule = MUADDataModule(\n    root=\"./data\",\n    batch_size=10,\n    version=\"small\",\n    train_transform=train_transform,\n    test_transform=test_transform,\n    num_workers=4,\n)\ndatamodule.prepare_data()\ndatamodule.setup(\"fit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a validation input sample (and RGB image)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\nimg, tgt = datamodule.train[0]\nt_muad_mean = torch.tensor(muad_mean, device=img.device)\nt_muad_std = torch.tensor(muad_std, device=img.device)\nimg = img * t_muad_std[:, None, None] + t_muad_mean[:, None, None]\nimg = F.to_dtype(img, torch.uint8, scale=True)\nimg_pil = F.to_pil_image(img)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(img_pil)\nplt.axis(\"off\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the same image above but segmented.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tmp_tgt = tgt.masked_fill(tgt == 255, 21)\ntgt_masks = tmp_tgt == torch.arange(22, device=tgt.device)[:, None, None]\nimg_segmented = draw_segmentation_masks(\n    img, tgt_masks, alpha=1, colors=datamodule.train.color_palette\n)\nimg_pil = F.to_pil_image(img_segmented)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(img_pil)\nplt.axis(\"off\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Instantiating the Model\nWe create the model easily using the blueprint from torch_uncertainty.models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = small_unet(\n    in_channels=datamodule.num_channels,\n    num_classes=datamodule.num_classes,\n    bilinear=True,\n    dropout_rate=0.1,  # We use dropout to enable MC Dropout later\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute class weights to mitigate class inbalance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def enet_weighing(dataloader, num_classes, c=1.02):\n    \"\"\"Computes class weights as described in the ENet paper.\n\n        w_class = 1 / (ln(c + p_class)),\n\n    where c is usually 1.02 and p_class is the propensity score of that\n    class:\n\n        propensity_score = freq_class / total_pixels.\n\n    References:\n        https://arxiv.org/abs/1606.02147\n\n    Args:\n        dataloader (``data.Dataloader``): A data loader to iterate over the\n            dataset.\n        num_classes (``int``): The number of classes.\n        c (``int``, optional): AN additional hyper-parameter which restricts\n            the interval of values for the weights. Default: 1.02.\n        ignore_indexes (``list``, optional): A list of indexes to ignore\n            when computing the weights. Default to `None`.\n\n    \"\"\"\n    class_count = 0\n    total = 0\n    for _, label in dataloader:\n        label = label.cpu()\n        # Flatten label\n        flat_label = label.flatten()\n        flat_label = flat_label[flat_label != 255]\n        flat_label = flat_label[flat_label < num_classes]\n\n        # Sum up the number of pixels of each class and the total pixel\n        # counts for each label\n        class_count += torch.bincount(flat_label, minlength=num_classes)\n        total += flat_label.size(0)\n\n    # Compute propensity score and then the weights for each class\n    propensity_score = class_count / total\n\n    return 1 / (torch.log(c + propensity_score))\n\n\nclass_weights = enet_weighing(datamodule.val_dataloader(), datamodule.num_classes)\nprint(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the training parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 2e-4\nLR_DECAY_EPOCHS = 20\nLR_DECAY = 0.1\nNB_EPOCHS = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Loss, the Routine, and the Trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We build the optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# Learning rate decay scheduler\nlr_updater = lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_EPOCHS, gamma=LR_DECAY)\n\n# Segmentation Routine\nseg_routine = SegmentationRoutine(\n    model=model,\n    num_classes=datamodule.num_classes,\n    loss=torch.nn.CrossEntropyLoss(weight=class_weights),\n    optim_recipe={\"optimizer\": optimizer, \"lr_scheduler\": lr_updater},\n)\n\ntrainer = TUTrainer(accelerator=\"gpu\", devices=1, max_epochs=NB_EPOCHS, enable_progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=seg_routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Testing the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = trainer.test(datamodule=datamodule, ckpt_path=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Loading a pre-trained model\nLet's now load a fully trained model to continue this tutorial\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_path = hf_hub_download(repo_id=\"torch-uncertainty/muad_tutorials\", filename=\"small_unet.pth\")\nmodel.load_state_dict(torch.load(model_path))\n# Replace the model in the routine\nseg_routine.model = model\n# Test the model\nresults = trainer.test(model=seg_routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Uncertainty evaluations with MCP\nHere we will just use as confidence score the Maximum class probability (MCP)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\nimg, target = datamodule.test[sample_idx]\n\nbatch_img = img.unsqueeze(0)\nbatch_target = target.unsqueeze(0)\nmodel.eval()\nwith torch.no_grad():\n    # Forward propagation\n    output_probs = model(batch_img).softmax(dim=1)\n    # remove the batch dimension\n    output_probs = output_probs.squeeze(0)\n    confidence, pred = output_probs.max(0)\n\n# Undo normalization on the image and convert to uint8.\nimg = img * t_muad_std[:, None, None] + t_muad_mean[:, None, None]\nimg = F.to_dtype(img, torch.uint8, scale=True)\n\ntmp_target = target.masked_fill(target == 255, 21)\ntarget_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\nimg_segmented = draw_segmentation_masks(\n    img, target_masks, alpha=1, colors=datamodule.test.color_palette\n)\n\npred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n\npred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=datamodule.test.color_palette)\n\n\nif confidence.ndim == 2:\n    confidence = confidence.unsqueeze(0)\n\nimg = F.to_pil_image(F.resize(img, 1024))\nimg_segmented = F.to_pil_image(F.resize(img_segmented, 1024))\npred_img = F.to_pil_image(F.resize(pred_img, 1024))\nconfidence_img = F.to_pil_image(F.resize(confidence, 1024))\n\n\nfig, axs = plt.subplots(1, 4, figsize=(25, 7))\nimages = [img, img_segmented, pred_img, confidence_img]\n\nfor ax, im in zip(axs, images, strict=False):\n    ax.imshow(im)\n    ax.axis(\"off\")\n\nplt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01, wspace=0.05)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Apply the MC Dropout wrapper\nThis technique decribed in this [paper](https://arxiv.org/abs/1506.02142/)\nallow us to have a better confidence score by using the dropout during test time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We wrap the model with the MC Dropout wrapper from torch_uncertainty.models.mc_dropout\nnum_estimators = 10\nmc_model = mc_dropout(\n    model,\n    num_estimators=num_estimators,\n    last_layer=False,  # We do not want to apply dropout on the last layer\n    on_batch=False,  # To reduce memory usage, we execute the forward passes sequentially\n)\n\nseg_routine = SegmentationRoutine(\n    model=mc_model,\n    num_classes=datamodule.num_classes,\n    loss=None,  # No loss needed for testing\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Testing the MC Dropout model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = trainer.test(model=seg_routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Uncertainty evaluations with MCP\nHere the confidence score is the Maximum class probability on the averaged probabilities\nof the different estimators.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\nimg, target = datamodule.test[sample_idx]\n\nbatch_img = img.unsqueeze(0)\nbatch_target = target.unsqueeze(0)\nmc_model.eval()\nwith torch.no_grad():\n    # Forward propagation\n    output_probs_per_est = mc_model(batch_img).softmax(dim=1)\n    output_probs = output_probs_per_est.mean(0)  # Average over the estimators\n    # remove the batch dimension\n    confidence, pred = output_probs.max(0)\n\n# Undo normalization on the image and convert to uint8.\nimg = img * t_muad_std[:, None, None] + t_muad_mean[:, None, None]\nimg = F.to_dtype(img, torch.uint8, scale=True)\n\ntmp_target = target.masked_fill(target == 255, 21)\ntarget_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\nimg_segmented = draw_segmentation_masks(\n    img, target_masks, alpha=1, colors=datamodule.test.color_palette\n)\n\npred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n\npred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=datamodule.test.color_palette)\n\n\nif confidence.ndim == 2:\n    confidence = confidence.unsqueeze(0)\n\nimg = F.to_pil_image(F.resize(img, 1024))\nimg_segmented = F.to_pil_image(F.resize(img_segmented, 1024))\npred_img = F.to_pil_image(F.resize(pred_img, 1024))\nconfidence_img = F.to_pil_image(F.resize(confidence, 1024))\n\n\nfig, axs = plt.subplots(1, 4, figsize=(25, 7))\nimages = [img, img_segmented, pred_img, confidence_img]\n\nfor ax, im in zip(axs, images, strict=False):\n    ax.imshow(im)\n    ax.axis(\"off\")\n\nplt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01, wspace=0.05)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}