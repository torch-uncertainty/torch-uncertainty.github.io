{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a LeNet with Monte-Carlo Dropout\n\nIn this tutorial, we'll train a LeNet classifier on the MNIST dataset using Monte-Carlo Dropout (MC Dropout), a computationally efficient Bayesian approximation method. To estimate the predictive mean and uncertainty (variance), we perform multiple forward passes through the network with dropout layers enabled in ``train`` mode.\n\nFor more information on Monte-Carlo Dropout, we refer the reader to the following resources:\n\n- What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? [NeurIPS 2017](https://browse.arxiv.org/pdf/1703.04977.pdf)\n- Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning [PMLR 2016](https://browse.arxiv.org/pdf/1506.02142.pdf)\n\n## Training a LeNet with MC Dropout using TorchUncertainty models and PyTorch Lightning\n\nIn this part, we train a LeNet with dropout layers, based on the model and routines already implemented in TU.\n\n### 1. Loading the utilities\n\nFirst, we have to load the following utilities from TorchUncertainty:\n\n- the cli handler: cli_main and argument parser: init_args\n- the model: LeNet, which lies in torch_uncertainty.models\n- the classification training routine in the torch_uncertainty.training.classification module\n- the datamodule that handles dataloaders: MNISTDataModule, which lies in the torch_uncertainty.datamodule\n- the optimizer wrapper in the torch_uncertainty.optimization_procedures module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty import cli_main, init_args\nfrom torch_uncertainty.datamodules import MNISTDataModule\nfrom torch_uncertainty.baselines import ResNet\nfrom torch_uncertainty.models.lenet import lenet\nfrom torch_uncertainty.routines.classification import ClassificationEnsemble\nfrom torch_uncertainty.optimization_procedures import optim_cifar10_resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also need import the neural network utils withing `torch.nn`.\n\nWe also import ArgvContext to avoid using the jupyter arguments as cli\narguments, and therefore avoid errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom functools import partial\nfrom pathlib import Path\n\nfrom torch import nn\nfrom cli_test_helpers import ArgvContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Creating the necessary variables\n\nIn the following, we will need to define the root of the datasets and the\nlogs, and to fake-parse the arguments needed for using the PyTorch Lightning\nTrainer. We also create the datamodule that handles the MNIST dataset,\ndataloaders and transforms. Finally, we create the model using the\nblueprint from torch_uncertainty.models.\n\nIt is important to specify the arguments ``version`` as ``mc-dropout``,\n``num_estimators`` and the ``dropout_rate`` to use Monte Carlo dropout.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = Path(os.path.abspath(\"\"))\n\n# We mock the arguments for the trainer\nwith ArgvContext(\n    \"file.py\",\n    \"--max_epochs\",\n    \"1\",\n    \"--enable_progress_bar\",\n    \"False\",\n    \"--version\",\n    \"mc-dropout\",\n    \"--dropout_rate\",\n    \"0.5\",\n    \"--num_estimators\",\n    \"16\",\n):\n    args = init_args(network=ResNet, datamodule=MNISTDataModule)\n\nnet_name = \"mc-dropout-lenet-mnist\"\n\n# datamodule\nargs.root = str(root / \"data\")\ndm = MNISTDataModule(**vars(args))\n\n\nmodel = lenet(\n    in_channels=dm.num_channels,\n    num_classes=dm.num_classes,\n    dropout_rate=args.dropout_rate,\n    num_estimators=args.num_estimators,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. The Loss and the Training Routine\nThis is a classification problem, and we use CrossEntropyLoss as the likelihood.\nWe define the training routine using the classification training routine from\ntorch_uncertainty.training.classification. We provide the number of classes\nand channels, the optimizer wrapper, the dropout rate, and the number of\nforward passes to perform through the network, as well as all the default\narguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "baseline = ClassificationEnsemble(\n    num_classes=dm.num_classes,\n    model=model,\n    loss=nn.CrossEntropyLoss,\n    optimization_procedure=optim_cifar10_resnet18,\n    **vars(args),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Gathering Everything and Training the Model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = cli_main(baseline, dm, root, net_name, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Testing the Model\nNow that the model is trained, let's test it on MNIST. Don't forget to call\n.eval() to enable dropout at inference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nimport numpy as np\n\n\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\ndataiter = iter(dm.val_dataloader())\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images[:4, ...]))\nprint(\"Ground truth: \", \" \".join(f\"{labels[j]}\" for j in range(4)))\n\nbaseline.eval()\nlogits = baseline(images).reshape(16, 128, 10)\n\nprobs = torch.nn.functional.softmax(logits, dim=-1)\n\n\nfor j in range(4):\n    values, predicted = torch.max(probs[:, j], 1)\n    print(\n        f\"Predicted digits for the image {j}: \",\n        \" \".join([str(image_id.item()) for image_id in predicted]),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "approximation of the posterior distribution.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}