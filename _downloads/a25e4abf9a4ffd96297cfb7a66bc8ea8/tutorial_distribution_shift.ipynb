{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Evaluating Model Performance Under Distribution Shift with TorchUncertainty\n\nIn this tutorial, we explore how to assess a model's robustness when faced with distribution shifts.\nSpecifically, we will:\n\n- Shortly train a **ResNet18** model on the standard **CIFAR-10** dataset.\n- Evaluate its performance on both the original CIFAR-10 test set and a corrupted version of CIFAR-10 to simulate distribution shift.\n- Analyze the model's performance and robustness under these conditions.\n\nBy the end of this tutorial, you will understand how to use TorchUncertainty to evaluate and interpret model behavior under distribution shifts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup\n\nFirst, we need to import the necessary libraries and set up our environment.\nThis includes importing PyTorch, TorchUncertainty components, and TorchUncertainty's Trainer (built on top of Lightning's).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n\nfrom torch_uncertainty import TUTrainer\nfrom torch_uncertainty.datamodules import CIFAR10DataModule\nfrom torch_uncertainty.models.classification.resnet import resnet\nfrom torch_uncertainty.routines.classification import ClassificationRoutine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataModule Setup\n\nTorchUncertainty provides convenient DataModules for standard datasets like CIFAR-10.\nDataModules handle data loading, preprocessing, and batching, simplifying the data pipeline. Each datamodule\nalso include the corresponding out-of-distribution and distribution shift datasets, which are then used by the routine.\nFor CIFAR-10, the corresponding distribution-shift dataset is CIFAR-10C as used in the community.\nTo enable Distribution Shift evaluation, activate the `eval_shift` flag as done below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the CIFAR-10 DataModule\ndatamodule = CIFAR10DataModule(\n    root=\"./data\",\n    batch_size=512,\n    num_workers=8,\n    eval_shift=True,\n    shift_severity=5,  # Set severity level of the corruption (1 to 5): max-strength!\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CIFAR-10C\n\nCIFAR-10C is a transformed version of CIFAR-10 test set. Dan Hendrycks and Thomas Dietterich applied computer vision\ntransforms, known as corruptions to degrade the quality of the image and test deep learning models in adverse conditions.\nThere are 15 (+4 optional) corruptions in total, including noise, blur, weather effects, etc. Each corruption has 5 different\nlevels of severity ranging from small corruptions to very strong effects on the image. You can set the desired corruption level with\nthe shift-severity argument. We refer to [1] for more details.\nYou can get a more detailed overview and examples of the corruptions on the corresponding tutorial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# These lines are usually not necessary (they are called by the Trainer),\n# but we want to get access to the dataset before training\ndatamodule.prepare_data()\ndatamodule.setup(\"test\")\n\n# Let's check the CIFAR-10C, it should contain (15+4)*10000 images for the selected severity level.\nprint(datamodule.shift)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization\n\nWe will use the ResNet18 architecture, a widely adopted convolutional neural network known for its deep residual learning capabilities.\nThe model is initialized with 10 output classes corresponding to the CIFAR-10 dataset categories.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the ResNet18 model with 10 output classes\nmodel = resnet(arch=18, in_channels=3, num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Classification Routine\n\nThe `ClassificationRoutine` is one of the most crucial building blocks in TorchUncertainty.\nIt streamlines the training and evaluation processes.\nIt integrates the model, loss function, and optimizer into a cohesive routine compatible with PyTorch Lightning's Trainer.\nThis abstraction simplifies the implementation of standard training loops and evaluation protocols.\nTo come back to what matters in this tutorial, the routine also handles the evaluation of the performance\nof the model under distribution shift detection. To enable it, activate the `eval_shift` flag. Note that you can also evaluate\nthe Out-of-distribution detection at the same time by also setting `eval_ood` to True.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the loss function: Cross-Entropy Loss for multi-class classification\ncriterion = nn.CrossEntropyLoss()\n\n# Define the optimizer: Adam optimizer with a learning rate of 0.001\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Initialize the ClassificationRoutine with the model, number of classes, loss function, and optimizer\nroutine = ClassificationRoutine(\n    model=model, num_classes=10, loss=criterion, optim_recipe=optimizer, eval_shift=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Model\n\nWith the routine defined, we can now set up the TUTrainer and commence training.\nThe TUTrainer handles the training loop, including epoch management, logging, and checkpointing.\nWe specify the maximum number of epochs, the precision and the device to be used.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the TUTrainer with a maximum of 10 epochs and the specified device\ntrainer = TUTrainer(\n    max_epochs=10, precision=\"16-mixed\", accelerator=\"cuda\", devices=1, enable_progress_bar=False\n)\n\n# Begin training the model using the CIFAR-10 DataModule\ntrainer.fit(routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating on In-Distribution and Distribution-shifted Data\n\nNow that the model is trained, we can evaluate its performance on the original in-distribution test set,\nas well as the distribution-shifted set. Typing the next line will automatically compute the in-distribution\nmetrics as well as their values on the distribution-shifted set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained model on the original CIFAR-10 test set and on CIFAR-10C\nresults = trainer.test(routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution-shift metrics\n\nThe distribution shift metrics are computed only when the `eval_shift` flag of the routine is True.\nIn this case, the values of the metrics are shown last. They correspond to the in-distribution metrics but\ncomputed on the distribution-shifted datasets, hence the worse results.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}