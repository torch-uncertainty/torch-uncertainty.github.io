{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Probabilistic Regression\n\nThis tutorial aims to provide an overview of some utilities in TorchUncertainty for probabilistic regression.\n\n## Building a MLP for Probabilistic Regression using TorchUncertainty distribution layers\n\nIn this section we cover the building of a very simple MLP outputting Normal distribution parameters.\n\n### 1. Loading the utilities\n\nWe disable some logging and warnings to keep the output clean.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch import nn\n\nimport logging\nlogging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Building the MLP model\n\nTo create a MLP model estimating a Normal distribution, we use the NormalLinear layer.\nThis layer is a wrapper around the nn.Linear layer, which outputs the location and scale of a Normal distribution.\nNote that any other distribution layer from TU can be used in the same way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.layers.distributions import NormalLinear\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, 50)\n        self.fc2 = NormalLinear(\n            base_layer=nn.Linear,\n            event_dim=out_features,\n            in_features=50,\n        )\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Setting up the data\n\nWe use the UCI Kin8nm dataset, which is a regression dataset with 8 features and 8192 samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.datamodules import UCIRegressionDataModule\n\n# datamodule\ndatamodule = UCIRegressionDataModule(\n    root=\"data\",\n    batch_size=32,\n    dataset_name=\"kin8nm\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Setting up the model and trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty import TUTrainer\n\ntrainer = TUTrainer(\n    accelerator=\"cpu\",\n    max_epochs=5,\n    enable_progress_bar=False,\n)\n\nmodel = MLP(in_features=8, out_features=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. The Loss, the Optimizer and the Training Routine\n\nWe use the DistributionNLLLoss to compute the negative log-likelihood of the Normal distribution.\nNote that this loss can be used with any Distribution from torch.distributions.\nFor the optimizer, we use the Adam optimizer with a learning rate of 5e-3.\nFinally, we create a RegressionRoutine to train the model. We indicate that the output dimension is 1 and the distribution family is \"normal\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.losses import DistributionNLLLoss\nfrom torch_uncertainty.routines import RegressionRoutine\n\nloss = DistributionNLLLoss()\n\ndef optim_regression(\n    model: nn.Module,\n    learning_rate: float = 5e-3,\n):\n    return torch.optim.Adam(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=0,\n    )\n\nroutine = RegressionRoutine(\n    output_dim=1,\n    model=model,\n    loss=loss,\n    optim_recipe=optim_regression(model),\n    dist_family=\"normal\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Training the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=routine, datamodule=datamodule)\nresults = trainer.test(model=routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Benchmarking different distributions\n\nOur MLP model assumes a Normal distribution as the output. However, we could be interested in comparing the performance of different distributions.\nTorchUncertainty provides a simple way to do this using the get_dist_linear_layer() function.\nLet us rewrite the MLP model to use it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.layers.distributions import get_dist_linear_layer\n\nclass MLP(nn.Module):\n    def __init__(self, in_features: int, out_features: int, dist_family: str):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, 50)\n        dist_layer = get_dist_linear_layer(dist_family)\n        self.fc2 = dist_layer(\n            base_layer=nn.Linear,\n            event_dim=out_features,\n            in_features=50,\n        )\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now train the model with different distributions.\nLet us train the model with a Normal, Laplace, Student's t, and Cauchy distribution.\nNote that we use the mode as the point-wise estimate of the distribution as the mean\nis not defined for the Cauchy distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for dist_family in [\"normal\", \"laplace\", \"student\", \"cauchy\"]:\n    print(\"#\" * 50)\n    print(f\">>> Training with {dist_family} distribution\")\n    print(\"#\" * 50)\n    trainer = TUTrainer(\n        accelerator=\"cpu\",\n        max_epochs=10,\n        enable_model_summary=False,\n        enable_progress_bar=False,\n    )\n    model = MLP(in_features=8, out_features=1, dist_family=dist_family)\n    routine = RegressionRoutine(\n        output_dim=1,\n        model=model,\n        loss=loss,\n        optim_recipe=optim_regression(model),\n        dist_family=dist_family,\n        dist_estimate=\"mode\",\n    )\n    trainer.fit(model=routine, datamodule=datamodule)\n    trainer.test(model=routine, datamodule=datamodule)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}