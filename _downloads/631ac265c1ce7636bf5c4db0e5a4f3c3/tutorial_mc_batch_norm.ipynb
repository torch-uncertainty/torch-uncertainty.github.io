{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a LeNet with Monte Carlo Batch Normalization\n\nIn this tutorial, we will train a LeNet classifier on the MNIST dataset using Monte-Carlo Batch Normalization (MCBN), a post-hoc Bayesian approximation method. \n\n## Training a LeNet with MCBN using TorchUncertainty models and PyTorch Lightning\nIn this part, we train a LeNet with batch normalization layers, based on the model and routines already implemented in TU.\n\n### 1. Loading the utilities\n\nFirst, we have to load the following utilities from TorchUncertainty:\n\n- the cli handler: cli_main and argument parser: init_args\n- the datamodule that handles dataloaders: MNISTDataModule, which lies in the torch_uncertainty.datamodule\n- the model: LeNet, which lies in torch_uncertainty.models\n- the mc-batch-norm wrapper: mc_dropout, which lies in torch_uncertainty.models\n- a resnet baseline to get the command line arguments: ResNet, which lies in torch_uncertainty.baselines\n- the classification training routine in the torch_uncertainty.training.classification module\n- the optimizer wrapper in the torch_uncertainty.optimization_procedures module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty import cli_main, init_args\nfrom torch_uncertainty.datamodules import MNISTDataModule\nfrom torch_uncertainty.models.lenet import lenet\nfrom torch_uncertainty.post_processing.mc_batch_norm import MCBatchNorm\nfrom torch_uncertainty.baselines.classification import ResNet\nfrom torch_uncertainty.routines.classification import ClassificationSingle\nfrom torch_uncertainty.optimization_procedures import optim_cifar10_resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also need import the neural network utils withing `torch.nn`.\n\nWe also import ArgvContext to avoid using the jupyter arguments as cli\narguments, and therefore avoid errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom pathlib import Path\n\nfrom torch import nn\nfrom cli_test_helpers import ArgvContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Creating the necessary variables\n\nIn the following, we will need to define the root of the datasets and the\nlogs, and to fake-parse the arguments needed for using the PyTorch Lightning\nTrainer. We also create the datamodule that handles the MNIST dataset,\ndataloaders and transforms. We create the model using the\nblueprint from torch_uncertainty.models and we wrap it into mc-dropout.\n\nIt is important to specify the arguments ``version`` as ``mc-dropout``,\n``num_estimators`` and the ``dropout_rate`` to use Monte Carlo dropout.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = Path(os.path.abspath(\"\"))\n\n# We mock the arguments for the trainer\nwith ArgvContext(\n    \"file.py\",\n    \"--max_epochs\",\n    \"1\",\n    \"--enable_progress_bar\",\n    \"False\",\n    \"--num_estimators\",\n    \"8\",\n    \"--max_epochs\",\n    \"2\"\n):\n    args = init_args(network=ResNet, datamodule=MNISTDataModule)\n\nnet_name = \"logs/lenet-mnist\"\n\n# datamodule\nargs.root = str(root / \"data\")\ndm = MNISTDataModule(**vars(args))\n\n\nmodel = lenet(\n    in_channels=dm.num_channels,\n    num_classes=dm.num_classes,\n    norm = nn.BatchNorm2d,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. The Loss and the Training Routine\nThis is a classification problem, and we use CrossEntropyLoss as the likelihood.\nWe define the training routine using the classification training routine from\ntorch_uncertainty.training.classification. We provide the number of classes\nand channels, the optimizer wrapper, the dropout rate, and the number of\nforward passes to perform through the network, as well as all the default\narguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "baseline = ClassificationSingle(\n    num_classes=dm.num_classes,\n    model=model,\n    loss=nn.CrossEntropyLoss,\n    optimization_procedure=optim_cifar10_resnet18,\n    **vars(args),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Gathering Everything and Training the Model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = cli_main(baseline, dm, root, net_name, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Wrapping the Model in a MCBatchNorm\nWe can now wrap the model in a MCBatchNorm to add stochasticity to the\npredictions. We specify that the BatchNorm layers are to be converted to\nMCBatchNorm layers, and that we want to use 8 stochastic estimators.\nThe amount of stochasticity is controlled by the ``mc_batch_size`` argument.\nThe larger the ``mc_batch_size``, the more stochastic the predictions will be.\nThe authors suggest 32 as a good value for ``mc_batch_size`` but we use 4 here\nto highlight the effect of stochasticity on the predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "baseline.model = MCBatchNorm(baseline.model, num_estimators=8, convert=True, mc_batch_size=32)\nbaseline.model.fit(dm.train)\nbaseline.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Testing the Model\nNow that the model is trained, let's test it on MNIST. Don't forget to call\n.eval() to enable Monte Carlo batch normalization at inference.\nIn this tutorial, we plot the most uncertain images, i.e. the images for which\nthe variance of the predictions is the highest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nimport numpy as np\n\n\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\ndataiter = iter(dm.val_dataloader())\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images[:4, ...]))\nprint(\"Ground truth: \", \" \".join(f\"{labels[j]}\" for j in range(4)))\n\nbaseline.eval()\nlogits = baseline(images).reshape(8, 128, 10)\n\nprobs = torch.nn.functional.softmax(logits, dim=-1)\n\n\nfor j in sorted(probs.var(0).sum(-1).topk(4).indices):\n    values, predicted = torch.max(probs[:, j], 1)\n    print(\n        f\"Predicted digits for the image {j}: \",\n        \" \".join([str(image_id.item()) for image_id in predicted]),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The predictions are mostly erroneous, which is expected since we selected\nthe most uncertain images. We also see that there stochasticity in the\npredictions, as the predictions for the same image differ depending on the\nstochastic estimator used.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}