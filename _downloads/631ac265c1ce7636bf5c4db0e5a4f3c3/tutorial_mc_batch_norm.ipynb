{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a LeNet with Monte Carlo Batch Normalization\n\nIn this tutorial, we will train a LeNet classifier on the MNIST dataset using Monte-Carlo Batch Normalization (MCBN), a post-hoc Bayesian approximation method. \n\n## Training a LeNet with MCBN using TorchUncertainty models and PyTorch Lightning\nIn this part, we train a LeNet with batch normalization layers, based on the model and routines already implemented in TU.\n\n### 1. Loading the utilities\n\nFirst, we have to load the following utilities from TorchUncertainty:\n\n- the Trainer from Lightning\n- the datamodule handling dataloaders: MNISTDataModule from torch_uncertainty.datamodules\n- the model: LeNet, which lies in torch_uncertainty.models\n- the MC Batch Normalization wrapper: mc_batch_norm, which lies in torch_uncertainty.post_processing\n- the classification training routine in the torch_uncertainty.routines\n- an optimization recipe in the torch_uncertainty.optim_recipes module.\n\nWe also need import the neural network utils within `torch.nn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nfrom lightning import Trainer\nfrom torch import nn\n\nfrom torch_uncertainty.datamodules import MNISTDataModule\nfrom torch_uncertainty.models.lenet import lenet\nfrom torch_uncertainty.optim_recipes import optim_cifar10_resnet18\nfrom torch_uncertainty.post_processing.mc_batch_norm import MCBatchNorm\nfrom torch_uncertainty.routines import ClassificationRoutine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Creating the necessary variables\nIn the following, we define the root of the datasets and the\nlogs. We also create the datamodule that handles the MNIST dataset\ndataloaders and transforms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(accelerator=\"cpu\", max_epochs=2, enable_progress_bar=False)\n\n# datamodule\nroot = Path(\"\") / \"data\"\ndatamodule = MNISTDataModule(root, batch_size=128)\n\n\nmodel = lenet(\n    in_channels=datamodule.num_channels,\n    num_classes=datamodule.num_classes,\n    norm=nn.BatchNorm2d,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. The Loss and the Training Routine\nThis is a classification problem, and we use CrossEntropyLoss as likelihood.\nWe define the training routine using the classification training routine from\ntorch_uncertainty.training.classification. We provide the number of classes,\nand the optimization recipe.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "routine = ClassificationRoutine(\n    num_classes=datamodule.num_classes,\n    model=model,\n    loss=nn.CrossEntropyLoss(),\n    optim_recipe=optim_cifar10_resnet18(model),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Gathering Everything and Training the Model\nYou can also save the results in a variable by saving the output of\n`trainer.test`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=routine, datamodule=datamodule)\ntrainer.test(model=routine, datamodule=datamodule);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Wrapping the Model in a MCBatchNorm\nWe can now wrap the model in a MCBatchNorm to add stochasticity to the\npredictions. We specify that the BatchNorm layers are to be converted to\nMCBatchNorm layers, and that we want to use 8 stochastic estimators.\nThe amount of stochasticity is controlled by the ``mc_batch_size`` argument.\nThe larger the ``mc_batch_size``, the more stochastic the predictions will be.\nThe authors suggest 32 as a good value for ``mc_batch_size`` but we use 4 here\nto highlight the effect of stochasticity on the predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "routine.model = MCBatchNorm(\n    routine.model, num_estimators=8, convert=True, mc_batch_size=16\n)\nroutine.model.fit(datamodule.train)\nroutine.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Testing the Model\nNow that the model is trained, let's test it on MNIST. Don't forget to call\n.eval() to enable Monte Carlo batch normalization at inference.\nIn this tutorial, we plot the most uncertain images, i.e. the images for which\nthe variance of the predictions is the highest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\n\n\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\ndataiter = iter(datamodule.val_dataloader())\nimages, labels = next(dataiter)\n\nroutine.eval()\nlogits = routine(images).reshape(8, 128, 10)\n\nprobs = torch.nn.functional.softmax(logits, dim=-1)\nmost_uncertain = sorted(probs.var(0).sum(-1).topk(4).indices)\n\n# print images\nimshow(torchvision.utils.make_grid(images[most_uncertain, ...]))\nprint(\"Ground truth: \", \" \".join(f\"{labels[j]}\" for j in range(4)))\n\nfor j in most_uncertain:\n    values, predicted = torch.max(probs[:, j], 1)\n    print(\n        f\"Predicted digits for the image {j}: \",\n        \" \".join([str(image_id.item()) for image_id in predicted]),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The predictions are mostly erroneous, which is expected since we selected\nthe most uncertain images. We also see that there stochasticity in the\npredictions, as the predictions for the same image differ depending on the\nstochastic estimator used.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}