{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Evidential Classification on a Toy Example\n\nThis tutorial aims to provide an introductory overview of Deep Evidential Classification (DEC) using a practical example. We demonstrate an application of DEC by tackling the toy-problem of fitting the MNIST dataset using a Multi-Layer Perceptron (MLP) neural network model. The output of the MLP is modeled as a Dirichlet distribution. The MLP is trained by minimizing the DEC loss function, composed of a Bayesian risk square error loss and a regularization term based on KL Divergence.\n\nDEC represents an evidential approach to quantifying uncertainty in neural network classification models. This method involves introducing prior distributions over the parameters of the Categorical likelihood function. Then, the MLP model estimates the parameters of the evidential distribution.\n\n## Training a LeNet with DEC using TorchUncertainty models\n\nIn this part, we train a neural network, based on the model and routines already implemented in TU.\n\n### 1. Loading the utilities\n\nTo train a LeNet with the DEC loss function using TorchUncertainty, we have to load the following utilities from TorchUncertainty:\n\n- the cli handler: cli_main and argument parser: init_args\n- the model: LeNet, which lies in torch_uncertainty.models\n- the classification training routine in the torch_uncertainty.training.classification module\n- the evidential objective: the DECLoss, which lies in the torch_uncertainty.losses file\n- the datamodule that handles dataloaders: MNISTDataModule, which lies in the torch_uncertainty.datamodule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty import cli_main, init_args\nfrom torch_uncertainty.models.lenet import lenet\nfrom torch_uncertainty.routines.classification import ClassificationSingle\nfrom torch_uncertainty.losses import DECLoss\nfrom torch_uncertainty.datamodules import MNISTDataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to define an optimizer using torch.optim as well as the\nneural network utils withing torch.nn, as well as the partial util to provide\nthe modified default arguments for the DEC loss.\n\nWe also import ArgvContext to avoid using the jupyter arguments as cli\narguments, and therefore avoid errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\nfrom cli_test_helpers import ArgvContext\nfrom torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Creating the Optimizer Wrapper\nWe follow the official implementation in DEC, use the Adam optimizer\nwith the default learning rate of 0.001 and a step scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optim_lenet(model: nn.Module) -> dict:\n    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.005)\n    exp_lr_scheduler = optim.lr_scheduler.StepLR(\n        optimizer, step_size=7, gamma=0.1\n    )\n    return {\"optimizer\": optimizer, \"lr_scheduler\": exp_lr_scheduler}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Creating the necessary variables\n\nIn the following, we need to define the root of the logs, and to\nfake-parse the arguments needed for using the PyTorch Lightning Trainer. We\nalso use the same MNIST classification example as that used in the\noriginal DEC paper. We only train for 5 epochs for the sake of time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = Path(os.path.abspath(\"\"))\n\n# We mock the arguments for the trainer. Replace with 25 epochs on your machine.\nwith ArgvContext(\n    \"file.py\",\n    \"--max_epochs\",\n    \"5\",\n    \"--enable_progress_bar\",\n    \"True\",\n):\n    args = init_args(datamodule=MNISTDataModule)\n\nnet_name = \"logs/dec-lenet-mnist\"\n\n# datamodule\nargs.root = str(root / \"data\")\ndm = MNISTDataModule(**vars(args))\n\n\nmodel = lenet(\n    in_channels=dm.num_channels,\n    num_classes=dm.num_classes,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. The Loss and the Training Routine\nNext, we need to define the loss to be used during training. To do this, we\nredefine the default parameters for the DEC loss using the partial\nfunction from functools. After that, we define the training routine using\nthe single classification model training routine from\ntorch_uncertainty.routines.classification.ClassificationSingle.\nIn this routine, we provide the model, the DEC loss, the optimizer,\nand all the default arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = partial(\n    DECLoss,\n    reg_weight=1e-2,\n)\n\nbaseline = ClassificationSingle(\n    model=model,\n    num_classes=dm.num_classes,\n    in_channels=dm.num_channels,\n    loss=loss,\n    optimization_procedure=optim_lenet,\n    **vars(args),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Gathering Everything and Training the Model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = cli_main(baseline, dm, root, net_name, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Testing the Model\nNow that the model is trained, let's test it on MNIST.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport torchvision.transforms.functional as F\n\nimport numpy as np\n\n\ndef imshow(img) -> None:\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\ndef rotated_mnist(angle: int) -> None:\n    \"\"\"Rotate MNIST images and show images and confidence.\n\n    Args:\n        angle: Rotation angle in degrees.\n    \"\"\"\n    rotated_images = F.rotate(images, angle)\n    # print rotated images\n    imshow(torchvision.utils.make_grid(rotated_images[:4, ...]))\n    print(\"Ground truth: \", \" \".join(f\"{labels[j]}\" for j in range(4)))\n\n    evidence = baseline(rotated_images)\n    alpha = torch.relu(evidence) + 1\n    strength = torch.sum(alpha, dim=1, keepdim=True)\n    probs = alpha / strength\n    entropy = -1 * torch.sum(probs * torch.log(probs), dim=1, keepdim=True)\n    for j in range(4):\n        predicted = torch.argmax(probs[j, :])\n        print(\n            f\"Predicted digits for the image {j}: {predicted} with strength \"\n            f\"{strength[j,0]:.3} and entropy {entropy[j,0]:.3}.\"\n        )\n\n\ndataiter = iter(dm.val_dataloader())\nimages, labels = next(dataiter)\n\nwith torch.no_grad():\n    baseline.eval()\n    rotated_mnist(0)\n    rotated_mnist(45)\n    rotated_mnist(90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n- **Deep Evidential Classification:** Murat Sensoy, Lance Kaplan, & Melih Kandemir (2018). Evidential Deep Learning to Quantify Classification Uncertainty [NeurIPS 2018](https://arxiv.org/pdf/1806.01768)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}