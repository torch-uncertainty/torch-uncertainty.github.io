{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Train a Bayesian Neural Network in 20 seconds\n\nIn this tutorial, we will train a variational inference Bayesian Neural Network (viBNN) LeNet classifier on the MNIST dataset.\n\n## Foreword on Bayesian Neural Networks\n\nBayesian Neural Networks (BNNs) are a class of neural networks that estimate the uncertainty on their predictions via uncertainty\non their weights. This is achieved by considering the weights of the neural network as random variables, and by learning their\nposterior distribution. This is in contrast to standard neural networks, which only learn a single set of weights (this can be\nseen as Dirac distributions on the weights).\n\nFor more information on Bayesian Neural Networks, we refer to the following resources:\n\n- Weight Uncertainty in Neural Networks [ICML2015](https://arxiv.org/pdf/1505.05424.pdf)\n- Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users [IEEE Computational Intelligence Magazine](https://arxiv.org/pdf/2007.06823.pdf)\n\n## Training a Bayesian LeNet using TorchUncertainty models and Lightning\n\nIn this first part, we train a Bayesian LeNet, based on the model and routines already implemented in TU.\n\n### 1. Loading the utilities\n\nTo train a BNN using TorchUncertainty, we have to load the following modules:\n\n- our TUTrainer\n- the model: bayesian_lenet, which lies in the torch_uncertainty.model.classification.lenet module\n- the classification training routine from torch_uncertainty.routines module\n- the Bayesian objective: the ELBOLoss, which lies in the torch_uncertainty.losses file\n- the datamodule that handles dataloaders: MNISTDataModule from torch_uncertainty.datamodules\n\nWe will also need to define an optimizer using torch.optim and Pytorch's\nneural network utils from torch.nn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nfrom torch import nn, optim\n\nfrom torch_uncertainty import TUTrainer\nfrom torch_uncertainty.datamodules import MNISTDataModule\nfrom torch_uncertainty.losses import ELBOLoss\nfrom torch_uncertainty.models.classification.lenet import bayesian_lenet\nfrom torch_uncertainty.routines import ClassificationRoutine\n\n# We also define the main hyperparameters, with just one epoch for the sake of time\nBATCH_SIZE = 512\nMAX_EPOCHS = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Creating the necessary variables\n\nIn the following, we instantiate our trainer, define the root of the datasets and the logs.\nWe also create the datamodule that handles the MNIST dataset, dataloaders and transforms.\nPlease note that the datamodules can also handle OOD detection by setting the `eval_ood`\nparameter to True, as well as distribution shift with `eval_shift`.\nFinally, we create the model using the blueprint from torch_uncertainty.models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = TUTrainer(accelerator=\"gpu\", devices=1, enable_progress_bar=False, max_epochs=MAX_EPOCHS)\n\n# datamodule\nroot = Path(\"data\")\ndatamodule = MNISTDataModule(root=root, batch_size=BATCH_SIZE, eval_ood=False, num_workers=8)\n\n# model\nmodel = bayesian_lenet(datamodule.num_channels, datamodule.num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. The Loss and the Training Routine\n\nThen, we just define the loss to be used during training, which is a bit special and called\nthe evidence lower bound. We use the hyperparameters proposed in the blitz\nlibrary. As we are training a classification model, we use the CrossEntropyLoss\nas the negative log likelihood. We then define the training routine using the classification\ntraining routine from torch_uncertainty.classification. We provide the model, the ELBO\nloss and the optimizer to the routine.\nWe use an Adam optimizer with a learning rate of 0.02.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = ELBOLoss(\n    model=model,\n    inner_loss=nn.CrossEntropyLoss(),\n    kl_weight=1 / 10000,\n    num_samples=3,\n)\n\nroutine = ClassificationRoutine(\n    model=model,\n    num_classes=datamodule.num_classes,\n    loss=loss,\n    optim_recipe=optim.Adam(model.parameters(), lr=2e-2),\n    is_ensemble=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Gathering Everything and Training the Model\n\nNow that we have prepared all of this, we just have to gather everything in\nthe main function and to train the model using our wrapper of Lightning Trainer.\nSpecifically, it needs the routine, that includes the model as well as the\ntraining/eval logic and the datamodule.\nThe dataset will be downloaded automatically in the root/data folder, and the\nlogs will be saved in the root/logs folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=routine, datamodule=datamodule)\ntrainer.test(model=routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Testing the Model\n\nNow that the model is trained, let's test it on MNIST.\nPlease note that we apply a reshape to the logits to determine the dimension corresponding to the ensemble\nand to the batch. As for TorchUncertainty 0.5.0, the ensemble dimension is merged with the batch dimension\nin this order (num_estimator x batch, classes).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nfrom einops import rearrange\n\n\ndef imshow(img) -> None:\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\nimages, labels = next(iter(datamodule.val_dataloader()))\n\n# print images\nimshow(torchvision.utils.make_grid(images[:4, ...]))\nprint(\"Ground truth: \", \" \".join(f\"{labels[j]}\" for j in range(4)))\n\n# Put the model in eval mode to use several samples\nmodel = routine.eval()\nlogits = routine(images[:4, ...])\nprint(\"Output logit shape (Num predictions x Batch) Classes: \", logits.shape)\nlogits = rearrange(logits, \"(m b) c -> b m c\", b=4)  # batch_size, num_estimators, num_classes\n\n# We apply the softmax on the classes then average over the estimators\nprobs = torch.nn.functional.softmax(logits, dim=-1)\navg_probs = probs.mean(dim=1)\nvar_probs = probs.std(dim=1)\n\npredicted = torch.argmax(avg_probs, -1)\n\nprint(\"Predicted digits: \", \" \".join(f\"{predicted[j]}\" for j in range(4)))\nprint(\n    \"Std. dev. of the scores over the posterior samples\",\n    \" \".join(f\"{var_probs[j][predicted[j]]:.3f}\" for j in range(4)),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we show the variance of the top prediction. This is a non-standard but intuitive way to show the diversity of the predictions\nof the ensemble. Ideally, the variance should be high when the prediction is incorrect.\n\n## References\n\n- **LeNet & MNIST:** LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. [Proceedings of the IEEE](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf).\n- **Bayesian Neural Networks:** Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). Weight Uncertainty in Neural Networks. [ICML 2015](https://arxiv.org/pdf/1505.05424.pdf).\n- **The Adam optimizer:** Kingma, D. P., & Ba, J. (2014). \"Adam: A method for stochastic optimization.\" [ICLR 2015](https://arxiv.org/pdf/1412.6980.pdf).\n- **The Blitz** [library](https://github.com/piEsposito/blitz-bayesian-deep-learning) (for the hyperparameters).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}