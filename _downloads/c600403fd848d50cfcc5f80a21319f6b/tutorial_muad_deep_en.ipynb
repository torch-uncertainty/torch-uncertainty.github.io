{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep ensembles Segmentation Tutorial using Muad Dataset\n\nThis tutorial demonstrates how to train a segmentation model on the MUAD dataset using TorchUncertainty.\nMUAD is a synthetic dataset designed for evaluating autonomous driving under diverse uncertainties.\nIt includes **10,413 images** across training, validation, and test sets, featuring adverse weather,\nlighting conditions, and out-of-distribution (OOD) objects. The dataset supports tasks like semantic segmentation,\ndepth estimation, and object detection.\n\nFor details and access, visit the [MUAD Website](https://muad-dataset.github.io/).\n\n## 1. Loading the utilities\n\nFirst, we load the following utilities from TorchUncertainty:\n\n- the TUTrainer which mostly handles the link with the hardware (accelerators, precision, etc)\n- the segmentation training & evaluation routine from torch_uncertainty.routines\n- the datamodule handling dataloaders: MUADDataModule from torch_uncertainty.datamodules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\nimport torchvision.transforms.v2.functional as F\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import tv_tensors\nfrom torchvision.transforms import v2\nfrom torchvision.utils import draw_segmentation_masks\n\nfrom torch_uncertainty import TUTrainer\nfrom torch_uncertainty.datamodules.segmentation import MUADDataModule\nfrom torch_uncertainty.models import deep_ensembles\nfrom torch_uncertainty.models.segmentation.unet import small_unet\nfrom torch_uncertainty.routines import SegmentationRoutine\nfrom torch_uncertainty.transforms import RepeatTarget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initializing the DataModule\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "muad_mean = MUADDataModule.mean\nmuad_std = MUADDataModule.std\n\ntrain_transform = v2.Compose(\n    [\n        v2.Resize(size=(256, 512), antialias=True),\n        v2.RandomHorizontalFlip(),\n        v2.ToDtype(\n            dtype={\n                tv_tensors.Image: torch.float32,\n                tv_tensors.Mask: torch.int64,\n                \"others\": None,\n            },\n            scale=True,\n        ),\n        v2.Normalize(mean=muad_mean, std=muad_std),\n    ]\n)\n\ntest_transform = v2.Compose(\n    [\n        v2.Resize(size=(256, 512), antialias=True),\n        v2.ToDtype(\n            dtype={\n                tv_tensors.Image: torch.float32,\n                tv_tensors.Mask: torch.int64,\n                \"others\": None,\n            },\n            scale=True,\n        ),\n        v2.Normalize(mean=muad_mean, std=muad_std),\n    ]\n)\n\n# datamodule providing the dataloaders to the trainer\ndatamodule = MUADDataModule(\n    root=\"./data\",\n    batch_size=10,\n    version=\"small\",\n    train_transform=train_transform,\n    test_transform=test_transform,\n    num_workers=4,\n)\ndatamodule.prepare_data()\ndatamodule.setup(\"fit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a validation input sample (and RGB image)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\nimg, tgt = datamodule.train[0]\nt_muad_mean = torch.tensor(muad_mean, device=img.device)\nt_muad_std = torch.tensor(muad_std, device=img.device)\nimg = img * t_muad_std[:, None, None] + t_muad_mean[:, None, None]\nimg = F.to_dtype(img, torch.uint8, scale=True)\nimg_pil = F.to_pil_image(img)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(img_pil)\nplt.axis(\"off\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the same image above but segmented.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tmp_tgt = tgt.masked_fill(tgt == 255, 21)\ntgt_masks = tmp_tgt == torch.arange(22, device=tgt.device)[:, None, None]\nimg_segmented = draw_segmentation_masks(\n    img, tgt_masks, alpha=1, colors=datamodule.train.color_palette\n)\nimg_pil = F.to_pil_image(img_segmented)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(img_pil)\nplt.axis(\"off\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Instantiating the Model\nWe create the model easily using the blueprint from torch_uncertainty.models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = small_unet(\n    in_channels=datamodule.num_channels,\n    num_classes=datamodule.num_classes,\n    bilinear=True,\n)\n\nensemble = deep_ensembles(\n    model,\n    num_estimators=4,\n    task=\"segmentation\",\n    reset_model_parameters=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute class weights to mitigate class inbalance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def enet_weighing(dataloader, num_classes, c=1.02):\n    \"\"\"Computes class weights as described in the ENet paper.\n\n        w_class = 1 / (ln(c + p_class)),\n\n    where c is usually 1.02 and p_class is the propensity score of that\n    class:\n\n        propensity_score = freq_class / total_pixels.\n\n    References:\n        https://arxiv.org/abs/1606.02147\n\n    Args:\n        dataloader (``data.Dataloader``): A data loader to iterate over the\n            dataset.\n        num_classes (``int``): The number of classes.\n        c (``int``, optional): AN additional hyper-parameter which restricts\n            the interval of values for the weights. Default: 1.02.\n        ignore_indexes (``list``, optional): A list of indexes to ignore\n            when computing the weights. Default to `None`.\n\n    \"\"\"\n    class_count = 0\n    total = 0\n    for _, label in dataloader:\n        label = label.cpu()\n        # Flatten label\n        flat_label = label.flatten()\n        flat_label = flat_label[flat_label != 255]\n        flat_label = flat_label[flat_label < num_classes]\n\n        # Sum up the number of pixels of each class and the total pixel\n        # counts for each label\n        class_count += torch.bincount(flat_label, minlength=num_classes)\n        total += flat_label.size(0)\n\n    # Compute propensity score and then the weights for each class\n    propensity_score = class_count / total\n\n    return 1 / (torch.log(c + propensity_score))\n\n\nclass_weights = enet_weighing(datamodule.val_dataloader(), datamodule.num_classes)\nprint(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the training parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 2e-4\nLR_DECAY_EPOCHS = 20\nLR_DECAY = 0.1\nNB_EPOCHS = 1\nNUM_ESTIMATORS = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Loss, the Routine, and the Trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We build the optimizer\noptimizer = optim.Adam(\n    ensemble.parameters(), lr=LEARNING_RATE * NUM_ESTIMATORS, weight_decay=WEIGHT_DECAY\n)\n\n# Learning rate decay scheduler\nlr_updater = lr_scheduler.StepLR(optimizer, LR_DECAY_EPOCHS, LR_DECAY)\n\nens_routine = SegmentationRoutine(\n    model=ensemble,\n    num_classes=datamodule.num_classes,\n    loss=torch.nn.CrossEntropyLoss(weight=class_weights),\n    format_batch_fn=RepeatTarget(NUM_ESTIMATORS),  # Repeat the target 4 times for the ensemble\n    optim_recipe={\"optimizer\": optimizer, \"lr_scheduler\": lr_updater},\n)\n\ntrainer = TUTrainer(\n    accelerator=\"gpu\",\n    devices=1,\n    max_epochs=NB_EPOCHS,\n    enable_progress_bar=False,\n    precision=\"16-mixed\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=ens_routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Testing the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = trainer.test(datamodule=datamodule, ckpt_path=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Uncertainty evaluations with MCP\nHere we will just use as confidence score the Maximum class probability (MCP)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img, target = datamodule.test[0]\n\nbatch_img = img.unsqueeze(0)\nbatch_target = target.unsqueeze(0)\nensemble.eval()\nwith torch.no_grad():\n    # Forward propagation\n    outputs = ensemble(batch_img)\n    outputs_proba = outputs.softmax(dim=1)\n    # average the outputs over the estimators\n    outputs_proba = outputs_proba.mean(dim=0)\n    # remove the batch dimension\n    outputs_proba = outputs_proba.squeeze(0)\n    confidence, pred = outputs_proba.max(0)\n\n# Undo normalization on the image and convert to uint8.\nmean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\nstd = torch.tensor([0.229, 0.224, 0.225], device=img.device)\nimg = img * std[:, None, None] + mean[:, None, None]\nimg = F.to_dtype(img, torch.uint8, scale=True)\n\ntmp_target = target.masked_fill(target == 255, 21)\ntarget_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\nimg_segmented = draw_segmentation_masks(\n    img, target_masks, alpha=1, colors=datamodule.test.color_palette\n)\n\npred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n\npred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=datamodule.test.color_palette)\n\n\nif confidence.ndim == 2:\n    confidence = confidence.unsqueeze(0)\n\nimg = F.to_pil_image(F.resize(img, 1024))\nimg_segmented = F.to_pil_image(F.resize(img_segmented, 1024))\npred_img = F.to_pil_image(F.resize(pred_img, 1024))\nconfidence_img = F.to_pil_image(F.resize(confidence, 1024))\n\n\nfig, axs = plt.subplots(1, 4, figsize=(25, 7))\nimages = [img, img_segmented, pred_img, confidence_img]\n\nfor ax, im in zip(axs, images, strict=False):\n    ax.imshow(im)\n    ax.axis(\"off\")\n\nplt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01, wspace=0.05)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}