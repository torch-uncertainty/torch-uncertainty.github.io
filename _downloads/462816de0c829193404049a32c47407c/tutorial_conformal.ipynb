{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Conformal Prediction on CIFAR-10 with TorchUncertainty\n\nWe evaluate the model's performance both before and after applying different conformal predictors (THR, APS, RAPS), and visualize how conformal prediction estimates the prediction sets.\n\nWe use the pretrained ResNet models we provide on Hugging Face.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom huggingface_hub import hf_hub_download\n\nfrom torch_uncertainty import TUTrainer\nfrom torch_uncertainty.datamodules import CIFAR10DataModule\nfrom torch_uncertainty.models.classification.resnet import resnet\nfrom torch_uncertainty.post_processing import ConformalClsAPS, ConformalClsRAPS, ConformalClsTHR\nfrom torch_uncertainty.routines import ClassificationRoutine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load pretrained model from Hugging Face repository\n\nWe use a ResNet18 model trained on CIFAR-10, provided by the TorchUncertainty team\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ckpt_path = hf_hub_download(repo_id=\"torch-uncertainty/resnet18_c10\", filename=\"resnet18_c10.ckpt\")\nmodel = resnet(in_channels=3, num_classes=10, arch=18, conv_bias=False, style=\"cifar\")\nckpt = torch.load(ckpt_path, weights_only=True)\nmodel.load_state_dict(ckpt)\nmodel = model.cuda().eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load CIFAR-10 Dataset & Define Dataloaders\n\nWe set eval_ood to True to evaluate the performance of Conformal scores for detecting out-of-distribution\nsamples. In this case, since we use a model trained on the full training set, we use the test set to as calibration\nset for the Conformal methods and for its evaluation. This is not a proper way to evaluate the coverage.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n\ndatamodule = CIFAR10DataModule(\n    root=\"./data\",\n    batch_size=BATCH_SIZE,\n    num_workers=8,\n    eval_ood=True,\n    postprocess_set=\"test\",\n)\ndatamodule.prepare_data()\ndatamodule.setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define the Lightning Trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = TUTrainer(accelerator=\"gpu\", devices=1, max_epochs=5, enable_progress_bar=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Function to Visualize the Prediction Sets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def visualize_prediction_sets(inputs, labels, confidence_scores, classes, num_examples=5) -> None:\n    _, axs = plt.subplots(2, num_examples, figsize=(15, 5))\n    for i in range(num_examples):\n        ax = axs[0, i]\n        img = np.clip(\n            inputs[i].permute(1, 2, 0).cpu().numpy() * datamodule.std + datamodule.mean, 0, 1\n        )\n        ax.imshow(img)\n        ax.set_title(f\"True: {classes[labels[i]]}\")\n        ax.axis(\"off\")\n        ax = axs[1, i]\n        for j in range(len(classes)):\n            ax.barh(classes[j], confidence_scores[i, j], color=\"blue\")\n        ax.set_xlim(0, 1)\n        ax.set_xlabel(\"Confidence Score\")\n    plt.tight_layout()\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Estimate Prediction Sets with ConformalClsTHR\n\nUsing alpha=0.01, we aim for a 1% error rate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"[Phase 2]: ConformalClsTHR calibration\")\nconformal_model = ConformalClsTHR(alpha=0.01, device=\"cuda\")\n\nroutine_thr = ClassificationRoutine(\n    num_classes=10,\n    model=model,\n    loss=None,  # No loss needed for evaluation\n    eval_ood=True,\n    post_processing=conformal_model,\n    ood_criterion=\"post_processing\",\n)\nperf_thr = trainer.test(routine_thr, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization of ConformalClsTHR prediction sets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs, labels = next(iter(datamodule.test_dataloader()[0]))\n\nconformal_model.cuda()\nconfidence_scores = conformal_model.conformal(inputs.cuda())\n\nclasses = datamodule.test.classes\n\nvisualize_prediction_sets(inputs, labels, confidence_scores[:5].cpu(), classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Estimate Prediction Sets with ConformalClsAPS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"[Phase 3]: ConformalClsAPS calibration\")\nconformal_model = ConformalClsAPS(alpha=0.01, device=\"cuda\", enable_ts=False)\n\nroutine_aps = ClassificationRoutine(\n    num_classes=10,\n    model=model,\n    loss=None,  # No loss needed for evaluation\n    eval_ood=True,\n    post_processing=conformal_model,\n    ood_criterion=\"post_processing\",\n)\nperf_aps = trainer.test(routine_aps, datamodule=datamodule)\nconformal_model.cuda()\nconfidence_scores = conformal_model.conformal(inputs.cuda())\nvisualize_prediction_sets(inputs, labels, confidence_scores[:5].cpu(), classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Estimate Prediction Sets with ConformalClsRAPS\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"[Phase 4]: ConformalClsRAPS calibration\")\nconformal_model = ConformalClsRAPS(\n    alpha=0.01, regularization_rank=3, penalty=0.002, model=model, device=\"cuda\", enable_ts=False\n)\n\nroutine_raps = ClassificationRoutine(\n    num_classes=10,\n    model=model,\n    loss=None,  # No loss needed for evaluation\n    eval_ood=True,\n    post_processing=conformal_model,\n    ood_criterion=\"post_processing\",\n)\nperf_raps = trainer.test(routine_raps, datamodule=datamodule)\nconformal_model.cuda()\nconfidence_scores = conformal_model.conformal(inputs.cuda())\nvisualize_prediction_sets(inputs, labels, confidence_scores[:5].cpu(), classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary\n\nIn this tutorial, we explored how to apply conformal prediction to a pretrained ResNet on CIFAR-10.\nWe evaluated three methods: Thresholding (THR), Adaptive Prediction Sets (APS), and Regularized APS (RAPS).\nFor each, we calibrated on a validation set, evaluated OOD performance, and visualized prediction sets.\nYou can explore further by adjusting `alpha`, changing the model, or testing on other datasets.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}