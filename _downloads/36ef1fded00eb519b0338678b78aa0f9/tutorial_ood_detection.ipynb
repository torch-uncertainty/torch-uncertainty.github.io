{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Out-of-distribution detection with TorchUncertainty\n\nThis tutorial demonstrates how to perform OOD detection using\nTorchUncertainty's ClassificationRoutine with a ResNet18 model trained on CIFAR-10,\nevaluating its performance with SVHN as the OOD dataset.\n\nWe will:\n\n- Set up the CIFAR-10 datamodule.\n- Initialize and shortly train a ResNet18 model using the ClassificationRoutine.\n- Evaluate the model's performance on both in-distribution and out-of-distribution data.\n- Analyze uncertainty metrics for OOD detection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup\n\nFirst, we need to import the necessary libraries and set up our environment.\nThis includes importing PyTorch, TorchUncertainty components, and TorchUncertainty's Trainer (built on top of Lightning's),\nas well as two criteria for OOD detection, the maximum softmax probability [1] and the Max Logit [2].\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n\nfrom torch_uncertainty import TUTrainer\nfrom torch_uncertainty.datamodules import CIFAR10DataModule\nfrom torch_uncertainty.models.classification.resnet import resnet\nfrom torch_uncertainty.ood_criteria import MaxLogitCriterion, MaxSoftmaxCriterion\nfrom torch_uncertainty.routines.classification import ClassificationRoutine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataModule Setup\n\nTorchUncertainty provides convenient DataModules for standard datasets like CIFAR-10.\nDataModules handle data loading, preprocessing, and batching, simplifying the data pipeline. Each datamodule\nalso include the corresponding out-of-distribution and distribution shift datasets, which are then used by the routine.\nFor CIFAR-10, the corresponding OOD-detection dataset is SVHN as used in the community.\nTo enable OOD evaluation, activate the `eval_ood` flag as done below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datamodule = CIFAR10DataModule(root=\"./data\", batch_size=512, num_workers=8, eval_ood=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization\n\nWe use the ResNet18 architecture, a widely adopted convolutional neural network known for its deep residual learning capabilities.\nThe model is initialized with 10 output classes corresponding to the CIFAR-10 dataset categories. When training on CIFAR, do not forget to\nset the style of the resnet to CIFAR, otherwise it will lose more information in the first convolution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the ResNet18 model\nmodel = resnet(arch=18, in_channels=3, num_classes=10, style=\"cifar\", conv_bias=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Classification Routine\n\nThe `ClassificationRoutine` is one of the most crucial building blocks in TorchUncertainty.\nIt streamlines the training and evaluation processes.\nIt integrates the model, loss function, and optimizer into a cohesive routine compatible with PyTorch Lightning's Trainer.\nThis abstraction simplifies the implementation of standard training loops and evaluation protocols.\nTo come back to what matters in this tutorial, the routine also handles OOD detection. To enable it,\njust activate the `eval_ood` flag. Note that you can also evaluate the distribution-shift performance\nof the model at the same time by also setting `eval_shift` to True.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Initialize the ClassificationRoutine, you could replace MaxSoftmaxCriterion by \"msp\"\nroutine = ClassificationRoutine(\n    model=model,\n    num_classes=10,\n    loss=criterion,\n    optim_recipe=optimizer,\n    eval_ood=True,\n    ood_criterion=MaxSoftmaxCriterion,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Training of the Model\n\nWith the routine defined, we can now set up the Trainer and commence training.\nThe Trainer handles the training loop, including epoch management, logging, and checkpointing.\nWe specify the maximum number of epochs, the precision and the device to be used. To reduce the tutorial building time,\nwe will train for a single epoch and load a model from [TorchUncertainty's HuggingFace](https://huggingface.co/torch-uncertainty).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialize the TUTrainer\ntrainer = TUTrainer(max_epochs=1, precision=\"16-mixed\", accelerator=\"cuda\", devices=1)\n\n# Train the model for 1 epoch using the CIFAR-10 DataModule\ntrainer.fit(routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model from HuggingFace\n\nWe simply download a ResNet-18 trained on CIFAR-10 from [TorchUncertainty's HuggingFace](https://huggingface.co/torch-uncertainty) and load it with\nthe `load_from_checkpoint` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom huggingface_hub import hf_hub_download\n\npath = hf_hub_download(\n    repo_id=\"torch-uncertainty/resnet18_c10\",\n    filename=\"resnet18_c10.ckpt\",\n)\nstate_dict = torch.load(path, map_location=\"cpu\", weights_only=True)\nroutine.model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating on In-Distribution and Out-of-distribution Data\n\nNow that the model is trained, we can evaluate its performance on the original in-distribution test set,\nas well as the OOD set. Typing the next line will automatically compute the in-distribution and OOD detection metrics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the CIFAR-10 (IID) and SVHN (OOD) test sets\nresults = trainer.test(routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Changing the OOD Criterion\n\nThe previous metrics for Out-of-distribution detection have been computed using the maximum softmax probability score [1],\nwhich corresponds to the likelihood of the prediction. We could use other scores such as the maximum logit [2]. To do this,\njust change the routine's `ood_criterion` and perform a second test.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "routine.ood_criterion = MaxLogitCriterion()\n\nresults = trainer.test(routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that you could create your own class if you want to implement a custom OOD detection score. When changing the\nOut-of-distribution criterion, all the In-distribution metrics remain the same. The only values that change\nare those of the regrouped in the OOD Detection category. Here we see that the AUPR, AUROC and FPR95 are worse using the maximum\nlogit score compared to the maximum softmax probability but it could depend on the model you are using.\n\n## References\n\n[1] Hendrycks, D., & Gimpel, K. (2016). A baseline for detecting misclassified and out-of-distribution examples in neural networks. In ICLR 2017.\n[2] Hendrycks, D., Basart, S., Mazeika, M., Zou, A., Kwon, J., Mostajabi, M., ... & Song, D. (2019). Scaling out-of-distribution detection for real-world settings. In ICML 2022.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}