{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deep Probabilistic Regression\n\nThis tutorial aims to provide an overview of some utilities in TorchUncertainty for probabilistic regression.\nContrary to pointwise prediction, probabilistic regression consists - in TorchUncertainty's context - in predicting\nthe parameters of a predefined distribution that fit best some training dataset. The distribution's formulation\nis fixed but the parameters are different for all data points, we say that the distribution is heteroscedastic.\n\n## Building a MLP for Probabilistic Regression using TorchUncertainty Distribution Layers\n\nIn this section we cover the building of a very simple MLP outputting Normal distribution parameters,\nthe mean and the standard deviation. These values will depend on the data point given as input.\n\n### 1. Loading the Utilities\n\nFirst, we disable some logging and warnings to keep the output clean.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport warnings\n\nimport torch\nfrom torch import nn\n\nlogging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\nwarnings.filterwarnings(\"ignore\")\n\n# Here are the trainer and dataloader main hyperparameters\nMAX_EPOCHS = 10\nBATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Building the NormalMLP Model\n\nTo create a NormalMLP model estimating a Normal distribution, we use the NormalLinear layer.\nThis layer is a wrapper around the nn.Linear layer, which outputs the location and scale of a Normal distribution in a dictionnary.\nAs you will see in the following, any other distribution layer from TU can be used in the same way. Check out the regression tutorial\nto learn how to create a NormalMLP more easily using the blueprints from torch_uncertainty.models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.layers.distributions import NormalLinear\n\n\nclass NormalMLP(nn.Module):\n    def __init__(self, in_features: int, out_features: int) -> None:\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, 50)\n        self.fc2 = NormalLinear(\n            base_layer=nn.Linear,\n            event_dim=out_features,\n            in_features=50,\n        )\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Setting up the Data\n\nWe use the UCI Kin8nm dataset, which is a regression dataset with 8 features and 8192 samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.datamodules import UCIRegressionDataModule\n\n# datamodule\ndatamodule = UCIRegressionDataModule(\n    root=\"data\", batch_size=BATCH_SIZE, dataset_name=\"kin8nm\", num_workers=4\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Setting up the Model and Trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty import TUTrainer\n\ntrainer = TUTrainer(\n    accelerator=\"gpu\",\n    devices=1,\n    max_epochs=MAX_EPOCHS,\n    enable_progress_bar=False,\n)\n\nmodel = NormalMLP(in_features=8, out_features=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. The Loss, the Optimizer and the Training Routine\n\nWe use the DistributionNLLLoss to compute the negative log-likelihood of the Normal distribution.\nNote that this loss can be used with any Distribution from torch.distributions.\nFor the optimizer, we use the Adam optimizer with a learning rate of 5e-2.\nFinally, we create a RegressionRoutine to train the model.\nWe indicate that the output dimension is 1 and the distribution family is \"normal\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.losses import DistributionNLLLoss\nfrom torch_uncertainty.routines import RegressionRoutine\n\nloss = DistributionNLLLoss()\n\nroutine = RegressionRoutine(\n    output_dim=1,\n    model=model,\n    loss=loss,\n    optim_recipe=torch.optim.Adam(model.parameters(), lr=5e-2),\n    dist_family=\"normal\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Training and Testing the Model\n\nThanks to the RegressionRoutine, we get the values from 4 metrics, the mean absolute error,\nthe mean squared error, its square root (RMSE) and the negative-log-likelihood (NLL). For all these metrics,\nlower is better.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=routine, datamodule=datamodule)\nresults = trainer.test(model=routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Benchmarking Different Distributions\n\nOur NormalMLP model assumes a Normal distribution as the output. However, we could be interested in comparing the performance of different distributions.\nTorchUncertainty provides a simple way to do this using the get_dist_linear_layer() function.\nLet us rewrite the NormalMLP model to use it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.layers.distributions import get_dist_linear_layer\n\n\nclass DistMLP(nn.Module):\n    def __init__(self, in_features: int, out_features: int, dist_family: str) -> None:\n        super().__init__()\n        self.fc1 = nn.Linear(in_features, 50)\n        dist_layer = get_dist_linear_layer(dist_family)\n        self.fc2 = dist_layer(\n            base_layer=nn.Linear,\n            event_dim=out_features,\n            in_features=50,\n        )\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now train the model with different distributions.\nLet us train the model with a Laplace, Student's t, and Cauchy distribution.\nNote that we use the mode as the point-wise estimate of the distribution as the mean\nis not defined for the Cauchy distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for dist_family in [\"laplace\", \"student\", \"cauchy\"]:\n    print(\"#\" * 38)\n    print(f\">>> Training with {dist_family.capitalize()} distribution\")\n    print(\"#\" * 38)\n    trainer = TUTrainer(\n        accelerator=\"gpu\",\n        devices=1,\n        max_epochs=MAX_EPOCHS,\n        enable_model_summary=False,\n        enable_progress_bar=False,\n    )\n    model = DistMLP(in_features=8, out_features=1, dist_family=dist_family)\n    routine = RegressionRoutine(\n        output_dim=1,\n        model=model,\n        loss=loss,\n        optim_recipe=torch.optim.Adam(model.parameters(), lr=5e-2),\n        dist_family=dist_family,\n        dist_estimate=\"mode\",\n    )\n    trainer.fit(model=routine, datamodule=datamodule)\n    trainer.test(model=routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Negative Log-Likelihood (NLL) is a good score to encompass the correctness of the predicted\ndistributions, evaluating both the correctness of the mode (the point prediction) and of the predicted uncertainty\naround the mode (\"represented\" by the variance). Although there is a lot of variability, in this case, it seems that\nthe Normal distribution often performs better.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}