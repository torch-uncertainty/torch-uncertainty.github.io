{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Improved Ensemble parameter-efficiency with Packed-Ensembles\n\n*This tutorial is adapted from a notebook part of a lecture given at the* |conference|_ *by Sebastian Starke, Peter Steinbach, Gianni Franchi, and Olivier Laurent.*\n\n\n.. |conference| replace:: *Helmholtz AI Conference*\n\nIn this notebook will work on the MNIST dataset that was introduced by Corinna Cortes, Christopher J.C. Burges, and later modified by Yann LeCun in the foundational paper:\n\n- [Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE.](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf).\n\nThe MNIST dataset consists of 70 000 images of handwritten digits from 0 to 9. The images are grayscale and 28x28-pixel sized. The task is to classify the images into their respective digits. The dataset can be automatically downloaded using the `torchvision` library.\n\nIn this notebook, we will train a model and an ensemble on this task and evaluate their performance. The performance will consist in the following metrics:\n\n- Accuracy: the proportion of correctly classified images,\n- Brier score: a measure of the quality of the predicted probabilities,\n- Calibration error: a measure of the calibration of the predicted probabilities,\n- Negative Log-Likelihood: the value of the loss on the test set.\n\nThroughout this notebook, we abstract the training and evaluation process using [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)\nand [TorchUncertainty](https://torch-uncertainty.github.io/).\n\nSimilarly to keras for tensorflow, PyTorch Lightning is a high-level interface for PyTorch that simplifies the training and evaluation process using a Trainer.\nTorchUncertainty is partly built on top of PyTorch Lightning and provides tools to train and evaluate models with uncertainty quantification.\n\nTorchUncertainty includes datamodules that handle the data loading and preprocessing. We don't use them here for tutorial purposes.\n\n## 1. Download, instantiate and visualize the datasets\n\nThe dataset is automatically downloaded using torchvision. We then visualize a few images to see a bit what we are working with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision.transforms as T\n\n# We set the number of epochs to some very low value for the sake of time\nMAX_EPOCHS = 3\n\n# Create the transforms for the images\ntrain_transform = T.Compose(\n    [\n        T.ToTensor(),\n        # We perform random cropping as data augmentation\n        T.RandomCrop(28, padding=4),\n        # As for the MNIST1d dataset, we normalize the data\n        T.Normalize((0.1307,), (0.3081,)),\n    ]\n)\ntest_transform = T.Compose(\n    [\n        T.Grayscale(num_output_channels=1),\n        T.ToTensor(),\n        T.CenterCrop(28),\n        T.Normalize((0.1307,), (0.3081,)),\n    ]\n)\n\n# Download and instantiate the dataset\nfrom torch.utils.data import Subset\nfrom torchvision.datasets import MNIST, FashionMNIST\n\ntrain_data = MNIST(root=\"./data/\", download=True, train=True, transform=train_transform)\ntest_data = MNIST(root=\"./data/\", train=False, transform=test_transform)\n# We only take the first 10k images to have the same number of samples as the test set using torch Subsets\nood_data = Subset(\n    FashionMNIST(root=\"./data/\", download=True, transform=test_transform),\n    indices=range(10000),\n)\n\n# Create the corresponding dataloaders\nfrom torch.utils.data import DataLoader\n\ntrain_dl = DataLoader(train_data, batch_size=512, shuffle=True, num_workers=8)\ntest_dl = DataLoader(test_data, batch_size=2048, shuffle=False, num_workers=4)\nood_dl = DataLoader(ood_data, batch_size=2048, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You could replace all this cell by simply loading the MNIST datamodule from TorchUncertainty.\nNow, let's visualize a few images from the dataset. For this task, we use the viz_data dataset that applies no transformation to the images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Datasets without transformation to visualize the unchanged data\nviz_data = MNIST(root=\"./data/\", train=False)\nood_viz_data = FashionMNIST(root=\"./data/\", download=True)\n\nprint(\"In distribution data:\")\nviz_data[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Out of distribution data:\")\nood_viz_data[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create & train the model\n\nWe will create a simple convolutional neural network (CNN): the LeNet model (also introduced by LeCun).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LeNet(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        num_classes: int,\n    ) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, 6, (5, 5))\n        self.conv2 = nn.Conv2d(6, 16, (5, 5))\n        self.pooling = nn.AdaptiveAvgPool2d((4, 4))\n        self.fc1 = nn.Linear(256, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = torch.flatten(out, 1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        return self.fc3(out)  # No softmax in the model!\n\n\n# Instantiate the model, the images are in grayscale so the number of channels is 1\nmodel = LeNet(in_channels=1, num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now need to define the optimization recipe:\n- the optimizer, here the standard stochastic gradient descent (SGD) with a learning rate of 0.05\n- the scheduler, here cosine annealing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optim_recipe(model, lr_mult: float = 1.0):\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.05 * lr_mult)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n    return {\"optimizer\": optimizer, \"scheduler\": scheduler}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train the model, we use [TorchUncertainty](https://torch-uncertainty.github.io/), a library that we have developed to ease\nthe training and evaluation of models with uncertainty.\n\n**Note:** To train supervised classification models we most often use the cross-entropy loss.\nWith weight-decay, minimizing this loss amounts to finding a Maximum a posteriori (MAP) estimate of the model parameters.\nThis means that the model is trained to predict the most likely class for each input given a diagonal Gaussian prior on the weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty import TUTrainer\nfrom torch_uncertainty.routines import ClassificationRoutine\n\n# Create the trainer that will handle the training\ntrainer = TUTrainer(accelerator=\"gpu\", devices=1, max_epochs=MAX_EPOCHS, enable_progress_bar=False)\n\n# The routine is a wrapper of the model that contains the training logic with the metrics, etc\nroutine = ClassificationRoutine(\n    num_classes=10,\n    model=model,\n    loss=nn.CrossEntropyLoss(),\n    optim_recipe=optim_recipe(model),\n    eval_ood=True,\n)\n\n# In practice, avoid performing the validation on the test set (if you do model selection)\ntrainer.fit(routine, train_dataloaders=train_dl, val_dataloaders=test_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the trained model on the test set - pay attention to the cls/Acc metric\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perf = trainer.test(routine, dataloaders=[test_dl, ood_dl])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This table provides a lot of information:\n\n**OOD Detection: Binary Classification MNIST vs. FashionMNIST**\n\n- AUPR/AUROC/FPR95: Measures the quality of the OOD detection. The higher the better for AUPR and AUROC, the lower the better for FPR95.\n\n**Calibration: Reliability of the Predictions**\n\n- ECE: Expected Calibration Error. The lower the better.\n- aECE: Adaptive Expected Calibration Error. The lower the better. (~More precise version of the ECE)\n\n**Classification Performance**\n\n- Accuracy: The ratio of correctly classified images. The higher the better.\n- Brier: The quality of the predicted probabilities (Mean Squared Error of the predictions vs. ground-truth). The lower the better.\n- Negative Log-Likelihood: The value of the loss on the test set. The lower the better.\n\n**Selective Classification & Grouping Loss**\n- We talk about these points later in the \"To go further\" section.\n\nBy setting `eval_shift` to True, we could also evaluate the performance of the models on MNIST-C, a dataset close to MNIST but with perturbations.\n\n## 3. Training an ensemble of models with TorchUncertainty\n\nYou have two options here, you can either train the ensemble directly if you have enough memory,\notherwise, you can train independent models and do the ensembling during the evaluation (sometimes called inference).\n\nIn this case, we will do it sequentially. In this tutorial, you have the choice between training multiple models,\nwhich will take time if you have no GPU, or downloading the pre-trained models that we have prepared for you.\n\n**Training the ensemble**\n\nTo train the ensemble, you will have to use the \"deep_ensembles\" function from TorchUncertainty, which will\nreplicate and change the initialization of your networks to ensure diversity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.models import deep_ensembles\nfrom torch_uncertainty.transforms import RepeatTarget\n\n# Create the ensemble model\nensemble = deep_ensembles(\n    LeNet(in_channels=1, num_classes=10),\n    num_estimators=2,\n    task=\"classification\",\n    reset_model_parameters=True,\n)\n\ntrainer = TUTrainer(accelerator=\"gpu\", devices=1, max_epochs=MAX_EPOCHS)\nens_routine = ClassificationRoutine(\n    is_ensemble=True,\n    num_classes=10,\n    model=ensemble,\n    loss=nn.CrossEntropyLoss(),  # The loss for the training\n    format_batch_fn=RepeatTarget(2),  # How to handle the targets when comparing the predictions\n    optim_recipe=optim_recipe(\n        ensemble, 2.0\n    ),  # The optimization scheme with the optimizer and the scheduler as a dictionnary\n    eval_ood=True,  # We want to evaluate the OOD-related metrics\n)\ntrainer.fit(ens_routine, train_dataloaders=train_dl, val_dataloaders=test_dl)\nens_perf = trainer.test(ens_routine, dataloaders=[test_dl, ood_dl])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feel free to run the notebook on your machine for a longer duration.\n\nWe need to multiply the learning rate by 2 to account for the fact that we have 2 models\nin the ensemble and that we average the loss over all the predictions.\n\n**Downloading the pre-trained models**\n\nWe have put the pre-trained models on Hugging Face that you can download with the utility function\n\"hf_hub_download\" imported just below. These models are trained for 75 epochs and are therefore not\ncomparable to the all the others trained in this notebook. The pretrained models can be seen\non [HuggingFace](https://huggingface.co/ENSTA-U2IS/tutorial-models) and TorchUncertainty's are [there](https://huggingface.co/torch-uncertainty).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.utils.hub import hf_hub_download\n\nall_models = []\nfor i in range(8):\n    hf_hub_download(\n        repo_id=\"ENSTA-U2IS/tutorial-models\",\n        filename=f\"version_{i}.ckpt\",\n        local_dir=\"./models/\",\n    )\n    model = LeNet(in_channels=1, num_classes=10)\n    state_dict = torch.load(f\"./models/version_{i}.ckpt\", map_location=\"cpu\", weights_only=True)[\n        \"state_dict\"\n    ]\n    state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n    model.load_state_dict(state_dict)\n    all_models.append(model)\n\nfrom torch_uncertainty.models import deep_ensembles\nfrom torch_uncertainty.transforms import RepeatTarget\n\nensemble = deep_ensembles(\n    all_models,\n    num_estimators=None,\n    task=\"classification\",\n    reset_model_parameters=True,\n)\n\nens_routine = ClassificationRoutine(\n    is_ensemble=True,\n    num_classes=10,\n    model=ensemble,\n    loss=nn.CrossEntropyLoss(),  # The loss for the training\n    format_batch_fn=RepeatTarget(8),  # How to handle the targets when comparing the predictions\n    optim_recipe=None,  # No optim recipe as the model is already trained\n    eval_ood=True,  # We want to evaluate the OOD-related metrics\n)\n\ntrainer = TUTrainer(accelerator=\"gpu\", devices=1, max_epochs=MAX_EPOCHS)\n\nens_perf = trainer.test(ens_routine, dataloaders=[test_dl, ood_dl])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. From Deep Ensembles to Packed-Ensembles\n\nIn the paper [Packed-Ensembles for Efficient Uncertainty Quantification](https://arxiv.org/abs/2210.09184)\npublished at the International Conference on Learning Representations (ICLR) in 2023, we introduced a\nmodification of Deep Ensembles to make it more computationally-efficient. The idea is to pack the ensemble\nmembers into a single model, which allows us to train the ensemble in a single forward pass.\nThis modification is particularly useful when the ensemble size is large, as it is often the case in practice.\n\nWe will need to update the model and replace the layers with their Packed equivalents. You can find the\ndocumentation of the Packed-Linear layer using this [link](https://torch-uncertainty.github.io/generated/torch_uncertainty.layers.PackedLinear.html),\nand the Packed-Conv2D, [here](https://torch-uncertainty.github.io/generated/torch_uncertainty.layers.PackedLinear.html).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n\nfrom torch_uncertainty.layers import PackedConv2d, PackedLinear\n\n\nclass PackedLeNet(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        num_classes: int,\n        alpha: int,\n        num_estimators: int,\n    ) -> None:\n        super().__init__()\n        self.num_estimators = num_estimators\n        self.conv1 = PackedConv2d(\n            in_channels,\n            6,\n            (5, 5),\n            alpha=alpha,\n            num_estimators=num_estimators,\n            first=True,\n        )\n        self.conv2 = PackedConv2d(\n            6,\n            16,\n            (5, 5),\n            alpha=alpha,\n            num_estimators=num_estimators,\n        )\n        self.pooling = nn.AdaptiveAvgPool2d((4, 4))\n        self.fc1 = PackedLinear(256, 120, alpha=alpha, num_estimators=num_estimators)\n        self.fc2 = PackedLinear(120, 84, alpha=alpha, num_estimators=num_estimators)\n        self.fc3 = PackedLinear(\n            84,\n            num_classes,\n            alpha=alpha,\n            num_estimators=num_estimators,\n            last=True,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = torch.flatten(out, 1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        return self.fc3(out)  # Again, no softmax in the model\n\n\n# Instantiate the model, the images are in grayscale so the number of channels is 1\npacked_model = PackedLeNet(in_channels=1, num_classes=10, alpha=2, num_estimators=4)\n\n# Create the trainer that will handle the training\ntrainer = TUTrainer(accelerator=\"gpu\", devices=1, max_epochs=MAX_EPOCHS)\n\n# The routine is a wrapper of the model that contains the training logic with the metrics, etc\npacked_routine = ClassificationRoutine(\n    is_ensemble=True,\n    num_classes=10,\n    model=packed_model,\n    loss=nn.CrossEntropyLoss(),\n    format_batch_fn=RepeatTarget(4),\n    optim_recipe=optim_recipe(packed_model, 4.0),\n    eval_ood=True,\n)\n\n# In practice, avoid performing the validation on the test set\ntrainer.fit(packed_routine, train_dataloaders=train_dl, val_dataloaders=test_dl)\n\npacked_perf = trainer.test(packed_routine, dataloaders=[test_dl, ood_dl])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training time should be approximately similar to the one of the single model that you trained before. However, please note that we are working with very small models, hence completely underusing your GPU. As such, the training time is not representative of what you would observe with larger models.\n\nYou can read more on Packed-Ensembles in the [paper](https://arxiv.org/abs/2210.09184) or the [Medium](https://medium.com/@adrien.lafage/make-your-neural-networks-more-reliable-with-packed-ensembles-7ad0b737a873) post.\n\n## To Go Further & More Concepts of Uncertainty in ML\n\n**Question 1:** Have a look at the models in the \"lightning_logs\". If you are on your own machine, try to visualize the learning curves with `tensorboard --logdir lightning_logs`.\n\n**Question 2:** Add a cell below and try to find the errors made by packed-ensembles on the test set. Visualize the errors and their labels and look at the predictions of the different sub-models. Are they similar? Can you think of uncertainty scores that could help you identify these errors?\n\n### Selective Classification\n\nSelective classification or \"prediction with rejection\" is a paradigm in uncertainty-aware machine learning where the model can decide not to make a prediction if the confidence score given by the model is below some pre-computed threshold. This can be useful in real-world applications where the cost of making a wrong prediction is high.\n\nIn constrast to calibration, the values of the confidence scores are not important, only the order of the scores. *Ideally, the best model will order all the correct predictions first, and all the incorrect predictions last.* In this case, there will be a threshold so that all the predictions above the threshold are correct, and all the predictions below the threshold are incorrect.\n\nIn TorchUncertainty, we look at 3 different metrics for selective classification:\n\n- **AURC**: The area under the Risk (% of errors) vs. Coverage (% of classified samples) curve. This curve expresses how the risk of the model evolves as we increase the coverage (the proportion of predictions that are above the selection threshold). This metric will be minimized by a model able to perfectly separate the correct and incorrect predictions.\n\nThe following metrics are computed at a fixed risk and coverage level and that have practical interests. The idea of these metrics is that you can set the selection threshold to achieve a certain level of risk and coverage, as required by the technical constraints of your application:\n\n- **Coverage at 5% Risk**: The proportion of predictions that are above the selection threshold when it is set for the risk to egal 5%. Set the risk threshold to your application constraints. The higher the better.\n- **Risk at 80% Coverage**: The proportion of errors when the coverage is set to 80%. Set the coverage threshold to your application constraints. The lower the better.\n\n### Grouping Loss\n\nThe grouping loss is a measure of uncertainty orthogonal to calibration. Have a look at [this paper](https://arxiv.org/abs/2210.16315) to learn about it. Check out their small library [GLest](https://github.com/aperezlebel/glest). TorchUncertainty includes a wrapper of the library to compute the grouping loss with eval_grouping_loss parameter.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}