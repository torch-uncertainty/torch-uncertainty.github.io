{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Improve Top-label Calibration with Temperature Scaling\n\nIn this tutorial, we use torch-uncertainty to improve the calibration of the top-label predictions\nto improve the reliability of the underlying neural network.\n\nWe also see how to use the datamodules outside any Lightning Trainer.\n\n## 1. Loading the utilities\n\nIn this tutorial, we will need:\n\n- torch to download the pretrained model\n- the Calibration Error metric to compute the ECE and evaluate the top-label calibration\n- the CIFAR-100 datamodule to handle the data\n- the Temperature Scaler to improve the top-label calibration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torchmetrics import CalibrationError\n\nfrom torch_uncertainty.datamodules import CIFAR100DataModule\nfrom torch_uncertainty.post_processing import TemperatureScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Downloading a Pre-trained Model\n\nTo avoid training a model on CIFAR-100 from scratch, we will use here a model from https://github.com/chenyaofo/pytorch-cifar-models (thank you!)\nThis can be done in a one liner:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_resnet20\", pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setting up the Datamodule and Dataloader\n\nTo get the dataloader from the datamodule, just call prepare_data, setup, and \nextract the first element of the test dataloader list. There are more than one \nelement if `:attr:ood_detection` is True.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dm = CIFAR100DataModule(root=\"./data\", ood_detection=False, batch_size=32)\ndm.prepare_data()\ndm.setup(\"test\")\n\n# Get the full test dataloader (unused in this tutorial)\ndataloader = dm.test_dataloader()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Iterate on the Dataloader and compute the ECE\n\nWe first split the original test set into a calibration set and a test set for proper evaluation.\n\nWhen computing the ECE, you need to provide the likelihoods associated with the inputs.\nTo do this, just call PyTorch's softmax.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n\n# Split datasets\ndataset = dm.test\ncal_dataset, test_dataset = random_split(dataset, [1000, len(dataset)-1000])\ncal_dataloader, test_dataloader = DataLoader(cal_dataset, batch_size=32), DataLoader(test_dataset, batch_size=32)\n\n# Initialize the ECE\nece = CalibrationError(task=\"multiclass\", num_classes=100)\n\n# Iterate on the calibration dataloader\nfor sample, target in test_dataloader:\n    logits = model(sample)\n    ece.update(logits, target)\n\n# Compute & print the calibration error\ncal = ece.compute()\n\nprint(f\"ECE before scaling - {cal*100:.3}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fit the Scaler to Improve the Calibration\n\nThe TemperatureScaler has one parameter that can be used to temper the softmax.\nWe minimize the tempered cross-entropy on a calibration set that we define here as\na subset of the test set and containing 1000 data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Fit the scaler on the calibration dataset\nscaler = TemperatureScaler()\nscaler = scaler.fit(model=model, calib_loader=cal_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Iterate again to compute the improved ECE\n\nWe create a wrapper of the original model and the scaler using torch.nn.Sequential.\nThis is possible because the scaler is derived from nn.Module.\n\nNote that you will need to first reset the ECE metric to avoid mixing the scores of the previous and current iterations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create the calibrated model\ncal_model = torch.nn.Sequential(model, scaler)\n\n# Reset the ECE\nece.reset()\n\n# Iterate on the test dataloader\nfor sample, target in test_dataloader:\n    logits = cal_model(sample)\n    ece.update(logits.softmax(-1), target)\n\ncal = ece.compute()\n\nprint(f\"ECE after scaling - {cal*100:.3}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The top-label calibration should be improved.\n\n### Notes\n\nTemperature scaling is very efficient when the calibration set is representative of the test set.\nIn this case, we say that the calibration and test set are drawn from the same distribution.\nHowever, this may not be True in real-world cases where dataset shift could happen.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n\n- **Expected Calibration Error:** Naeini, M. P., Cooper, G. F., & Hauskrecht, M. (2015). Obtaining Well Calibrated Probabilities Using Bayesian Binning. In [AAAI 2015](https://arxiv.org/pdf/1411.0160.pdf)\n- **Temperature Scaling:** Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. In [ICML 2017](https://arxiv.org/pdf/1706.04599.pdf)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}