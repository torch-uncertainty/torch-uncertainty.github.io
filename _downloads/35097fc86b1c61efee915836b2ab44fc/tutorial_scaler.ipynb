{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Improve Top-label Calibration with Temperature Scaling\n\nIn this tutorial, we use *TorchUncertainty* to improve the calibration\nof the top-label predictions and the reliability of the underlying neural network.\n\nThis tutorial provides extensive details on how to use the TemperatureScaler\nclass, however, this is done automatically in the classification routine when setting\nthe `calibration_set` to val or test.\n\nThrough this tutorial, we also see how to use the datamodules outside any Lightning trainers,\nand how to use TorchUncertainty's models.\n\n## 1. Loading the Utilities\n\nIn this tutorial, we will need:\n\n- TorchUncertainty's Calibration Error metric to compute to evaluate the top-label calibration with ECE and plot the reliability diagrams\n- the CIFAR-100 datamodule to handle the data\n- a ResNet 18 as starting model\n- the temperature scaler to improve the top-label calibration\n- a utility function to download HF models easily\n\nIf you use the classification routine, the plots will be automatically available in the tensorboard logs if you use the `log_plots` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.datamodules import CIFAR100DataModule\nfrom torch_uncertainty.metrics import CalibrationError\nfrom torch_uncertainty.models.resnet import resnet\nfrom torch_uncertainty.post_processing import TemperatureScaler\nfrom torch_uncertainty.utils import load_hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading a model from TorchUncertainty's HF\n\nTo avoid training a model on CIFAR-100 from scratch, we load a model from Hugging Face.\nThis can be done in a one liner:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Build the model\nmodel = resnet(in_channels=3, num_classes=100, arch=18, style=\"cifar\", conv_bias=False)\n\n# Download the weights (the config is not used here)\nweights, config = load_hf(\"resnet18_c100\")\n\n# Load the weights in the pre-built model\nmodel.load_state_dict(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setting up the Datamodule and Dataloaders\n\nTo get the dataloader from the datamodule, just call prepare_data, setup, and\nextract the first element of the test dataloader list. There are more than one\nelement if eval_ood is True: the dataloader of in-distribution data and the dataloader\nof out-of-distribution data. Otherwise, it is a list of 1 element.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dm = CIFAR100DataModule(root=\"./data\", eval_ood=False, batch_size=32)\ndm.prepare_data()\ndm.setup(\"test\")\n\n# Get the full test dataloader (unused in this tutorial)\ndataloader = dm.test_dataloader()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Iterating on the Dataloader and Computing the ECE\n\nWe first split the original test set into a calibration set and a test set for proper evaluation.\n\nWhen computing the ECE, you need to provide the likelihoods associated with the inputs.\nTo do this, just call PyTorch's softmax.\n\nTo avoid lengthy computations (without GPU), we restrict the calibration computation to a subset\nof the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n\n# Split datasets\ndataset = dm.test\ncal_dataset, test_dataset, other = random_split(\n    dataset, [1000, 1000, len(dataset) - 2000]\n)\ntest_dataloader = DataLoader(test_dataset, batch_size=32)\n\n# Initialize the ECE\nece = CalibrationError(task=\"multiclass\", num_classes=100)\n\n# Iterate on the calibration dataloader\nfor sample, target in test_dataloader:\n    logits = model(sample)\n    probs = logits.softmax(-1)\n    ece.update(probs, target)\n\n# Compute & print the calibration error\nprint(f\"ECE before scaling - {ece.compute():.3%}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also compute and plot the top-label calibration figure. We see that the\nmodel is not well calibrated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = ece.plot()\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fitting the Scaler to Improve the Calibration\n\nThe TemperatureScaler has one parameter that can be used to temper the softmax.\nWe minimize the tempered cross-entropy on a calibration set that we define here as\na subset of the test set and containing 1000 data. Look at the code run by TemperatureScaler\n`fit` method for more details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Fit the scaler on the calibration dataset\nscaled_model = TemperatureScaler(model=model)\nscaled_model = scaled_model.fit(calibration_set=cal_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Iterating Again to Compute the Improved ECE\n\nWe can directly use the scaler as a calibrated model.\n\nNote that you will need to first reset the ECE metric to avoid mixing the scores of\nthe previous and current iterations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reset the ECE\nece.reset()\n\n# Iterate on the test dataloader\nfor sample, target in test_dataloader:\n    logits = scaled_model(sample)\n    probs = logits.softmax(-1)\n    ece.update(probs, target)\n\nprint(f\"ECE after scaling - {ece.compute():.3%}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We finally compute and plot the scaled top-label calibration figure. We see\nthat the model is now better calibrated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = ece.plot()\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The top-label calibration should be improved.\n\n## Notes\n\nTemperature scaling is very efficient when the calibration set is representative of the test set.\nIn this case, we say that the calibration and test set are drawn from the same distribution.\nHowever, this may not hold true in real-world cases where dataset shift could happen.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n- **Expected Calibration Error:** Naeini, M. P., Cooper, G. F., & Hauskrecht, M. (2015). Obtaining Well Calibrated Probabilities Using Bayesian Binning. In [AAAI 2015](https://arxiv.org/pdf/1411.0160.pdf).\n- **Temperature Scaling:** Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. In [ICML 2017](https://arxiv.org/pdf/1706.04599.pdf).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}