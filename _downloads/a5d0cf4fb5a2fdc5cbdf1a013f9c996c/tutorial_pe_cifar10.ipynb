{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# From a Standard Classifier to a Packed-Ensemble\n\nThis tutorial is heavily inspired by PyTorch's [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#test-the-network-on-the-test-data)\ntutorial.\n\nLet's dive step by step into the process to modify a standard classifier into a\npacked-ensemble classifier.\n\n## Dataset\n\nIn this tutorial we will use the CIFAR10 dataset available in the torchvision\npackage. The CIFAR10 dataset consists of 60000 32x32 colour images in 10\nclasses, with 6000 images per class. There are 50000 training images and 10000\ntest images.\n\nHere is an example of what the data looks like:\n\n.. figure:: /_static/img/cifar10.png\n   :alt: cifar10\n   :figclass: figure-caption\n\n   Sample of the CIFAR-10 dataset\n\n## Training an image Packed-Ensemble classifier\n\nHere is the outline of the process:\n\n1. Load and normalizing the CIFAR10 training and test datasets using\n   ``torchvision``\n2. Define a Packed-Ensemble from a standard classifier\n3. Define a loss function\n4. Train the Packed-Ensemble on the training data\n5. Test the Packed-Ensemble on the test data and evaluate its performance\n   w.r.t. uncertainty quantification and OOD detection\n\n### 1. Load and normalize CIFAR10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\ntorch.set_num_threads(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output of torchvision datasets are PILImage images of range [0, 1].\nWe transform them to Tensors of normalized range [-1, 1].\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If running on Windows and you get a BrokenPipeError, try setting\n    the num_worker of torch.utils.data.DataLoader() to 0.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=transform\n)\ntrainloader = DataLoader(\n    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n)\n\ntestset = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=False, download=True, transform=transform\n)\ntestloader = DataLoader(\n    testset, batch_size=batch_size, shuffle=False, num_workers=2\n)\n\nclasses = (\n    \"plane\",\n    \"car\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us show some of the training images, for fun.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()\n    plt.figure(figsize=(10, 3))\n    plt.axis(\"off\")\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images, pad_value=1))\n# print labels\nprint(\" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Define a Packed-Ensemble from a standard classifier\nFirst we define a standard classifier for CIFAR10 for reference. We will use a\nconvolutional neural network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\nfrom torch import nn\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.flatten(1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's modify the standard classifier into a Packed-Ensemble classifier of\nparameters $M=4,\\ \\alpha=2\\text{ and }\\gamma=1$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n\nfrom torch_uncertainty.layers import PackedConv2d, PackedLinear\n\n\nclass PackedNet(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        M = 4\n        alpha = 2\n        gamma = 1\n        self.conv1 = PackedConv2d(\n            3, 6, 5, alpha=alpha, num_estimators=M, gamma=gamma, first=True\n        )\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = PackedConv2d(6, 16, 5, alpha=alpha, num_estimators=M, gamma=gamma)\n        self.fc1 = PackedLinear(\n            16 * 5 * 5, 120, alpha=alpha, num_estimators=M, gamma=gamma\n        )\n        self.fc2 = PackedLinear(120, 84, alpha=alpha, num_estimators=M, gamma=gamma)\n        self.fc3 = PackedLinear(\n            84, 10 * M, alpha=alpha, num_estimators=M, gamma=gamma, last=True\n        )\n\n        self.num_estimators = M\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = rearrange(x, \"e (m c) h w -> (m e) c h w\", m=self.num_estimators)\n        x = x.flatten(1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\npacked_net = PackedNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Define a Loss function and optimizer\nLet's use a Classification Cross-Entropy loss and SGD with momentum.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(packed_net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Train the Packed-Ensemble on the training data\nLet's train the Packed-Ensemble on the training data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # forward + backward + optimize\n        outputs = packed_net(inputs)\n        loss = criterion(outputs, labels.repeat(packed_net.num_estimators))\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:  # print every 2000 mini-batches\n            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n            running_loss = 0.0\n\nprint(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save our trained model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "PATH = \"./cifar_packed_net.pth\"\ntorch.save(packed_net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Test the Packed-Ensemble on the test data\nLet us display an image from the test set to get familiar.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images, pad_value=1))\nprint(\n    \"GroundTruth: \",\n    \" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let us load back in our saved model (note: saving and re-loading the\nmodel wasn't necessary here, we only did it to illustrate how to do so):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "packed_net = PackedNet()\npacked_net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us see what the Packed-Ensemble thinks these examples above are:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logits = packed_net(images)\nlogits = rearrange(logits, \"(n b) c -> b n c\", n=packed_net.num_estimators)\nprobs_per_est = F.softmax(logits, dim=-1)\noutputs = probs_per_est.mean(dim=1)\n\n_, predicted = torch.max(outputs, 1)\n\nprint(\n    \"Predicted: \",\n    \" \".join(f\"{classes[predicted[j]]:5s}\" for j in range(batch_size)),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results seem pretty good.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}