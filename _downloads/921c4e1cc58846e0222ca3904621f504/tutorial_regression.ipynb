{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training an MLP for Tabular Regression with TorchUncertainty\n\nIn this tutorial, we will train a multi-layer-perceptron on a UCI regression dataset using TorchUncertainty.\nYou will discover two of the core tools from TorchUncertainty, namely\n\n- the routine: a model wrapper, which handles the training and evaluation logics, here for regression\n- the datamodules: python classes, which provide the dataloaders used by the routine\n\nThe regression routine is not as complete as the classification routine, so reach out if you would like the\nteam to implement more features or if you want to contribute!\n\n## 1. Loading the utilities\n\nFirst, we have to load the following utilities from TorchUncertainty:\n\n- the TUTrainer which mostly handles the link with the hardware (accelerators, precision, etc)\n- the regression training & evaluation routine from torch_uncertainty.routines\n- the datamodule handling dataloaders: UCIRegressionDataModule from torch_uncertainty.datamodules\n- the model: mlp from torch_uncertainty.models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport torch\nfrom torch import nn\n\nfrom torch_uncertainty import TUTrainer\nfrom torch_uncertainty.datamodules import UCIRegressionDataModule\nfrom torch_uncertainty.models.mlp import mlp\nfrom torch_uncertainty.routines import RegressionRoutine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Defining the Trainer and the DataModule\n\nIn the following, we first create the trainer and instantiate the datamodule that handles the UCI regression dataset,\ndataloaders and transforms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = TUTrainer(accelerator=\"gpu\", devices=1, max_epochs=20, enable_progress_bar=False)\n\n# datamodule providing the dataloaders to the trainer, specifically we use the yacht dataset\ndatamodule = UCIRegressionDataModule(\n    root=Path(\"data\"), dataset_name=\"yacht\", batch_size=32, num_workers=4\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Instantiating the Pointwise Model\n\nWe create the model easily using the blueprint from torch_uncertainty.models. In this first case, we will just\ntry to predict a pointwise prediction. Later, we will predict a distribution instead to model the uncertainty.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = mlp(in_features=6, num_outputs=1, hidden_dims=[10, 10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The Loss and the Training Routine\n\nThis is a classification problem, and we use CrossEntropyLoss as the (negative-log-)likelihood.\nWe define the training routine using the classification routine from torch_uncertainty.routines.\nWe provide the number of classes, the model, the optimization recipe, the loss, and tell the routine\nthat our model is an ensemble at evaluation time with the `is_ensemble` flag to get the corresponding metrics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "routine = RegressionRoutine(\n    output_dim=1,\n    model=model,\n    loss=nn.MSELoss(),\n    optim_recipe=torch.optim.Adam(model.parameters(), lr=0.01),\n    is_ensemble=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gathering Everything and Training the Model\n\nWe can now train the model using the trainer. We pass the routine and the datamodule\nto the fit and test methods of the trainer. It will automatically evaluate uncertainty\nmetrics that you will find in the table below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.fit(model=routine, datamodule=datamodule)\nresults = trainer.test(model=routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance of the model is much better than random, this means that we have learnt\npatterns from the input data. However, we just provide pointwise estimates and do not model\nthe uncertainty. To predict estimations of uncertainty in regression, we can try to optimize\nthe parameters of some distributions, for instance, we could assume that the samples follow a\nLaplace distribution of unknown parameters.\n\n## 6. Instantiating the Uncertain Model & its Routine\n\nAs before, we create the model easily using the blueprint from torch_uncertainty.models. This time,\nwe state that we want to predict a Laplace distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.losses import DistributionNLLLoss\n\nprob_routine = mlp(in_features=6, num_outputs=1, hidden_dims=[10, 10], dist_family=\"laplace\")\n\nloss = DistributionNLLLoss()\n\n# We create a new routine, this time probabilistic\nprob_routine = RegressionRoutine(\n    output_dim=1,\n    model=prob_routine,\n    loss=loss,\n    optim_recipe=torch.optim.Adam(prob_routine.parameters(), lr=0.01),\n    dist_family=\"laplace\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training & Testing the Probabilistic Model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = TUTrainer(accelerator=\"gpu\", devices=1, max_epochs=20, enable_progress_bar=False)\n\ntrainer.fit(model=prob_routine, datamodule=datamodule)\nresults = trainer.test(model=prob_routine, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, we get another metric, the negative log-likelihood (the lower the better) that\nrepresents how likely it would be to obtain the evaluation set with the parameters of the Laplace\ndistribution predicted by the model.\nYou will get more information on probabilistic regression in the dedicated tutorial.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}