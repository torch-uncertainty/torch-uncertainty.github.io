{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Mixup and MixupMP Training & Ensembles with TorchUncertainty\n\n*This tutorial illustrates how to train models using Mixup and MixupMP*\n*in the same style as the Packed-Ensembles tutorial you provided.*\n\nIn this notebook we will show:\n1. Standard Mixup training\n2. Assembling Mixup-trained models into a Deep Ensemble\n3. MixupMP training\n4. Assembling MixupMP models the same way\n\nThroughout this notebook we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)\nand [TorchUncertainty](https://torch-uncertainty.github.io/) for training.\n\nNote: This script does *not* actually run training \u2014 it shows the configuration and conceptual usage only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Training with Mixup\n\nTo train a Mixup model using TorchUncertainty with Lightning,\nyou need to configure the routine's `mixup_params`:\n\n- `mixtype: \"mixup\"` tells the routine to use standard Mixup augmentation.\n- `mixup_alpha` controls the strength of mixing (Beta(\u03b1,\u03b1)).\n\nMixup internally creates convex combinations of images and labels\nand computes the soft-label cross entropy under the hood.\n\nThis config snippet focuses on *only* the mixup setup.\nRefer to your standard training setup for other keys.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Example Lightning CLI config for Mixup training:\nmixup_config = r\"\"\"\n# lightning.pytorch==2.1.3\nseed_everything: false\neval_after_fit: true\n\ntrainer:\n  accelerator: gpu\n  devices: 1\n  precision: 16-mixed\n  max_epochs: 200\n  logger:\n    class_path: lightning.pytorch.loggers.TensorBoardLogger\n    init_args:\n      save_dir: logs/wideresnet28x10\n      name: mixup\n      default_hp_metric: false\n  callbacks:\n    - class_path: torch_uncertainty.callbacks.TUClsCheckpoint\n    - class_path: lightning.pytorch.callbacks.LearningRateMonitor\n      init_args:\n        logging_interval: step\n    - class_path: lightning.pytorch.callbacks.EarlyStopping\n      init_args:\n        monitor: val/cls/Acc\n        patience: 1000\n        check_finite: true\n\nroutine:\n  model:\n    class_path: torch_uncertainty.models.classification.wideresnet28x10\n    init_args:\n      in_channels: 3\n      num_classes: 10\n      dropout_rate: 0.0\n      style: cifar\n  num_classes: 10\n  loss: CrossEntropyLoss\n\n  # Mixup-specific parameters\n  mixup_params:\n    mixtype: \"mixup\"\n    mixup_alpha: 2\n\ndata:\n  root: ./data\n  batch_size: 128\n  num_workers: 4\n\noptimizer:\n  class_path: torch.optim.SGD\n  init_args:\n    lr: 0.1\n    momentum: 0.9\n    weight_decay: 5e-4\n    nesterov: true\n\nlr_scheduler:\n  class_path: torch.optim.lr_scheduler.MultiStepLR\n  init_args:\n    milestones:\n      - 60\n      - 120\n      - 160\n    gamma: 0.2\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. How to Ensemble Mixup Models\n\nOnce you have trained multiple Mixup models (e.g., using the previous config\nunder different versions / runs), you can assemble them with `deep_ensembles`.\nThis is identical to ensembling vanilla models, except the ckpt paths point\nto your Mixup-trained checkpoints.\n\nTorchUncertainty's `deep_ensembles` helper will load each checkpoint\nand produce an ensemble model that averages predictions across all members.:contentReference[oaicite:0]{index=0}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ensemble_mixup_config = r\"\"\"\n# lightning.pytorch==2.1.3\nseed_everything: false\neval_after_fit: true\n\ntrainer:\n  accelerator: gpu\n  devices: 1\n  precision: 16-mixed\n  max_epochs: 200\n  logger:\n    class_path: lightning.pytorch.loggers.TensorBoardLogger\n    init_args:\n      save_dir: logs/wideresnet28x10\n      name: mixup_ensemble\n      default_hp_metric: false\n  callbacks:\n    - class_path: torch_uncertainty.callbacks.TUClsCheckpoint\n    - class_path: lightning.pytorch.callbacks.LearningRateMonitor\n      init_args:\n        logging_interval: step\n    - class_path: lightning.pytorch.callbacks.EarlyStopping\n      init_args:\n        monitor: val/cls/Acc\n        patience: 1000\n        check_finite: true\n\nroutine:\n  model:\n    class_path: torch_uncertainty.models.deep_ensembles\n    init_args:\n      core_models:\n        class_path: torch_uncertainty.models.classification.wideresnet28x10\n        init_args:\n          in_channels: 3\n          num_classes: 10\n          style: cifar\n          dropout_rate: 0.0\n      num_estimators: 4\n      task: classification\n      # Replace with your trained mixup Checkpoint paths\n      ckpt_paths:\n        - path/to/mixup/version_0.ckpt\n        - path/to/mixup/version_1.ckpt\n        - path/to/mixup/version_2.ckpt\n        - path/to/mixup/version_3.ckpt\n\n  num_classes: 10\n  is_ensemble: true\n  format_batch_fn:\n    class_path: torch_uncertainty.transforms.RepeatTarget\n    init_args:\n      num_repeats: 4\n\ndata:\n  root: ./data\n  batch_size: 128\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training with MixupMP\n\nMixupMP is a variant of mixup that produces posterior samples from a predictive\ndistribution that is more realistic than deep ensembles alone, according to the\n`MixupMP` paper (Martingale posterior based construction).\n\nThe two key differences vs standard Mixup:\n  - You use a specialized loss: `MixupMPLoss`\n  - You set `mixtype: \"mixupmp\"` in mixup_params\n\nThis causes the routine to sample augmented predictive variations as part of\nthe MixupMP methodology for uncertainty.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mixupmp_config = r\"\"\"\n# lightning.pytorch==2.1.3\nseed_everything: false\neval_after_fit: true\n\ntrainer:\n  accelerator: gpu\n  devices: 1\n  precision: 16-mixed\n  max_epochs: 200\n  logger:\n    class_path: lightning.pytorch.loggers.TensorBoardLogger\n    init_args:\n      save_dir: logs/wideresnet28x10\n      name: mixupmp\n      default_hp_metric: false\n  callbacks:\n    - class_path: torch_uncertainty.callbacks.TUClsCheckpoint\n    - class_path: lightning.pytorch.callbacks.LearningRateMonitor\n      init_args:\n        logging_interval: step\n    - class_path: lightning.pytorch.callbacks.EarlyStopping\n      init_args:\n        monitor: val/cls/Acc\n        patience: 1000\n        check_finite: true\n\nroutine:\n  model:\n    class_path: torch_uncertainty.models.classification.wideresnet28x10\n    init_args:\n      in_channels: 3\n      num_classes: 10\n      style: cifar\n      dropout_rate: 0.0\n\n  num_classes: 10\n\n  # Use MixupMP-specific loss with default parameters\n  loss: torch_uncertainty.losses.MixupMPLoss\n\n  mixup_params:\n    mixtype: \"mixupmp\"\n    mixup_alpha: 2\n\ndata:\n  root: ./data\n  batch_size: 128\n  num_workers: 4\n\noptimizer:\n  class_path: torch.optim.SGD\n  init_args:\n    lr: 0.1\n    momentum: 0.9\n    weight_decay: 5e-4\n    nesterov: true\n\nlr_scheduler:\n  class_path: torch.optim.lr_scheduler.MultiStepLR\n  init_args:\n    milestones:\n      - 60\n      - 120\n      - 160\n    gamma: 0.2\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. How to Ensemble MixupMP Models\n\nEnsembling MixupMP models is conceptually the same as ensembling\nany other model: train N different runs (with different seeds / settings),\nthen point their ckpt_paths to the Deep Ensembles config.\n\nThe only difference is that each individual model is a MixupMP-trained\ncheckpoint instead of standard training or mixup training.\n\nUse the exact same deep_ensembles format as shown above (Section 2),\nbut give it the paths to your MixupMP checkpoints.\n\n## 5. References\n\nFor more information on Mixup Ensembles, we refer to the following resources:\n\n- Combining ensembles and data augmentation can harm your calibration\n  [ICLR 2021](https://arxiv.org/pdf/2010.09875)\n  (Yeming Wen, Ghassen Jerfel, Rafael Muller, Michael W. Dusenberry,\n  Jasper Snoek, Balaji Lakshminarayanan, and Dustin Tran)\n\n- Uncertainty quantification and deep ensembles\n  [NeurIPS 2021](https://arxiv.org/pdf/2007.08792)\n  (Rahul Rahaman & Alexandre H. Thiery)\n\nFor more information on MixupMP, we refer to the following resource:\n\n- Posterior Uncertainty Quantification in Neural Networks using Data Augmentation\n  [AISTATS 2024](https://arxiv.org/abs/2403.12729)\n  (Luhuan Wu & Sinead Williamson)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}