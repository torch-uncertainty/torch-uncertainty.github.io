{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Train a Bayesian Neural Network in Three Minutes\n\nIn this tutorial, we will train a Bayesian Neural Network (BNN) LeNet classifier on the MNIST dataset.\n\n## Foreword on Bayesian Neural Networks\n\nBayesian Neural Networks (BNNs) are a class of neural networks that can estimate the uncertainty of their predictions via uncertainty on their weights. This is achieved by considering the weights of the neural network as random variables, and by learning their posterior distribution. This is in contrast to standard neural networks, which only learn a single set of weights, which can be seen as Dirac distributions on the weights.\n\nFor more information on Bayesian Neural Networks, we refer the reader to the following resources:\n\n- Weight Uncertainty in Neural Networks [ICML2015](https://arxiv.org/pdf/1505.05424.pdf)\n- Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users [IEEE Computational Intelligence Magazine](https://arxiv.org/pdf/2007.06823.pdf)\n\n## Training a Bayesian LeNet using TorchUncertainty models and PyTorch Lightning\n\nIn this part, we train a bayesian LeNet, based on the model and routines already implemented in TU.\n\n### 1. Loading the utilities\n\nTo train a BNN using TorchUncertainty, we have to load the following utilities from TorchUncertainty:\n\n- the cli handler: cli_main and argument parser: init_args\n- the model: bayesian_lenet, which lies in the torch_uncertainty.model module\n- the classification training routine in the torch_uncertainty.training.classification module\n- the bayesian objective: the ELBOLoss, which lies in the torch_uncertainty.losses file\n- the datamodule that handles dataloaders: MNISTDataModule, which lies in the torch_uncertainty.datamodule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty import cli_main, init_args\nfrom torch_uncertainty.datamodules import MNISTDataModule\nfrom torch_uncertainty.losses import ELBOLoss\nfrom torch_uncertainty.models.lenet import bayesian_lenet\nfrom torch_uncertainty.routines.classification import ClassificationSingle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also need to define an optimizer using torch.optim as well as the\nneural network utils withing torch.nn, as well as the partial util to provide\nthe modified default arguments for the ELBO loss.\n\nWe also import ArgvContext to avoid using the jupyter arguments as cli\narguments, and therefore avoid errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom cli_test_helpers import ArgvContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Creating the Optimizer Wrapper\nWe will use the Adam optimizer with the default learning rate of 0.001.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optim_lenet(model: nn.Module) -> dict:\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=1e-3,\n    )\n    return {\"optimizer\": optimizer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Creating the necessary variables\n\nIn the following, we will need to define the root of the datasets and the\nlogs, and to fake-parse the arguments needed for using the PyTorch Lightning\nTrainer. We also create the datamodule that handles the MNIST dataset,\ndataloaders and transforms. Finally, we create the model using the\nblueprint from torch_uncertainty.models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root = Path(os.path.abspath(\"\"))\n\n# We mock the arguments for the trainer\nwith ArgvContext(\n    \"file.py\",\n    \"--max_epochs\",\n    \"1\",\n    \"--enable_progress_bar\",\n    \"False\",\n):\n    args = init_args(datamodule=MNISTDataModule)\n\nnet_name = \"bayesian-lenet-mnist\"\n\n# datamodule\nargs.root = str(root / \"data\")\ndm = MNISTDataModule(**vars(args))\n\n# model\nmodel = bayesian_lenet(dm.num_channels, dm.num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. The Loss and the Training Routine\nThen, we just have to define the loss to be used during training. To do this,\nwe redefine the default parameters from the ELBO loss using the partial\nfunction from functools. We use the hyperparameters proposed in the blitz\nlibrary. As we are train a classification model, we use the CrossEntropyLoss\nas the likelihood.\nWe then define the training routine using the classification training routine\nfrom torch_uncertainty.training.classification. We provide the model, the ELBO\nloss and the optimizer, as well as all the default arguments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss = partial(\n    ELBOLoss,\n    model=model,\n    criterion=nn.CrossEntropyLoss(),\n    kl_weight=1 / 50000,\n    num_samples=3,\n)\n\nbaseline = ClassificationSingle(\n    model=model,\n    num_classes=dm.num_classes,\n    in_channels=dm.num_channels,\n    loss=loss,\n    optimization_procedure=optim_lenet,\n    **vars(args),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Gathering Everything and Training the Model\n\nNow that we have prepared all of this, we just have to gather everything in\nthe main function and to train the model using the PyTorch Lightning Trainer.\nSpecifically, it needs the baseline, that includes the model as well as the\ntraining routine, the datamodule, the root for the datasets and the logs, the\nname of the model for the logs and all the training arguments.\nThe dataset will be downloaded automatically in the root/data folder, and the\nlogs will be saved in the root/logs folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = cli_main(baseline, dm, root, net_name, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Testing the Model\n\nNow that the model is trained, let's test it on MNIST\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nimport numpy as np\n\n\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\ndataiter = iter(dm.val_dataloader())\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images[:4, ...]))\nprint(\"Ground truth: \", \" \".join(f\"{labels[j]}\" for j in range(4)))\n\nlogits = model(images)\nprobs = torch.nn.functional.softmax(logits, dim=-1)\n\n_, predicted = torch.max(probs, 1)\n\nprint(\"Predicted digits: \", \" \".join(f\"{predicted[j]}\" for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n- **LeNet & MNIST:** LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. [Proceedings of the IEEE](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n- **Bayesian Neural Networks:** Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). Weight Uncertainty in Neural Networks [ICML 2015](https://arxiv.org/pdf/1505.05424.pdf)\n- **The Adam optimizer:** Kingma, D. P., & Ba, J. (2014). \"Adam: A method for stochastic optimization.\" [ICLR 2015](https://arxiv.org/pdf/1412.6980.pdf)\n- **The Blitz** [library](https://github.com/piEsposito/blitz-bayesian-deep-learning) (for the hyperparameters)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}